[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Transit Data Analysis",
    "section": "",
    "text": "This analysis will examine the fiscal characteristics of major U.S. public transit systems using publicly available data. For more details on the problem description, please refer to: Mini-Project #01. The primary objective is to answer key questions related to transit agencies, focusing on areas such as farebox recovery performance, ridership trends, and operating expenses. The analysis will involve tasks such as renaming columns, recoding modes, and addressing instructor-specified questions using various transit data sources."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Transit Data Analysis",
    "section": "",
    "text": "Install Required Packages\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nif(!require(\"lubridate\")) install.packages(\"lubridate\")\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\n\n\n\nLoad the packages\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\n\n\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\n\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Agency, City, State, NTD ID, Organization Type, Reporter Type, UZA...\ndbl  (2): Report Year, UACE Code\nnum (10): Primary UZA Population, Agency VOMS, Mode VOMS, Vehicle Operations...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`"
  },
  {
    "objectID": "mp01.html#library-setup",
    "href": "mp01.html#library-setup",
    "title": "Transit Data Analysis",
    "section": "Library Setup",
    "text": "Library Setup\n\nInstall Required Packages\nWe will be analyzing various data from various sources. Following libraries are needed for this analysis. First check if the library is already installed and then install if not installed.\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\n\n\nLoad the packages\nOnce the packages are installed, those will be loaded to the workspace so that they can be used later.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)"
  },
  {
    "objectID": "mp01.html#load-data",
    "href": "mp01.html#load-data",
    "title": "Transit Data Analysis",
    "section": "Load Data",
    "text": "Load Data\nSince we have now setup libraries, we will now download the data to our project so that we can use later fo our analysis. You might get an error when trying to download the file programmatically. If the error persists, download the files manually and rename those and copy them to project folder.\n\nLoading Fare Revenue Data\nWe will first Load Fare revenue data from 2022 Fare Revenue table. This table Contains data on revenues a transit agency earns from carrying passengers, organized by mode and type of service. Reported as funds earned, funds expended on operations, and funds expended on capital.\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif (!file.exists(\"2022_fare_revenue.xlsx\")) {\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n  # directory.\n  download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\",\n    destfile = \"2022_fare_revenue.xlsx\",\n    quiet = FALSE,\n    method = \"wget\"\n  )\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(\n    -`State/Parent NTD ID`,\n    -`Reporter Type`,\n    -`Reporting Module`,\n    -`TOS`,\n    -`Passenger Paid Fares`,\n    -`Organization Paid Fares`\n  ) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(\n    `NTD ID`, # Sum over different `TOS` for the same `Mode`\n    `Agency Name`, # These are direct operated and sub-contracted\n    `Mode`\n  ) |&gt; # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n\n\nNext load Expenses\nThe 2022 Annual dataset containing data on expenses applied to operate public transportation services for each agency, by mode, and type of service operated. Divides expenses among NTD expense functions and object classes.\n\n# Next, expenses\nif (!file.exists(\"2022_expenses.csv\")) {\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_expenses.csv\" in your project\n  # directory.\n  download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\",\n    destfile = \"2022_expenses.csv\",\n    quiet = FALSE,\n    method = \"wget\"\n  )\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(\n    `NTD ID`,\n    `Agency`,\n    `Total`,\n    `Mode`\n  ) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()"
  },
  {
    "objectID": "mp01.html#tasks",
    "href": "mp01.html#tasks",
    "title": "Transit Data Analysis",
    "section": "Tasks",
    "text": "Tasks\nNow, we will complete the tasks mentioned in this page\n\n\n\n\n\n\nTask 1 - Creating Syntatic Names\n\n\n\nRename a column: UZA Name to metro_area.\n\nUSAGE &lt;- USAGE |&gt; rename(metro_area = \"UZA Name\")\n\nWe will also rename few other columns to make them more readable\n\nUSAGE &lt;- USAGE |&gt;\n  rename(Passenger_Trips = UPT, Vehicle_Miles = VRM)\n\n\n\n\n\n\n\n\n\nTask 2: Recoding the Mode column\n\n\n\nFind Unique Modes and Print.\n\nunique_modes &lt;- USAGE |&gt;\n  distinct(Mode)\n\nprint(unique_modes)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\nNow we will get the meaning of these symbols from NDT website. Once we have the meaning for each Acronyms, we will replace using case-when.\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Motorbus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\n\n\n\n\n\n\n\nTask 3: Answering Instructor Specified Questions with dplyr\n\n\n\n\n1. What transit agency had the most total VRM in this sample?\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_VRM = sum(Vehicle_Miles, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_VRM)) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Total_VRM\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n2. What transit mode had the most total VRM in this sample?\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(Vehicle_Miles, na.rm=TRUE)) |&gt;\n  arrange(desc(Total_VRM)) |&gt;\n  datatable(options = list(pageLength = 1, dom = 't'),  # Only display top row\n          rownames = FALSE) |&gt; \n  formatRound(\"Total_VRM\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n\ntotal_trips &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month == \"2024-05-01\") |&gt;\n  summarize(Total_Trips = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  pull(Total_Trips)\n\nmessage &lt;- sprintf(\n  \"There were %s trips taken on the NYC Subway (Heavy Rail) in May 2024.\",\n  format(total_trips, big.mark = \",\")\n)\n\ncat(message)\n\nThere were 180,458,819 trips taken on the NYC Subway (Heavy Rail) in May 2024.\n\n\n\n\n5. How much did NYC subway ridership fall between April 2019 and April 2020?\nTo solve this, we will first find ridership for 2019 and 2020 separately. Then we will subtract to get change and get percentage.\n\n# Filter and summarize data for April 2019\napril_2019 &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month == \"2019-04-01\") |&gt;\n  summarize(Total_Trips_2019 = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  pull(Total_Trips_2019)\n\n# Filter and summarize data for April 2020\napril_2020 &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month == \"2020-04-01\") |&gt;\n  summarize(Total_Trips_2020 = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  pull(Total_Trips_2020)\n\n# Calculate the absolute difference and percentage drop\nridership_difference &lt;- april_2019 - april_2020\npercentage_drop &lt;- (ridership_difference / april_2019) * 100\n\n# Print the custom message with the result and percentage drop\nmessage &lt;- sprintf(\n  \"NYC subway ridership fell by %s trips between April 2019 and April 2020, which is a %.2f%% decrease.\",\n  format(ridership_difference, big.mark = \",\"), percentage_drop\n)\n\ncat(message)\n\nNYC subway ridership fell by 211,969,660 trips between April 2019 and April 2020, which is a 91.28% decrease.\n\n\n\n\n\n\n\n\n\n\n\nTask 4: Explore and Analyze\n\n\n\nFind three more interesting transit facts in this data other than those above.\n\n1. Top 5 Transit Agencies by Total Passenger Trips\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_Trips = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_Trips)) |&gt;\n  head(5) |&gt;\n  datatable(options = list(pageLength = 5, dom = \"t\"), rownames = FALSE) |&gt;\n  formatRound(\"Total_Trips\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n2. Top 5 Transit Modes by Total Vehicle Miles\n\ntop_modes_vrm &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(Vehicle_Miles, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_VRM)) |&gt;\n  head(5)\n\ndatatable(top_modes_vrm, options = list(pageLength = 5, dom = \"t\"), rownames = FALSE) |&gt;\n  formatRound(\"Total_VRM\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n3. Top 5 Agencies with Largest Decrease in Ridership between 2019 and 2020\n\nridership_decline &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(\n    Trips_2019 = sum(ifelse(month == \"2019-04-01\", Passenger_Trips, NA), na.rm = TRUE),\n    Trips_2020 = sum(ifelse(month == \"2020-04-01\", Passenger_Trips, NA), na.rm = TRUE)\n  ) |&gt;\n  mutate(Decline = Trips_2019 - Trips_2020) |&gt;\n  arrange(desc(Decline)) |&gt;\n  head(5)\n\ndatatable(ridership_decline, options = list(pageLength = 5, dom = \"t\"), rownames = FALSE) |&gt;\n  formatRound(c(\"Trips_2019\", \"Trips_2020\", \"Decline\"), digits = 0, mark = \",\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 5: Table Summarization\n\n\n\nCreate a new table from USAGE that has annual total (sum) UPT and VRM for 2022. This will require use of the group_by, summarize, and filter functions. You will also want to use the year function, to extract a year from the month column.\nThe resulting table should have the following columns:\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\n\nMake sure to ungroup your table after creating it.\nName this table USAGE_2022_ANNUAL.\nThis will be done with following command.\n\n# Create the USAGE_2022_ANNUAL table\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  # Extract the year from the month column\n  mutate(year = year(month)) |&gt;\n  # Filter for the year 2022\n  filter(year == 2022) |&gt;\n  # Group by the necessary columns\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  # Summarize total UPT and VRM for the year\n  summarize(UPT = sum(Passenger_Trips, na.rm = TRUE), VRM = sum(Vehicle_Miles, na.rm = TRUE)) |&gt;\n  # Ungroup the table\n  ungroup()\n\nLet’s verify that all the columns are there.\n\ncat(colnames(USAGE_2022_ANNUAL), sep = \"\\n\")\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\n\n\n\n\nNow, Let’s join with FINANCIALS to create single table and create USAGE_AND_FINANCIALS. However, before joining, let’s view the table Financials:\n\nFINANCIALS |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nHere we notice that Mode is Acronym. However, USAGE_2022_ANNUAL doesn’t have acronyms. To make sure we can join both the table, let’s change FINANCIALS mode to match USAGE_2022_ANNUAL.\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Motorbus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nNow, let’s join to create USAGE_AND_FINANCIALS. We will join NTD ID and Mode as they are present in both the tables to create USAGE_AND_FINANCIALS.\n\n USAGE_AND_FINANCIALS &lt;- left_join(\n  USAGE_2022_ANNUAL,\n  FINANCIALS,\n  join_by(`NTD ID`, Mode)\n) |&gt;\n  drop_na()\n\nLet’s view few records to make sure we have them:\n\nUSAGE_AND_FINANCIALS |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nBefore we answer the questions, we will rename few columns to make them more readable:\n\nUSAGE_AND_FINANCIALS &lt;- USAGE_AND_FINANCIALS |&gt;\n  rename(Passenger_Trips = UPT, Vehicle_Miles = VRM)\n\n\n\n\n\n\n\nTask 6: Farebox Recovery Among Major Systems\n\n\n\nUsing the USAGE_AND_FINANCIALS table, answer the following questions:\n\n1. Which transit system (agency and mode) had the most UPT in 2022?\n\n USAGE_AND_FINANCIALS |&gt;\n  select(Agency, Mode, Passenger_Trips) |&gt;\n  arrange(desc(Passenger_Trips)) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Passenger_Trips\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n2. Which transit system (agency and mode) had the highest farebox recovery (Total Fares to Expenses)?\n\n    USAGE_AND_FINANCIALS |&gt;\n  mutate(Farebox_Recovery = `Total Fares` / Expenses) |&gt;\n  filter(!is.na(`Expenses`) &`Expenses`&gt;0) |&gt;\n  arrange(desc(Farebox_Recovery)) |&gt;\n  select(Agency, Mode, Farebox_Recovery) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Farebox_Recovery\", mark = \",\")\n\n\n\n\n\n\n\n3 Which transit system (agency and mode) has the lowest expenses per UPT?\n\n  USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_UPT = Expenses / Passenger_Trips) |&gt;\n  arrange(Expenses_per_UPT) |&gt;\n  select(Agency, Mode, Expenses_per_UPT) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Expenses_per_UPT\", mark = \",\")\n\n\n\n\n\n\n\n4. Which transit system (agency and mode) has the highest total fares per UPT?\n\n  USAGE_AND_FINANCIALS |&gt;\n   mutate(Fares_per_UPT = `Total Fares` / Passenger_Trips) |&gt;\n   arrange(desc(Fares_per_UPT)) |&gt;\n   select(Agency, Mode, Fares_per_UPT) |&gt;\n   datatable(\n     options = list(pageLength = 1, dom = \"t\"), # Only display top row\n     rownames = FALSE\n   ) |&gt;\n   formatRound(\"Fares_per_UPT\", mark = \",\")\n\n\n\n\n\n\n\n5. Which transit system (agency and mode) has the lowest expenses per VRM?\n\n  USAGE_AND_FINANCIALS |&gt;\n   mutate(Expenses_per_VRM = Expenses / Vehicle_Miles) |&gt;\n   arrange(Expenses_per_VRM) |&gt;\n   select(Agency, Mode, Expenses_per_VRM) |&gt;\n   datatable(\n     options = list(pageLength = 1, dom = \"t\"), # Only display top row\n     rownames = FALSE\n   ) |&gt;\n   formatRound(\"Expenses_per_VRM\", mark = \",\")\n\n\n\n\n\n\n\n6. Which transit system (agency and mode) has the highest total fares per VRM?\n\n  USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_VRM = `Total Fares` / Vehicle_Miles) |&gt;\n  arrange(desc(Fares_per_VRM)) |&gt;\n  select(Agency, Mode, Fares_per_VRM) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Fares_per_VRM\", mark = \",\")"
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Transit Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn my view, the Transit Authority of Central Kentucky’s Vanpool stands out as the most efficient transit system due to its farebox recovery ratio exceeding 100%, meaning it generates more fare revenue than its operating costs. This high level of financial self-sufficiency is uncommon in public transit and makes it highly efficient from a financial sustainability perspective.\nOverall, this was an interesting assignment to understand basics DT operations using Transportation data. This analysis provided valuable insights into the financial and operational performance of different transit agencies and modes."
  },
  {
    "objectID": "mp01.html#key-points-from-this-analysis",
    "href": "mp01.html#key-points-from-this-analysis",
    "title": "Transit Data Analysis",
    "section": "Key points from this analysis",
    "text": "Key points from this analysis\n\nMTA New York City Transit had the most passenger trips in 2022.\nNYC Subway saw a 91.28% drop in ridership between April 2019 and April 2020 due to COVID-19.\nTransit Authority of Central Kentucky’s Vanpool had the highest farebox recovery, covering costs effectively through fare revenue.\nNorth Carolina State University’s Motorbus service had the lowest expenses per passenger trip.\nThe Motorbus mode, across all agencies, is the one that collects the most fare revenue for every mile the buses travel while carrying passengers.\nThe transit system with the highest total fares per Vehicle Revenue Mile is the Chicago Water Taxi in the Ferryboat mode."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project 02",
    "section": "",
    "text": "IMPD PROJECT\n\n\nIntroduction\nThe goal of this project is to leverage data-driven insights to identify the key characteristics of successful movies and develop a compelling proposal for a new film. By analyzing historical IMDb data on movie ratings, genres, and key personnel, we aim to guide creative decisions with statistical evidence, ultimately proposing a high-potential movie idea that aligns with current industry trends and audience preferences.please refer to: Mini-Project #02\n\n\nData Sources\nData from the The IMDb non-commercial release can be used for this project. Specifically, which are made freely available for non-commercial use, provide comprehensive information about films, including ratings, genres, and key personnel, allowing for analysis of historical trends and the development of a data-driven movie proposal.\nThe IMDb dataset was initially too large, so for this project, a pre-processed and downsized version provided by the professor on GitHub. This version retains the essential information but is optimized for more manageable data analysis in R, allowing us to run the necessary tasks more efficiently without overloading system resources.\n\n\nData Description\nThe data includes comprehensive information about movies, such as ratings, genres, and key personnel. The dataset consists of multiple tables:\nname_basics_small: Contains information about actors, directors, and other personnel.\ntitle_basics_small: Provides basic movie details like title, genre, and release year.\ntitle_ratings_small: Contains IMDb user ratings and vote counts for movies.\ntitle_crew_small: Includes data on directors and writers for each title.\ntitle_principals_small: Details about the key actors and their roles in each movie.\ntitle_episodes_small: Contains data on TV episodes related to series.\n\n\nLoading Packages\nOnce the packages are installed, those will be loaded to the workspace so that they can be used later.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(tidyr)\nlibrary(data.table)\n\n\n\nLoad Data\nUsing the following code, I manage to download and process of the IMDb datasets, including name.basics, title.basics, title.episode, title.ratings, title.crew, and title.principals.\n\nget_imdb_file &lt;- function(fname) {\n  BASE_URL &lt;- \"https://github.com/michaelweylandt/STA9750/tree/main/miniprojects/mini02_preprocessed/\"\n  fname_ext &lt;- paste0(fname, \".csv.zip\")\n  rds_file &lt;- paste0(fname, \".rds\")\n\n  # Check if the .rds file already exists\n  if (file.exists(rds_file)) {\n    return(readRDS(rds_file)) # Load the data from the saved .rds file\n  } else {\n    # Download only if the .tsv.gz file does not exist\n    if (!file.exists(fname_ext)) {\n      FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n      download.file(FILE_URL, destfile = fname_ext, mode = \"wb\") # Ensure binary mode for downloading compressed files\n    }\n    \n    # Use data.table::fread for faster reading\n    df &lt;- as.data.frame(fread(fname_ext))\n    saveRDS(df, rds_file, compress = FALSE)\n    return(df)\n  }\n}\n\n\n# Load IMDb data\nNAME_BASICS &lt;- get_imdb_file(\"name_basics_small\")\nTITLE_BASICS &lt;- get_imdb_file(\"title_basics_small\")\nTITLE_RATINGS &lt;- get_imdb_file(\"title_ratings_small\")\nTITLE_EPISODES &lt;- get_imdb_file(\"title_episodes_small\")\nTITLE_CREW &lt;- get_imdb_file(\"title_crew_small\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title_principals_small\")\n\n\n\nData Sampling\nGiven the size of the data, we began by down-selecting to create a more manageable dataset for analysis. We will only select actors who are known for more than 1 movie/show.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\n\n\nData Visualization\nWe visualize key metrics to gain insights. For example, we create histograms of IMDb ratings to see the distribution of ratings across movies.\n\nTITLE_RATINGS |&gt;\n  ggplot(aes(x = numVotes)) +\n  geom_histogram(bins = 30) +\n  xlab(\"Number of IMDB Ratings\") +\n  ylab(\"Number of Titles\") +\n  ggtitle(\"Majority of IMDB Titles Have Less than 1000 Ratings\") +\n  theme_bw() +\n  scale_x_log10(label = scales::comma) +\n  scale_y_continuous(label = scales::comma)\n\n\n\n\n\n\n\n\n\n\nData Cleaning\nNow, let’s filter and join the IMDb datasets, correct column types, and clean the data for analysis by ensuring numeric and logical fields are properly formatted.\n\nTITLE_RATINGS |&gt;\n  pull(numVotes) |&gt;\n  quantile()\n\n     0%     25%     50%     75%    100% \n    100     165     332     970 2942823 \n\n\nNow, let’s only keep records that have atleast 100 records\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  filter(numVotes &gt;= 100)\n\nNow, semi_join will be used to keep records that are present in title_rating. This will help will reducing the size of data.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(parentTconst == tconst)\n  )\n\nTITLE_EPISODES &lt;- bind_rows(\n  TITLE_EPISODES_1,\n  TITLE_EPISODES_2\n) |&gt;\n  distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\n# Following are not used. Removing\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\n\nData Cleaning before Task 1\nWe use mutate and as.numeric to convert string values in the NAME_BASICS birthYear and deathYear columns to numeric format for proper analysis.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(\n    birthYear = as.numeric(birthYear),\n    deathYear = as.numeric(deathYear)\n  )\n\n# Let's view data now. \nglimpse(NAME_BASICS)\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\n\n\n\n\n\n\nTask 1: Column Type Correction\n\n\n\nCorrect the column types of the TITLE tables using a combination of mutate and the coercion functions as.numeric and as.logical.\nNow, we will examine the dataset using glimpse and clean the data by converting columns in TITLE_BASICS and TITLE_EPISODES to numeric and boolean types, and split knownForTitles in NAME_BASICS for further analysis.\n\nglimpse(TITLE_BASICS)\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;chr&gt; \"1894\", \"1892\", \"1892\", \"1892\", \"1893\", \"1894\", \"1894\",…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nAs we can see, isAdult can be boolean. Startyear, endYear, runTimeMinutes can be numbers.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    startYear = as.numeric(startYear),\n    endYear = as.numeric(endYear),\n    runtimeMinutes = as.numeric(runtimeMinutes),\n    isAdult = as.logical(isAdult)\n  )\n\nNow, let’s do some data cleaning for other tables. Starting with TITLE_EPISODES\n\nglimpse(TITLE_EPISODES)\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nIt can be seen that seasonNumber and Episode Number are numbers. Converting those:\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(\n    seasonNumber = as.numeric(seasonNumber),\n    episodeNumber = as.numeric(episodeNumber)\n  )\n\nNAME_BASICS will be analyzed next.\n\nglimpse(NAME_BASICS)\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nIt can be seen that knownForTitles column contains multiple comma separated records. We can use the separate_longer_delim function to break these into multiple rows.\n\nNAME_BASICS |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  slice_head(n = 10)\n\n      nconst     primaryName birthYear deathYear\n1  nm0000001    Fred Astaire      1899      1987\n2  nm0000001    Fred Astaire      1899      1987\n3  nm0000001    Fred Astaire      1899      1987\n4  nm0000001    Fred Astaire      1899      1987\n5  nm0000002   Lauren Bacall      1924      2014\n6  nm0000002   Lauren Bacall      1924      2014\n7  nm0000002   Lauren Bacall      1924      2014\n8  nm0000002   Lauren Bacall      1924      2014\n9  nm0000003 Brigitte Bardot      1934        NA\n10 nm0000003 Brigitte Bardot      1934        NA\n                    primaryProfession knownForTitles\n1        actor,miscellaneous,producer      tt0072308\n2        actor,miscellaneous,producer      tt0050419\n3        actor,miscellaneous,producer      tt0053137\n4        actor,miscellaneous,producer      tt0027125\n5  actress,soundtrack,archive_footage      tt0037382\n6  actress,soundtrack,archive_footage      tt0075213\n7  actress,soundtrack,archive_footage      tt0117057\n8  actress,soundtrack,archive_footage      tt0038355\n9   actress,music_department,producer      tt0057345\n10  actress,music_department,producer      tt0049189\n\n\n\n\n\n\n\n\n\n\nTask 2: Instructor-Provided Questions\n\n\n\n\n2.1. How many movies, TV series, and TV episodes are in the data set?\nWe calculate the total number of movies, TV series, and TV episodes after removing records with less than 100 ratings.\n\n# Count movies, TV series, and TV episodes\nresult &lt;- TITLE_BASICS |&gt;\n  group_by(titleType) |&gt;\n  summarise(count = n())\n\n# Now lets make a message\nmessage &lt;- sprintf(\n  \"The dataset contains %d movies, %d TV series, and %d TV episodes.\",\n  result$count[result$titleType == \"movie\"],\n  result$count[result$titleType == \"tvSeries\"],\n  result$count[result$titleType == \"tvEpisode\"]\n)\n\ncat(message)\n\nThe dataset contains 131662 movies, 29789 TV series, and 155722 TV episodes.\n\n\n\n\n2.2. Who is the oldest living person in our data set?\nWe find the oldest living person by filtering for those with missing death years and sorting by birth year. However, we need to make sure that are not including records before 1900 as they might be missing.\n\n# Find the oldest living person, born after a reasonable cutoff (e.g., 1900)\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt; # Only living people\n  filter(!is.na(birthYear)) |&gt; # Exclude missing birth years\n  filter(birthYear &gt;= 1900) |&gt; # Consider only people born after 1900\n  arrange(birthYear) |&gt; # Sort by birth year\n  head(1) # Get the oldest person\n\n# Calculate current age\ncurrent_year &lt;- as.numeric(format(Sys.Date(), \"%Y\"))\noldest_person_age &lt;- current_year - oldest_person$birthYear\n\n# Create a dynamic message\nmessage &lt;- sprintf(\n  \"The oldest living person in the dataset is %s, born in %d. They are currently %d years old.\",\n  oldest_person$primaryName,\n  oldest_person$birthYear,\n  oldest_person_age\n)\n\ncat(message)\n\nThe oldest living person in the dataset is Léonide Azar, born in 1900. They are currently 124 years old.\n\n\n\n\n2.3. Find the TV episode with 10/10 rating and 200,000 ratings\nWe use filtering and joins to identify highly rated TV episodes.\n\n# Step 1: Find the TV episode with a 10/10 rating and at least 200,000 votes\ntop_rated_episode &lt;- TITLE_RATINGS |&gt;\n  filter(averageRating == 10, numVotes &gt;= 200000) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  head(1)\n\n\nmessage &lt;- sprintf(\n    \"The top-rated TV episode is %s with %d votes and a perfect 10/10 rating.\",\n    top_rated_episode$primaryTitle,\n    top_rated_episode$numVotes\n  )\n\n\n# Print the message\ncat(message)\n\nThe top-rated TV episode is Ozymandias with 227589 votes and a perfect 10/10 rating.\n\n\n\n\n2.4. What four projects is the actor Mark Hamill most known for?\n\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  pull(knownForTitles)\n\n\ntconsts &lt;- unlist(strsplit(mark_hamill, \",\"))\n\n\nTITLE_BASICS |&gt;\n  filter(tconst %in% tconsts) |&gt;\n  select(primaryTitle, titleType, startYear) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n2.5. TV series with more than 12 episodes and the highest average rating\n\n# Filter for TV episodes with ratings and join with TITLE_BASICS\nepisode_ratings &lt;- TITLE_EPISODES |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"parentTconst\" = \"tconst\"))\n\n# Count episodes per series and filter for series with more than 12 episodes\ntop_rated_series &lt;- episode_ratings |&gt;\n  group_by(parentTconst, primaryTitle) |&gt;\n  summarise(\n    avg_rating = mean(averageRating, na.rm = TRUE),\n    num_episodes = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(num_episodes &gt; 12) |&gt;\n  arrange(desc(avg_rating)) |&gt;\n  head(1)\n\nmessage &lt;- sprintf(\n    \"The TV series with the highest average rating is %s with an average rating of %.2f across %d episodes.\",\n    top_rated_series$primaryTitle,\n    top_rated_series$avg_rating,\n    top_rated_series$num_episodes\n  )\ncat(message)\n\nThe TV series with the highest average rating is Kavya - Ek Jazbaa, Ek Junoon with an average rating of 9.75 across 113 episodes.\n\n\n\n\n2.6. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\nlibrary(ggplot2)\n\n# Find all episodes of Happy Days\nhappy_days &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\")\n\n# Join with episodes and ratings\nhappy_days_ratings &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst %in% happy_days$tconst) |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  group_by(seasonNumber) |&gt;\n  summarise(avg_rating = mean(averageRating, na.rm = TRUE)) |&gt;\n  arrange(seasonNumber)\n\n# Create a line graph to visualize the average ratings by season\nggplot(happy_days_ratings, aes(x = seasonNumber, y = avg_rating)) +\n  geom_line(color = \"pink\") + \n  geom_point(color = \"red\") +\n  labs(\n    title = \"Average Ratings of Happy Days by Season\",\n    x = \"Season Number\",\n    y = \"Average Rating\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBased on the results, it does appear that later seasons of Happy Days generally have lower average ratings than the earlier seasons:\n\nSeasons 1 through 3 have relatively high ratings, all around or above 7.5.\nStarting from Season 4, there is a noticeable decline, with Season 8 reaching a low of 5.3.\nThere is a small improvement in Seasons 9 to 11, but they remain lower compared to the earlier seasons.\n\nThis confirms that later seasons, particularly after Season 3, tend to have lower average ratings compared to the earlier ones, supporting the hypothesis that the quality of the show may have declined in its later years\n\n\n\n\n\n\n\n\n\nTask 3: Custom Success Metric\n\n\n\n\n3.1. Choose the top 5-10 movies on your m-0[] that combines IMDb ratings with the number of votes. We define success as follows:\nA success metric is a way to measure how well something has performed, in this case, movies. For your project, we want to measure both the quality of a movie (how good people think it is) and its popularity (how many people have seen and rated it).\nWe use two key pieces of information from IMDb:\n\nAverage IMDb Rating: This tells us how good the movie is, based on the ratings it has received.\nNumber of Votes: This tells us how many people rated the movie, which helps us understand how popular it is.\n\nTo create the success metric, we combine these two things in a formula:\nSuccess Metric = Average IMDb Rating * log10(Number of Votes)\n\nThe average rating shows how much people liked the movie.\nThe logarithm of the number of votes is used to make sure that popular movies with lots of ratings get credit for being well-known, but without letting movies with lots of votes (but mediocre ratings) dominate.\n\nOverall, this will look for movies that are both highly rated and widely popular, and this formula helps to rank movies based on both factors. The higher the success metric, the more successful the movie is considered.\nLet’s add a column-success_metric to store the new metric.\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(success_metric = averageRating * log10(numVotes))\n\n\n\n3.1. Choose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\n\nmovies_only &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\")\n\n# Add a custom success metric to the movies_ratings table\nmovies_ratings &lt;- TITLE_RATINGS |&gt;\n  inner_join(movies_only, by = \"tconst\")\n\n\n# View the top 10 movies by success_metric\nmovies_ratings |&gt;\n  arrange(desc(success_metric)) |&gt;\n  head(10) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable()\n\n\n\n\n\nThese are the top 10 movies on my metric and they are commercially successful movies.\n\n\n3.2. Choose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\n\n# Select 3-5 movies with a high number of votes but low success metric\nmovies_ratings |&gt;\n  filter(numVotes &gt; 100000) |&gt; # Filter for popular movies\n  arrange(success_metric) |&gt; # Sort by lowest success metric\n  head(5) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable()\n\n\n\n\n\nThese are top 5 movies with high number of votes but low success metric, indicating low-quality popular movies.\n\n\n3.3. Choose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nFor this question let’s go with famous Director Steven Spielberg.\n\n# Filter for Steven Spielberg in the NAME_BASICS table\nspielberg_nconst &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  pull(nconst)\n\n# Filter TITLE_CREW for movies directed by Steven Spielberg\nspielberg_projects &lt;- TITLE_CREW |&gt;\n  filter(directors == spielberg_nconst) # Spielberg's nconst from the previous step\n\n# Join with TITLE_BASICS to get movie titles\nspielberg_movies &lt;- spielberg_projects |&gt;\n  inner_join(movies_ratings, by = \"tconst\") # Join with the ratings table to get success metric\n\n# Arrange by success metric to see top movies\nspielberg_movies |&gt;\n  arrange(desc(success_metric)) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable()\n\n\n\n\n\nThis table shows a prestige director-Steven Spielberg with many projects having high success metrics, indicating a consistently successful career in terms of popularity and quality.\n\n\n3.4. Perform at least one other form of ‘spot check’ validation.\n\n# Split genres into multiple rows\n# Define blockbusters as movies with over 200,000 votes\nmovies_by_type &lt;- TITLE_RATINGS |&gt;\n  mutate(movie_type = ifelse(numVotes &gt;= 200000, \"Blockbuster\", \"Independent\")) |&gt;\n  group_by(movie_type) |&gt;\n  summarise(\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    num_movies = n(),\n    .groups = \"drop\"\n  )\n\n# View the result\nmovies_by_type |&gt;\n  DT::datatable()\n\n\n\n\n\n\nInterpretation:\nBlockbusters Tend to Have Higher Success: It’s expected that blockbuster movies score higher on the success metric because they usually attract a larger number of votes and typically have bigger production budgets, more marketing, and wider releases, leading to more exposure.The higher score (41.36) for blockbusters reflects their broad popular awareness combined with high ratings.\nIndependent Films: While independent films have a much lower average success score (18.25), they are still in large numbers. Independent films tend to have smaller audiences and therefore fewer ratings, which impacts their success metric.\n\n\n\n3.5. Come up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\nThe logic behind determining successful movies was to first calculate the quantiles of the success_metric to understand its distribution. We selected the 90th percentile as the success threshold, meaning only the top 10% of movies would be considered “successful.” Then, we filtered the dataset based on this threshold, allowing us to focus on high-performing movies for further analysis.\n\n# Determine quantiles for success_metric\nquantile(movies_ratings$success_metric, probs = seq(0, 1, by = 0.05))\n\n       0%        5%       10%       15%       20%       25%       30%       35% \n 2.068186  8.611309 10.187988 11.248161 12.058833 12.761020 13.412522 14.017286 \n      40%       45%       50%       55%       60%       65%       70%       75% \n14.604024 15.214337 15.854272 16.539468 17.309076 18.185141 19.151397 20.321881 \n      80%       85%       90%       95%      100% \n21.707109 23.449321 25.858492 29.826115 60.159507 \n\n\nSince 95% of movies have less than 25.85 rating, let’s consider this as a success threshold(v)\n\nsuccess_threshold &lt;- 25.85\n\n\n\n\n\n\n\n\n\n\nTask 4.Using questions like the following, identify a good “genre” for your next film. You do not need to answer these questions precisely, but these are may help guide your thinking.\n\n\n\n\nPrepare Data for Analysis\nNow, lets store popular movies. First mutate startYear as numeric, after which we will create separate records for separate genre with separate_rows and rename the genres column to genre.\n\nmovies_ratings &lt;- movies_ratings |&gt;\n  mutate(startYear = as.numeric(startYear))\n\n# Separate the genres into individual rows (some movies have multiple genres)\nmovies_genre &lt;- movies_ratings |&gt;\n  separate_rows(genres, sep = \",\")\n\n# Rename the 'genres' column to 'genre'\nmovies_genre &lt;- movies_genre |&gt;\n  rename(genre = genres)\n\nThis table will show the genre with the most successful movies in each decade. For example, “Drama” might dominate many decades, as it’s a consistently successful genre.\n\n\n4.1. What Was the Genre with the Most “Successes” in Each Decade?\n\n# Create a new column for the decade\nmovies_genre &lt;- movies_genre |&gt;\n  mutate(decade = floor(startYear / 10) * 10)\n\n# Filter for successful movies (using the threshold from earlier)\nsuccessful_movies_by_decade &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(decade, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(decade, desc(num_successes))\n\n# Find the top genre in each decade\nsuccessful_movies_by_decade |&gt;\n  group_by(decade) |&gt;\n  slice_max(order_by = num_successes, n = 1) |&gt;\n  DT::datatable()\n\n\n\n\n\nLooking at the answer, it can be said that Drama is the most famous genre each decade.\n\n\n4.2. What Genre Consistently Has the Most “Successes”?\n\n# Total number of successes per genre across all decades\ntotal_successes_by_genre &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(total_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_successes))\n\ntotal_successes_by_genre |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per the analysis result from this Drama Genre seems to have most success with the most numbers of suceesful Titles.\n\n\n4.3. What Genre Used to Reliably Produce “Successes” but Has Fallen Out of Favor?\nFor this, we will analyze success before 2000 and after 2000.\n\n# Successes in earlier decades (before 2000) vs recent decades (2000 and later)\nsuccess_by_era &lt;- movies_genre |&gt;\n  mutate(era = ifelse(decade &lt; 2000, \"Before 2000\", \"2000 and After\")) |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(era, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = era, values_from = num_successes, values_fill = 0) |&gt;\n  mutate(fall_out = `Before 2000` &gt; 0 & `2000 and After` == 0) |&gt;\n  filter(fall_out == TRUE)\n\nsuccess_by_era |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per above result, Film-Noir seems to have zero successes after 2000.\n\n\n4.4. What Genre Has Produced the Most “Successes” Since 2010?\n\n# Filter for movies since 2010 and count successes by genre\nmovies_genre |&gt;\n  filter(startYear &gt;= 2010, success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(num_successes)) |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per above result, Drama seems to have the most Successes after 2010.\n\n\n4.5. Does the Genre with the Most Successes Have the Highest Success Rate?\n\n# Calculate the total number of movies per genre and the number of successes\nmovies_genre |&gt;\n  group_by(genre) |&gt;\n  summarise(\n    total_movies = n(),\n    num_successes = sum(success_metric &gt;= success_threshold),\n    success_rate = num_successes / total_movies * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(success_rate)) |&gt;\n  DT::datatable()\n\n\n\n\n\nAlthough the Drama genre has most successful movies, it doesn’t seem to have the highest success rate. Biography genre has the most success rate. Drama is in 14th place when ranking by success rate.\n\n\n4.6. What Genre Has Become More Popular in Recent Years?\nTo do this analysis, we will compare movies before 2010 and after 2010.\n\nmovies_by_era &lt;- movies_genre |&gt;\n  filter(!is.na(startYear)) |&gt;  # Filter out rows with NA in startYear\n  mutate(era = ifelse(startYear &gt;= 2010, \"Post 2010\", \"Pre 2010\")) |&gt;  # Categorize movies by era\n  group_by(era, genre) |&gt;\n  summarise(num_successful_movies = n(), .groups = \"drop\") |&gt; \n  pivot_wider(names_from = era, values_from = num_successful_movies, values_fill = 0)\n\n# Calculate the absolute difference and percentage increase between eras\nmovies_by_era |&gt;\n  mutate(\n    abs_difference = `Post 2010` - `Pre 2010`,  # Absolute difference\n    percent_increase = (`Post 2010` - `Pre 2010`) / `Pre 2010` * 100  # Percentage increase\n  ) |&gt;\n  # Filter for genres with at least one movie in both eras and significant increase\n  filter(`Pre 2010` &gt; 0 & `Post 2010` &gt; 0) |&gt;\n  filter(percent_increase &gt;= 50)  |&gt;\n  select(genre, `Pre 2010`, `Post 2010`, abs_difference, percent_increase) |&gt;\n  arrange(desc(percent_increase)) |&gt;\n  head(1) |&gt;\n  DT::datatable()\n\n\n\n\n\nNews genre seems to have the highest increase in popularity.\n\n\n\n\n\n\n\n\n\nTask 5: Key Personnel\n\n\n\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\nActors with successful movies\nLet’s list down the actors with their numbers of movies which is categorized successful.\n\n# Join TITLE_PRINCIPALS with movies_ratings to get actors with successful movies\nsuccessful_actors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n# Join with NAME_BASICS to get actor names\nsuccessful_actors &lt;- successful_actors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_actors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nThis result says that most successful actor are Samuel L. Jackson & Robert De Niro with 72 & 71 sucessful movies.\n\n\nDirectors with successful movies\nNow let’s find the most successful director with their numbers of movies which is categorized successful.\n\nsuccessful_directors &lt;- TITLE_CREW |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(!is.na(directors)) |&gt;\n  separate_rows(directors, sep = \",\") |&gt;\n  group_by(directors) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(5)\n\nsuccessful_directors &lt;- successful_directors |&gt;\n  inner_join(NAME_BASICS, by = c(\"directors\" = \"nconst\")) |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_directors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nThis result says that most successful director is Woody Allen with 48 sucessful movies.\n\n\n5.1: Generate a Graphic for Actor/Director Success\n\nlibrary(ggplot2)\n\n# Combine actors and directors for visualization\nkey_talent &lt;- rbind(\n  successful_actors |&gt; head(2),\n  successful_directors |&gt; head(1)\n)\n\n# Create a bar plot for key talent success\nggplot(key_talent, aes(x = reorder(primaryName, -num_successful_movies), y = num_successful_movies)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(\n    title = \"Number of Successful Movies for Key Talent\",\n    x = \"Talent\",\n    y = \"Number of Successful Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe graph below demonstrates that Samuel L. Jackson and Robert De Nior lead the way with the highest number of successful movies, while Woody Allen stands out as the top director in terms of successful films.\n\n\n5.2: Create a Table for Actor/Director Success\n\nif (!require(\"kableExtra\")) install.packages(\"kableExtra\")\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Add role information for actors and directors\nactors &lt;- successful_actors |&gt;\n  head(2) |&gt;\n  mutate(role = \"Actor\")\ndirectors &lt;- successful_directors |&gt;\n  head(1) |&gt;\n  mutate(role = \"Director\")\n\n# Combine actors and directors into one table\nkey_talent &lt;- rbind(actors, directors)\n\n\n# Create a table showing the role, number of successful movies, and average success metric\nkey_talent |&gt;\n  select(primaryName, role, num_successful_movies, avg_success_metric) |&gt;\n  kable(col.names = c(\"Name\", \"Role\", \"Successful Movies\", \"Average Success Metric\")) |&gt;\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\n\n\n\n\nName\nRole\nSuccessful Movies\nAverage Success Metric\n\n\n\n\nSamuel L. Jackson\nActor\n72\n30.64902\n\n\nRobert De Niro\nActor\n71\n31.50732\n\n\nWoody Allen\nDirector\n48\n32.34535\n\n\n\n\n\n\n\nThe table below demonstrates that Samuel L. Jackson and Robert De Nirolead the way with the highest number of successful movies, while Woody Allen stands out as the top director in terms of successful films.\n\n\n\n\n\n\n\n\n\nTask 6: Finding a Classic Movie to Remake\n\n\n\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\n\n6.1: Filter for Classic Movies\n\n# Define the year cutoff for remakes (25 years ago)\nyear_cutoff &lt;- 1999\n\n# Filter for classic movies that haven't been remade in the last 25 years\nclassic_movies &lt;- movies_ratings |&gt;\n  filter(\n    startYear &lt; year_cutoff,\n    averageRating &gt;= 7.5,\n    numVotes &gt;= 50000\n  ) |&gt;\n  arrange(desc(averageRating))\n\nclassic_movies |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per result, these are the rank for classic movies.\n\n\n6.2: Check if Original Actors Are Still Alive\n\noriginal_movie_tconst &lt;- classic_movies$tconst[1] # Example: select the first movie in the list\n\n# Find actors in the original movie\noriginal_actors &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == original_movie_tconst, category %in% c(\"actor\", \"actress\")) |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, birthYear, deathYear)\n\n# Filter for actors who are still alive\noriginal_actors |&gt;\n  filter(is.na(deathYear)) |&gt;\n  DT::datatable()\n\n\n\n\n\nThis result says that the these top actors are still alive.\n\n\n6.3: Check if Original Directors/writers Are Still Alive\n\n# Find directors and writers from the original movie\noriginal_crew &lt;- TITLE_CREW |&gt;\n  filter(tconst == original_movie_tconst) |&gt;\n  separate_rows(directors, writers, sep = \",\") |&gt;\n  pivot_longer(c(directors, writers), names_to = \"role\", values_to = \"nconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, role, birthYear, deathYear)\n\n# Filter for crew members who are still alive\nalive_crew &lt;- original_crew |&gt;\n  filter(is.na(deathYear))\n\nalive_crew |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nThis result says that the these top directors and writers are still alive.\n\n\n6.4 Once you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\nTo proceed with the remake of “The Shawshank Redemption,” I have confirmed that several key figures from the original are still alive. Morgan Freeman and Tim Robbins, who played iconic roles, are both alive and could potentially be included in the project for “fan service.” Additionally, Frank Darabont, the original director, and Stephen King, the writer, are also still active.\nTo move forward, we will need to contact the legal department to secure the necessary rights from Castle Rock Entertainment for the film and Stephen King’s estate for the novella adaptation. Including classic actors like Freeman and Robbins in cameo roles would not only pay tribute to the original but also help attract loyal fans while building excitement for a new generation of viewers.\n\n\n\n\n\n\n\n\n\nTask 7: Elevator Pitch\n\n\n\nElevator Pitch: Remake of “The Shawshank Redemption”\nI’ve got a proposal that’s going to excite both loyal fans and a whole new audience — a modern remake of “The Shawshank Redemption.” This film has been a staple of cinema since 1994, ranked #1 on IMDb with a near-perfect rating of 9.3/10 and over 2.9 million votes. It’s clear: people love this story of hope, resilience, and friendship, and it’s time to bring it back with a fresh perspective.\nWe have an incredible opportunity to remake this classic with a stunning team. Woody Allen, known for his unique storytelling style, is our director. With 48 successful movies, Woody brings the creative depth needed to honor the original while offering a new artistic take.\nFor casting, we’re aiming high: Samuel L. Jackson as “Red.” With 72 hit films to his name, Jackson has the gravitas and warmth to bring this iconic character to life. And to make it even more dynamic, Robert De Niro, with 71 successful films, will play a key supporting role, ensuring this cast delivers both emotional depth and star power.\nWhy drama? In recent years, drama has consistently been the top-performing genre, producing the most successful films. From 2010 onward, it has led with over 3,290 successful titles, proving that audiences are hungry for emotionally-driven stories. With the powerhouse combination of Woody Allen, Samuel L. Jackson, and Robert De Niro, we’re confident that this remake will not only honor the original but also become a hit for modern viewers.\nTogether, this dream team is set to breathe new life into one of the greatest stories ever told, while maintaining the emotional core that made the original so beloved.\n\n\n\nKey Points for the movie:\nClassic Movie: The Shawshank Redemption is a highly rated film with over 2.9 million IMDb votes and a 9.3/10 rating, making it a prime candidate for a remake.\nStar Power: Samuel L. Jackson and Robert De Niro have over 140 successful films between them, bringing star power and acting prowess to the remake.\nDirector: Woody Allen, with 48 successful films, provides the artistic vision necessary to honor the original while offering a fresh perspective.\nMarket Potential: Drama continues to dominate as the top-performing genre, with over 3,290 successful titles released since 2010, making this remake a perfect fit for current audience preferences.\nNostalgia & Fan Service: Original actors like Morgan Freeman and Tim Robbins can make cameo appearances, blending nostalgia with a modern twist to attract loyal fans and new viewers alike."
  },
  {
    "objectID": "mp02.html#library-setup",
    "href": "mp02.html#library-setup",
    "title": "Mini Project 2",
    "section": "Library Setup",
    "text": "Library Setup\n\nInstall Required Packages\nWe will be analyzing various data from various sources. Following libraries are needed for this analysis. First check if the library is already installed and then install if not installed.\n\nif (!require(\"readr\")) install.packages(\"readr\") \nif (!require(\"dplyr\")) install.packages(\"dplyr\") \nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"stringr\")) install.packages(\"stringr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\n\n\n\nLoad the packages\nOnce the packages are installed, those will be loaded to the workspace so that they can be used later.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tidyr)"
  },
  {
    "objectID": "mp02.html#load-data",
    "href": "mp02.html#load-data",
    "title": "Mini Project 2",
    "section": "Load Data",
    "text": "Load Data\nSince we have now setup libraries, we will now download the data to our project so that we can use later fo our analysis\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\")\n\n\nTITLE_BASICS &lt;- get_imdb_file(\"title.basics\")\n\n\nTITLE_RATINGS &lt;- get_imdb_file(\"title.ratings\")\n\n\nTITLE_CREW &lt;- get_imdb_file(\"title.crew\")\n\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")"
  },
  {
    "objectID": "oct17class.html",
    "href": "oct17class.html",
    "title": "oct17class",
    "section": "",
    "text": "library(ggplot2)\n\nScatter plot of price vs carat, faceted by cut\nggplot(diamonds, aes(x = carat, y = price)) + geom_point(alpha = 0.5) + # Add transparency to handle overplotting facet_wrap(~ cut) + # Facet by cut labs(title = “Price vs Carat, Faceted by Cut”, x = “Carat”, y = “Price”) + theme_minimal() ````"
  },
  {
    "objectID": "mp02.html#task-3",
    "href": "mp02.html#task-3",
    "title": "Mini Project 02",
    "section": "Task 3",
    "text": "Task 3\n\n# Add a custom success metric to TITLE_RATINGS\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(success_metric = averageRating * log10(numVotes))\n\nglimpse(TITLE_RATINGS)\n\nRows: 373,996\nColumns: 4\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ averageRating  &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, …\n$ numVotes       &lt;dbl&gt; 2095, 283, 2102, 183, 2837, 196, 889, 2242, 215, 7724, …\n$ success_metric &lt;dbl&gt; 18.930749, 13.730004, 21.597113, 12.217236, 21.407728, …\n\n\n\nmovies_only &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\")\n\n# Add a custom success metric to the movies_ratings table\nmovies_ratings &lt;- TITLE_RATINGS |&gt;\n  inner_join(movies_only, by = \"tconst\")\n\n\n# View the top 10 movies by success_metric\nmovies_ratings |&gt;\n  arrange(desc(success_metric)) |&gt;\n  head(10) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n                                        primaryTitle averageRating numVotes\n1                           The Shawshank Redemption           9.3  2951083\n2                                    The Dark Knight           9.0  2932304\n3                                      The Godfather           9.2  2057179\n4      The Lord of the Rings: The Return of the King           9.0  2020203\n5                                       Pulp Fiction           8.9  2266176\n6                                          Inception           8.8  2602440\n7  The Lord of the Rings: The Fellowship of the Ring           8.9  2049867\n8                                         Fight Club           8.8  2382865\n9                                       Forrest Gump           8.8  2308847\n10                                  Schindler's List           9.0  1480407\n   success_metric\n1        60.17083\n2        58.20488\n3        58.08210\n4        56.74856\n5        56.56211\n6        56.45535\n7        56.17436\n8        56.11848\n9        55.99788\n10       55.53343"
  },
  {
    "objectID": "mp02.html#low-scoring-popular-movies-validation",
    "href": "mp02.html#low-scoring-popular-movies-validation",
    "title": "Mini Project 02",
    "section": "Low-Scoring Popular Movies Validation",
    "text": "Low-Scoring Popular Movies Validation\n\n# Select 3-5 movies with a high number of votes but low success metric\nmovies_ratings |&gt;\n  filter(numVotes &gt; 100000) |&gt; # Filter for popular movies\n  arrange(success_metric) |&gt; # Sort by lowest success metric\n  head(5) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n       primaryTitle averageRating numVotes success_metric\n1             Radhe           1.9   180234        9.98609\n2        Epic Movie           2.4   110309       12.10227\n3         Adipurush           2.7   134356       13.84629\n4 Meet the Spartans           2.8   112308       14.14115\n5          365 Days           3.3   100866       16.51236\n\n\n\n3. Prestige Actor or Director Validation\n\n# Example for a director like Steven Spielberg\n# TITLE_CREW |&gt;\n#    select(directors)\n\n# Filter for Steven Spielberg in the NAME_BASICS table\nspielberg_nconst &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  pull(nconst)\n\n# Filter TITLE_CREW for movies directed by Steven Spielberg\nspielberg_projects &lt;- TITLE_CREW |&gt;\n  filter(directors == spielberg_nconst) # Spielberg's nconst from the previous step\n\n# Join with TITLE_BASICS to get movie titles\nspielberg_movies &lt;- spielberg_projects |&gt;\n  inner_join(movies_ratings, by = \"tconst\") # Join with the ratings table to get success metric\n\n# Arrange by success metric to see top movies\nspielberg_movies |&gt;\n  arrange(desc(success_metric)) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n# spielberg_success &lt;- movies_ratings |&gt;\n#     filter(tconst %in% spielberg_tconst) |&gt;\n#     arrange(desc(success_metric))\n#\n# print(spielberg_success)"
  },
  {
    "objectID": "mp02.html#other-spot-check-validation",
    "href": "mp02.html#other-spot-check-validation",
    "title": "Mini Project 02",
    "section": "4. Other Spot-Check Validation",
    "text": "4. Other Spot-Check Validation\n\n# # Compare success scores across genres\n# genre_success &lt;- movies_ratings |&gt;\n#     group_by(genre) |&gt;\n#     summarise(avg_success = mean(success_metric, na.rm = TRUE)) |&gt;\n#     arrange(desc(avg_success))\n#\n# print(genre_success)\n\nlibrary(tidyr)\n\n# Split genres into multiple rows\nmovies_with_genres &lt;- movies_ratings |&gt;\n  separate_rows(genres, sep = \",\")\n\n# Calculate average success metric by genre\nmovies_with_genres |&gt;\n  group_by(genres) |&gt;\n  summarise(\n    avg_success = mean(success_metric, na.rm = TRUE),\n    num_movies = n()\n  ) |&gt; # Count how many movies are in each genre\n  arrange(desc(avg_success)) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n5. Numerical Threshold for Success\n\n# Determine quantiles for success_metric\nquantile(movies_ratings$success_metric, probs = seq(0, 1, by = 0.05))\n\n       0%        5%       10%       15%       20%       25%       30%       35% \n 2.068186  8.621344 10.187988 11.247041 12.058179 12.759427 13.407628 14.013592 \n      40%       45%       50%       55%       60%       65%       70%       75% \n14.601748 15.212304 15.853725 16.539468 17.308795 18.184748 19.151397 20.320207 \n      80%       85%       90%       95%      100% \n21.703213 23.447132 25.849774 29.816939 60.170827 \n\n\n\n\nFinding movies with success\n\n# Define a threshold for success based on the 90th percentile\nsuccess_threshold &lt;- 25.85\n\n# Filter movies that are considered \"successful\"\nmovies_ratings |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  head(100) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n###Examining Success by Genre and Decade 1. Prepare Data for Analysis\n\n# Ensure that startYear is numeric\nmovies_ratings &lt;- movies_ratings |&gt;\n  mutate(startYear = as.numeric(startYear))\n\n# Separate the genres into individual rows (some movies have multiple genres)\nmovies_genre &lt;- movies_ratings |&gt;\n  separate_rows(genres, sep = \",\")\n\n# Rename the 'genres' column to 'genre'\nmovies_genre &lt;- movies_genre |&gt;\n  rename(genre = genres)\n\n\nWhat Was the Genre with the Most “Successes” in Each Decade?\n\n\n# Create a new column for the decade\nmovies_genre &lt;- movies_genre |&gt;\n  mutate(decade = floor(startYear / 10) * 10)\n\n# Filter for successful movies (using the threshold from earlier)\nsuccessful_movies_by_decade &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(decade, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(decade, desc(num_successes))\n\n# Find the top genre in each decade\nsuccessful_movies_by_decade |&gt;\n  group_by(decade) |&gt;\n  slice_max(order_by = num_successes, n = 1) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n3. What Genre Consistently Has the Most “Successes”?\n\n# Total number of successes per genre across all decades\ntotal_successes_by_genre &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(total_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_successes))\n\ntotal_successes_by_genre |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n4. What Genre Used to Reliably Produce “Successes” but Has Fallen Out of Favor?\n\n# Successes in earlier decades (before 2000) vs recent decades (2000 and later)\nsuccess_by_era &lt;- movies_genre |&gt;\n  mutate(era = ifelse(decade &lt; 1980, \"Before 2000\", \"2000 and After\")) |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(era, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = era, values_from = num_successes, values_fill = 0) |&gt;\n  mutate(fall_out = `Before 2000` &gt; 0 & `2000 and After` == 0) |&gt;\n  filter(fall_out == TRUE)\n\nsuccess_by_era |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n5. What Genre Has Produced the Most “Successes” Since 2010?\n\n# Filter for movies since 2010 and count successes by genre\nmovies_genre |&gt;\n  filter(startYear &gt;= 2010, success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(num_successes)) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n6. Does the Genre with the Most Successes Have the Highest Success Rate?\n\n# Calculate the total number of movies per genre and the number of successes\nmovies_genre |&gt;\n  group_by(genre) |&gt;\n  summarise(\n    total_movies = n(),\n    num_successes = sum(success_metric &gt;= success_threshold),\n    success_rate = num_successes / total_movies * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(success_rate)) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n7. What Genre Has Become More Popular in Recent Years?\n\n# Compare genre popularity by counting the number of movies in recent years (post-2010) vs earlier\n# Ensure there are no missing startYear values and proceed with the calculation\nmovies_genre |&gt;\n  filter(!is.na(startYear)) |&gt; # Filter out rows with NA in startYear\n  mutate(era = ifelse(startYear &gt;= 2010, \"Post 2010\", \"Pre 2010\")) |&gt;\n  group_by(era, genre) |&gt;\n  summarise(num_movies = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = era, values_from = num_movies, values_fill = 0) |&gt;\n  mutate(popularity_increase = `Post 2010` &gt; `Pre 2010`) |&gt;\n  filter(popularity_increase == TRUE) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\nTask 5: Key Personnel\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\n# Join TITLE_PRINCIPALS with movies_ratings to get actors with successful movies\nsuccessful_actors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n# Join with NAME_BASICS to get actor names\nsuccessful_actors &lt;- successful_actors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_actors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nDirectors: Now let’s find the most successful director:\n\n# Join TITLE_CREW with movies_ratings to get directors with successful movies\nsuccessful_directors &lt;- TITLE_CREW |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(!is.na(directors)) |&gt;\n  separate_rows(directors, sep = \",\") |&gt;\n  group_by(directors) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(5)\n\n# Join with NAME_BASICS to get director names\nsuccessful_directors &lt;- successful_directors |&gt;\n  inner_join(NAME_BASICS, by = c(\"directors\" = \"nconst\")) |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_directors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nStep 2: Generate a Graphic for Actor/Director Success\n\nlibrary(ggplot2)\n\n# Combine actors and directors for visualization\nkey_talent &lt;- rbind(\n  successful_actors |&gt; head(2),\n  successful_directors |&gt; head(1)\n)\n\n# Create a bar plot for key talent success\nggplot(key_talent, aes(x = reorder(primaryName, -num_successful_movies), y = num_successful_movies)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(\n    title = \"Number of Successful Movies for Key Talent\",\n    x = \"Talent\",\n    y = \"Number of Successful Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nStep 3: Create a Table for Actor/Director Success\n\nif (!require(\"kableExtra\")) install.packages(\"kableExtra\")\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Add role information for actors and directors\nactors &lt;- successful_actors |&gt;\n  head(2) |&gt;\n  mutate(role = \"Actor\")\ndirectors &lt;- successful_directors |&gt;\n  head(1) |&gt;\n  mutate(role = \"Director\")\n\n# Combine actors and directors into one table\nkey_talent &lt;- rbind(actors, directors)\n\n\n# Create a table showing the role, number of successful movies, and average success metric\nkey_talent |&gt;\n  select(primaryName, role, num_successful_movies, avg_success_metric) |&gt;\n  kable(col.names = c(\"Name\", \"Role\", \"Successful Movies\", \"Average Success Metric\")) |&gt;\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\n\n\n\n\nName\nRole\nSuccessful Movies\nAverage Success Metric\n\n\n\n\nSamuel L. Jackson\nActor\n72\n30.51962\n\n\nRobert De Niro\nActor\n71\n31.50986\n\n\nWoody Allen\nDirector\n48\n32.34295\n\n\n\n\n\n\n\n\n\nTask 6: Finding a Classic Movie to Remake\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\nStep 1: Filter for Classic Movies\n\n# Define the year cutoff for remakes (25 years ago)\nyear_cutoff &lt;- 1999\n\n# Filter for classic movies that haven't been remade in the last 25 years\nclassic_movies &lt;- movies_ratings |&gt;\n  filter(\n    startYear &lt; year_cutoff,\n    averageRating &gt;= 7.5,\n    numVotes &gt;= 50000\n  ) |&gt;\n  arrange(desc(averageRating))\n\nclassic_movies |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nStep 2: Check if Original Actors Are Still Alive\n\n# Get key actors from the original movie\noriginal_movie_tconst &lt;- classic_movies$tconst[1] # Example: select the first movie in the list\n\n# Find actors in the original movie\noriginal_actors &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == original_movie_tconst, category %in% c(\"actor\", \"actress\")) |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, birthYear, deathYear)\n\n# Filter for actors who are still alive\nalive_actors &lt;- original_actors |&gt;\n  filter(is.na(deathYear))\n\nprint(alive_actors)\n\n        primaryName birthYear deathYear\n1       Tim Robbins      1958        NA\n2    Morgan Freeman      1937        NA\n3        Bob Gunton      1945        NA\n4    William Sadler      1950        NA\n5      Clancy Brown      1959        NA\n6       Gil Bellows      1967        NA\n7      Mark Rolston      1956        NA\n8    Jeffrey DeMunn      1947        NA\n9 Larry Brandenburg      1948        NA\n\n\nStep 2: Check if Original Directors/writers Are Still Alive\n\n# Find directors and writers from the original movie\noriginal_crew &lt;- TITLE_CREW |&gt;\n  filter(tconst == original_movie_tconst) |&gt;\n  separate_rows(directors, writers, sep = \",\") |&gt;\n  pivot_longer(c(directors, writers), names_to = \"role\", values_to = \"nconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, role, birthYear, deathYear)\n\n# Filter for crew members who are still alive\nalive_crew &lt;- original_crew |&gt;\n  filter(is.na(deathYear))\n\nalive_crew |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nStep 4: Consider Including Original Talent and Legal Clearance"
  },
  {
    "objectID": "mp02.html#key-points-for-the-movie",
    "href": "mp02.html#key-points-for-the-movie",
    "title": "Mini Project 02",
    "section": "Key Points for the movie:",
    "text": "Key Points for the movie:\nClassic Movie: The Shawshank Redemption is a highly rated film with over 2.9 million IMDb votes and a 9.3/10 rating, making it a prime candidate for a remake.\nStar Power: Samuel L. Jackson and Robert De Niro have over 140 successful films between them, bringing star power and acting prowess to the remake.\nDirector: Woody Allen, with 48 successful films, provides the artistic vision necessary to honor the original while offering a fresh perspective.\nMarket Potential: Drama continues to dominate as the top-performing genre, with over 3,290 successful titles released since 2010, making this remake a perfect fit for current audience preferences.\nNostalgia & Fan Service: Original actors like Morgan Freeman and Tim Robbins can make cameo appearances, blending nostalgia with a modern twist to attract loyal fans and new viewers alike."
  },
  {
    "objectID": "Mini03.html",
    "href": "Mini03.html",
    "title": "Mini Project 03",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "ocr24.html",
    "href": "ocr24.html",
    "title": "oct24",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n``` ::: {.cell}\n:::"
  },
  {
    "objectID": "Mini03.html#installment",
    "href": "Mini03.html#installment",
    "title": "Mini Project 03",
    "section": "Installment",
    "text": "Installment\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\n\n###Load data"
  },
  {
    "objectID": "TransitDATA.html#installment",
    "href": "TransitDATA.html#installment",
    "title": "Mini Project 03",
    "section": "Installment",
    "text": "Installment\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(httr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\nLoad data\n\nurl &lt;- \"https://data.ny.gov/api/views/vtvh-gimj/rows.csv?accessType=DOWNLOAD\"\n\n\nmta_data &lt;- read.csv(url)\n\n# View the first few rows\nhead(mta_data)\n\n       month   division line day_type num_on_time_trips num_sched_trips\n1 2020-01-01 A DIVISION    1        1              9035           10554\n2 2020-01-01 A DIVISION    1        2              2866            3050\n3 2020-01-01 A DIVISION    2        1              6058            7330\n4 2020-01-01 A DIVISION    2        2              1674            2340\n5 2020-01-01 A DIVISION    3        1              5732            6603\n6 2020-01-01 A DIVISION    3        2              1527            1875\n  terminal_on_time_performance\n1                    0.8560735\n2                    0.9396721\n3                    0.8264666\n4                    0.7153846\n5                    0.8680903\n6                    0.8144000\n\n\n\nglimpse(mta_data)\n\nRows: 2,665\nColumns: 7\n$ month                        &lt;chr&gt; \"2020-01-01\", \"2020-01-01\", \"2020-01-01\",…\n$ division                     &lt;chr&gt; \"A DIVISION\", \"A DIVISION\", \"A DIVISION\",…\n$ line                         &lt;chr&gt; \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"…\n$ day_type                     &lt;int&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,…\n$ num_on_time_trips            &lt;int&gt; 9035, 2866, 6058, 1674, 5732, 1527, 6552,…\n$ num_sched_trips              &lt;int&gt; 10554, 3050, 7330, 2340, 6603, 1875, 8124…\n$ terminal_on_time_performance &lt;dbl&gt; 0.8560735, 0.9396721, 0.8264666, 0.715384…\n\n\n\n\nWhich subway lines maintain the highest schedule?\n\n# Calculate the average terminal on-time performance for each line\nlibrary(dplyr)\n\ntop_lines &lt;- mta_data |&gt;\n  group_by(line)|&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# View the top lines with the highest average on-time performance\nhead(top_lines)\n\n# A tibble: 6 × 2\n  line   avg_on_time_performance\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 S 42nd                   0.996\n2 GS                       0.995\n3 S Fkln                   0.994\n4 FS                       0.993\n5 S Rock                   0.967\n6 H                        0.965\n\n\n###How does the terminal on-time performance vary across different subway lines from 2020 to 2024?\n\n# Assuming mta_data is already loaded in R\n\n# Convert 'month' column to Date type if not already\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Filter for data from 2020 to 2024\nmta_data_filtered &lt;- subset(mta_data, month &gt;= as.Date(\"2020-01-01\") & month &lt;= as.Date(\"2024-12-31\"))\n\n# Plotting\nggplot(mta_data_filtered, aes(x = month, y = terminal_on_time_performance, color = line)) +\n  geom_line() +\n  labs(title = \"On-Time Performance by Subway Line (2020-2024)\",\n       x = \"Month\",\n       y = \"On-Time Performance (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLine with the maximum and minimum number of on-time trips\n\n# Exclude rows where line is \"Systemwide\" becuase that is overall all\nfiltered_data &lt;- subset(mta_data, line != \"Systemwide\")\n\n# Calculate the total number of on-time trips for each line\nline_on_time_trips &lt;- aggregate(num_on_time_trips ~ line, data = filtered_data, sum)\n\n# Find the line with the maximum number of on-time trips\nmax_on_time_line &lt;- line_on_time_trips[which.max(line_on_time_trips$num_on_time_trips), ]\n\n# Find the line with the minimum number of on-time trips\nmin_on_time_line &lt;- line_on_time_trips[which.min(line_on_time_trips$num_on_time_trips), ]\n\n# Display the results\nprint(paste(\"The line with the maximum on-time trips is Line\", max_on_time_line$line,\n            \"with\", max_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n[1] \"The line with the maximum on-time trips is Line 7 with 862955 on-time trips.\"\n\nprint(paste(\"The line with the minimum on-time trips is Line\", min_on_time_line$line,\n            \"with\", min_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n[1] \"The line with the minimum on-time trips is Line S Rock with 4721 on-time trips.\"\n\n\n\n\nwhich Month has most and least on-time performance?\n\n# Convert the 'month' column to Date type if it's not already\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Calculate the average terminal on-time performance for each month\nmonthly_performance &lt;- aggregate(terminal_on_time_performance ~ format(month, \"%Y-%m\"), data = mta_data, mean)\n\n# Rename columns for clarity\ncolnames(monthly_performance) &lt;- c(\"month\", \"avg_terminal_on_time_performance\")\n\n# Find the month with the highest terminal on-time performance\nmax_performance_month &lt;- monthly_performance[which.max(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Find the month with the lowest terminal on-time performance\nmin_performance_month &lt;- monthly_performance[which.min(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Display the results\nprint(paste(\"The month with the highest terminal on-time performance is\", max_performance_month$month,\n            \"with an average on-time performance of\", round(max_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n[1] \"The month with the highest terminal on-time performance is 2020-05 with an average on-time performance of 0.94 %.\"\n\nprint(paste(\"The month with the lowest terminal on-time performance is\", min_performance_month$month,\n            \"with an average on-time performance of\", round(min_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n[1] \"The month with the lowest terminal on-time performance is 2024-02 with an average on-time performance of 0.78 %.\"\n\n\n\n\nYearly on-time performance trend from 2020 to 2024\n\n# Analyze the yearly on-time performance trend from 2020 to 2024\nmta_data |&gt;\n  # Convert 'month' to Date format if not already in Date format\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = format(month, \"%Y\")) |&gt;\n  \n  # Calculate the average on-time performance for each year\n  group_by(year) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  \n  # Plot the trend over years\n  ggplot(aes(x = as.numeric(year), y = avg_terminal_on_time_performance)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Yearly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Year\",\n       y = \"Average On-Time Performance (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Calculate the average on-time performance for each subway line and select the top 5\ntop_5_lines &lt;- mta_data |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) # Select the top 5 lines\n\n# Display the result\ntop_5_lines\n\n# A tibble: 5 × 2\n  line   avg_on_time_performance\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 S 42nd                   0.996\n2 GS                       0.995\n3 S Fkln                   0.994\n4 FS                       0.993\n5 S Rock                   0.967\n\n\n\n# Calculate the total number of on-time trips for each subway line\nline_on_time_trips &lt;- mta_data |&gt;\n  group_by(line) |&gt;\n  summarize(total_on_time_trips = sum(num_on_time_trips, na.rm = TRUE))\n\n# Get the top 5 lines with the maximum on-time trips\ntop_5_max_trips &lt;- line_on_time_trips |&gt;\n  arrange(desc(total_on_time_trips)) |&gt;\n  slice_head(n = 5)\n\n# Get the top 5 lines with the minimum on-time trips\ntop_5_min_trips &lt;- line_on_time_trips |&gt;\n  arrange(total_on_time_trips) |&gt;\n  slice_head(n = 5)\n\n# Display the results\nlist(Top_5_Max_On_Time_Trips = top_5_max_trips, Top_5_Min_On_Time_Trips = top_5_min_trips)\n\n$Top_5_Max_On_Time_Trips\n# A tibble: 5 × 2\n  line       total_on_time_trips\n  &lt;chr&gt;                    &lt;int&gt;\n1 Systemwide            10417622\n2 7                       862955\n3 L                       780783\n4 GS                      702897\n5 6                       663427\n\n$Top_5_Min_On_Time_Trips\n# A tibble: 5 × 2\n  line   total_on_time_trips\n  &lt;chr&gt;                &lt;int&gt;\n1 S Rock                4721\n2 S Fkln                6752\n3 JZ                    7667\n4 S 42nd               14897\n5 B                   163207\n\n\n\n# Convert 'month' column to Date format if it is not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = format(month, \"%Y\"))\n\n# Calculate the average terminal on-time performance for each year\nyearly_performance &lt;- mta_data |&gt;\n  group_by(year) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE))\n\n# Plot the trend over years\nggplot(yearly_performance, aes(x = as.numeric(year), y = avg_terminal_on_time_performance)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Yearly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Year\",\n       y = \"Average On-Time Performance (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Convert 'month' column to Date format if not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = format(month, \"%Y\"))\n\n# Calculate the average on-time performance for each year\nyearly_performance &lt;- mta_data |&gt;\n  group_by(year) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  mutate(period = ifelse(year %in% c(\"2020\", \"2021\"), \"COVID Period (2020-2021)\", \"Post-COVID Recovery (2022-2024)\"))\n\n# Plot the trends for each period\nggplot(yearly_performance, aes(x = as.numeric(year), y = avg_terminal_on_time_performance, color = period)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Terminal On-Time Performance: COVID vs. Post-COVID Recovery\",\n       x = \"Year\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Period\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"COVID Period (2020-2021)\" = \"red\", \"Post-COVID Recovery (2022-2024)\" = \"green\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = as.numeric(format(month, \"%Y\")))\n\n# Separate data into COVID period (2020-2021) and post-COVID recovery (2022-2024)\ncovid_period &lt;- mta_data |&gt;\n  filter(year %in% c(2020, 2021)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) # Select top 5 lines\n\npost_covid_period &lt;- mta_data |&gt;\n  filter(year %in% c(2022, 2023, 2024)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) # Select top 5 lines\n\n# Display the results\nlist(COVID_Period_Top_5_Lines = covid_period, Post_COVID_Recovery_Top_5_Lines = post_covid_period)\n\n$COVID_Period_Top_5_Lines\n# A tibble: 5 × 2\n  line  avg_on_time_performance\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 FS                      0.994\n2 GS                      0.993\n3 H                       0.973\n4 L                       0.941\n5 7                       0.917\n\n$Post_COVID_Recovery_Top_5_Lines\n# A tibble: 5 × 2\n  line   avg_on_time_performance\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 S 42nd                   0.996\n2 GS                       0.996\n3 S Fkln                   0.994\n4 FS                       0.992\n5 S Rock                   0.967\n\n\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = as.numeric(format(month, \"%Y\")))\n\n# Calculate top 5 lines in 2020, filtering out NAs\ntop_5_2020 &lt;- mta_data |&gt;\n  filter(year == 2020 & !is.na(terminal_on_time_performance)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) |&gt;\n  mutate(year = \"2020\")\n\n# Calculate top 5 lines in 2022, filtering out NAs\ntop_5_2022 &lt;- mta_data |&gt;\n  filter(year == 2022 & !is.na(terminal_on_time_performance)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) |&gt;\n  mutate(year = \"2022\")\n\n# Combine both years into one data frame for plotting\ntop_lines_combined &lt;- bind_rows(top_5_2020, top_5_2022)\n\n# Plotting the top 5 lines in each year\nggplot(top_lines_combined, aes(x = reorder(line, avg_on_time_performance), \n                               y = avg_on_time_performance, fill = year)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Top 5 Reliable Subway Lines for 2020 and 2022\",\n       x = \"Subway Line\",\n       y = \"Average On-Time Performance (%)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"2020\" = \"blue\", \"2022\" = \"orange\")) +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n# Load the necessary library\nlibrary(readr)\n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/candy-power-ranking/candy-data.csv\"\ncandy_data &lt;- read_csv(url)\n\nRows: 85 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): competitorname\ndbl (12): chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewaf...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Display the first few rows of the data\nhead(candy_data)\n\n# A tibble: 6 × 13\n  competitorname chocolate fruity caramel peanutyalmondy nougat crispedricewafer\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;\n1 100 Grand              1      0       1              0      0                1\n2 3 Musketeers           1      0       0              0      1                0\n3 One dime               0      0       0              0      0                0\n4 One quarter            0      0       0              0      0                0\n5 Air Heads              0      1       0              0      0                0\n6 Almond Joy             1      0       0              1      0                0\n# ℹ 6 more variables: hard &lt;dbl&gt;, bar &lt;dbl&gt;, pluribus &lt;dbl&gt;,\n#   sugarpercent &lt;dbl&gt;, pricepercent &lt;dbl&gt;, winpercent &lt;dbl&gt;\n\n\n\nglimpse(candy_data)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ fruity           &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ caramel          &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ peanutyalmondy   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ nougat           &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ crispedricewafer &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ hard             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,…\n$ bar              &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ pluribus         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…\n\n\n\ncorrelation &lt;- cor(candy_data$sugarpercent, candy_data$winpercent)\nprint(paste(\"Correlation between sugar content and win percentage:\", correlation))\n\n[1] \"Correlation between sugar content and win percentage: 0.229150657128007\"\n\n\n\n# Calculate the correlation between pricepercent and winpercent\ncorrelation_price &lt;- cor(candy_data$pricepercent, candy_data$winpercent)\nprint(paste(\"Correlation between price and win percentage:\", correlation_price))\n\n[1] \"Correlation between price and win percentage: 0.345325409967685\"\n\n\n\n# Find the candy with the highest win percentage\nmost_popular_candy &lt;- candy_data[which.max(candy_data$winpercent), ]\n\n# Display the name and win percentage of the most popular candy\nmost_popular_candy_name &lt;- most_popular_candy$competitorname\nmost_popular_candy_winpercent &lt;- most_popular_candy$winpercent\nprint(paste(\"The most popular candy during Halloween is:\", most_popular_candy_name, \n            \"with a win percentage of\", round(most_popular_candy_winpercent, 2), \"%\"))\n\n[1] \"The most popular candy during Halloween is: Reese's Peanut Butter cup with a win percentage of 84.18 %\"\n\n\n\n# Load necessary library\nlibrary(dplyr)\n\n# Find the 5 least popular candies based on win percentage\nleast_popular_5 &lt;- candy_data |&gt;\n  arrange(winpercent) |&gt;\n  slice(1:5)\n\n# Plot the win percentage of the 5 least popular candies\nbarplot(least_popular_5$winpercent,\n        names.arg = least_popular_5$competitorname,\n        main = \"5 Least Popular Candies\",\n        xlab = \"Candy\",\n        ylab = \"Win Percentage (%)\",\n        las = 2, \n        col = \"pink\")\n\n\n\n\n\n\n\n\n\n\nHow has the overall terminal on-time performance trended each month from 2020 to 2024?\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Calculate the average on-time performance for each month across all lines\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Calculate the average on-time performance for each month across all lines\nmonthly_performance &lt;- mta_data |&gt;\n  group_by(month = format(month, \"%Y-%m\")) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  mutate(month = as.Date(paste0(month, \"-01\"), format = \"%Y-%m-%d\"))\n\n# Add a column to indicate the period (COVID vs Post-COVID)\nmonthly_performance &lt;- monthly_performance |&gt;\n  mutate(period = ifelse(format(month, \"%Y\") %in% c(\"2020\", \"2021\"), \"COVID Period (2020-2021)\", \"Post-COVID Recovery (2022-2024)\"))\n\n# Plot the monthly trend with different colors for each period\nggplot(monthly_performance, aes(x = month, y = avg_terminal_on_time_performance, color = period)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Monthly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Period\") +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"3 months\") +\n  scale_color_manual(values = c(\"COVID Period (2020-2021)\" = \"pink\", \"Post-COVID Recovery (2022-2024)\" = \"purple\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Extract month name, month number, and assign a season based on month\nmta_data &lt;- mta_data |&gt;\n  mutate(month_name = format(month, \"%B\"),\n         month_num = as.numeric(format(month, \"%m\")),\n         season = case_when(\n           month_num %in% c(12, 1, 2) ~ \"Winter\",\n           month_num %in% c(3, 4, 5) ~ \"Spring\",\n           month_num %in% c(6, 7, 8) ~ \"Summer\",\n           month_num %in% c(9, 10, 11) ~ \"Fall\"\n         ))\n\n# Calculate the average on-time performance for each month across all years\nseasonal_performance &lt;- mta_data |&gt;\n  group_by(month_name, month_num, season) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(month_num)  # Arrange by month number for chronological order\n\n`summarise()` has grouped output by 'month_name', 'month_num'. You can override\nusing the `.groups` argument.\n\n# Plot the seasonal/monthly trend with colors by season\nggplot(seasonal_performance, aes(x = reorder(month_name, month_num), y = avg_on_time_performance, color = season, group = 1)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Seasonal/Monthly Terminal On-Time Performance Trend Across Years\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Season\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Winter\" = \"skyblue\", \"Spring\" = \"green\", \"Summer\" = \"orange\", \"Fall\" = \"brown\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini project 03",
    "section": "",
    "text": "TOPIC : Do Proportional Electoral College Allocations Yield a More Representative Presidency?\n\n\nIntroduction\nFor this project I will analyze whether the U.S. Electoral College provides a fair and representative outcome in presidential elections, particularly when different allocation methods are used. The findings will be presented in the form of a fact-checking report, analyzing claims of bias within the Electoral College and assessing how allocation rules influence election results.\n\n\nsetup libraries needed for analysis\nTo prepare for the analysis in this project, I am setting up and loading several R libraries that are essential for data manipulation, visualization, spatial analysis, and animation. This will ensure I have the tools needed for a comprehensive analysis of electoral data and effective, dynamic visualizations.\n\n# Let's Install the libraries \nif (!require(tidyverse)) install.packages(\"tidyverse\")\nif (!require(readr)) install.packages(\"readr\")\nif (!require(sf)) install.packages(\"sf\")\nif (!require(httr)) install.packages(\"httr\")\nif (!require(zip)) install.packages(\"zip\")\nif (!require(patchwork)) install.packages(\"patchwork\")\nif (!require(ggrepel)) install.packages(\"ggrepel\")\nif (!require(gganimate)) install.packages(\"gganimate\")\nif (!require(scales)) install.packages(\"scales\")\nif (!require(magick)) install.packages(\"magick\")\n\n\n# Let's Load the Libraries\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(sf)\nlibrary(httr)\nlibrary(zip)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(magick)\n\n\n\nManually Download us_house_votes.csv and president.csv.\nFollowing the project instructions, I manually downloaded two key datasets from the MIT Election Data Science Lab using my web browser. The us_house_votes.csv for U.S. House election votes (1976-2022) and president.csv for presidential votes (1976-2020). I saved these locally and loaded them into R for the analysis. I also used suppressMessages for cleaner output.\n\n# Load votes data and election data\nhouse_vote_data &lt;- suppressMessages(read_csv(\"mp03data/1976-2022-house.csv\"))\npresidential_election_data &lt;- suppressMessages(read_csv(\"mp03data/1976-2020-president.csv\"))\n\n\n\n\n\n\n\nTask 1: Download Congressional Shapefiles 1976-2012\n\n\n\nDownload data from 1976-2012\nNow I will automate download and extraction of shapefiles that contain the boundaries of U.S. congressional districts for each election year from 1976 to 2012. The data is sourced from Here For instance, the 94th Congress, spanning January 14, 1975, to October 1, 1976, is labeled ‘094’ in file names so I’ll use sprintf to format each Congress number as a three-digit code (e.g., ‘094’ for the 94th Congress), applying the same approach for all election years. Additionally, I will store the file location for each year.\n\n# Create a empty list - congressional_boundaries_shape_file_location_map to store, to store the mapping of each election year to the download location of its corresponding shapefile.\n\ncongressional_boundaries_shape_file_location_map &lt;- list()\n\n# Now I’m setting the starting year as 1976, as specified in the project guidelines.\nstarting_year &lt;- 1976\n\n# I created the get_congressional_boundaries_shape_files() function to automate downloading, storing, and extracting shapefiles for each congressional session based on the specified Congress number. Also,use sprintf.\n\nget_congressional_boundaries_shape_files &lt;- function(congress) {\n  congress_number &lt;- sprintf(\"%03d\", congress)\n\n  \n# Set base URL and file name pattern\n  base_url &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  file_name &lt;- paste0(\"districts\", congress_number, \".zip\")\n  \n# file_download_location is a local path where each zip file will be saved.\n  file_download_location &lt;- paste0(\"mp03data/zip_files/\", congress_number, \".zip\")\n  \n# To calculate the election year based on the Congress number.\n  election_year &lt;- starting_year + (congress - 94) * 2\n\n# store file location for given year\n  congressional_boundaries_shape_file_location_map[[election_year]] &lt;&lt;- file_download_location\n  \n\n# Unzip and load the shape file\n  unzip_dir &lt;- paste0(\"mp03data/shp_data/\", congress_number, \"/\")\n\n\n# Only download if the file does not already exist\n  if (!file.exists(file_download_location)) {\n    file_url &lt;- paste0(base_url, file_name)\n    dir.create(\"mp03data/zip_files\", recursive = TRUE)\n    download.file(file_url, destfile = file_download_location, mode = \"wb\")\n  }\n\n  unzip(file_download_location, exdir = unzip_dir)\n}\n\n# This loop iterates each Congress from 1976 to 2012, covering the period from 1976 (94th Congress) to 2012 (112th Congress). \nfor (congress in 94:112) { \n  get_congressional_boundaries_shape_files(congress)\n}\n\n\n\n\n\n\n\n\n\nTask 2: Download Congressional Shapefiles 2014-2022\n\n\n\nDownload Congressional data from 2014-2022\nlet’s ensure that the data for congressional boundaries from 2014 to 2022 is downloaded and organized efficiently, ready for further use in analysis. Also, the name of the files Here are ending with these numbers- “118”, “116”, “115”, “114” so, I checked each version in sequence until it finds one that works for a specific year.\n\n# This will be the template for the URL where the shapefiles are stored\nbase_url_template &lt;- \"https://www2.census.gov/geo/tiger/TIGER%d/CD/tl_%d_us_cd%s.zip\"\n\n# The code will try each version in sequence until it finds one that works for a specific year.\nversions &lt;- c(\"118\", \"116\", \"115\", \"114\")  \n\n# Specified the folder where the files will be saved.\ndownload_dir &lt;- \"mp03data\"\n\n# Create directory for saving shapefiles if it doesn't exist\nif (!dir.exists(download_dir)) {\n  dir.create(download_dir)\n}\n# Now Looping through years 2014:2022\nfor (year in 2014:2022) {\n  file_downloaded &lt;- FALSE\n\n# Try each version until one succeeds\n  for (version in versions) {\n# Construct the file name and URL using sprintf\n    file_name &lt;- paste0(\"congressional_districts_\", year, \"_cd\", version, \".zip\")\n    file_path &lt;- file.path(download_dir, file_name)\n    url &lt;- sprintf(base_url_template, year, year, version)\n    \n    # Check if file already exists to avoid re-downloading\n    if (!file.exists(file_path)) {\n      # Perform the download\n      response &lt;- GET(url)\n\n      if (status_code(response) == 200) {\n        # Write the downloaded content to a file\n        writeBin(content(response, \"raw\"), file_path)\n\n           # store file location for given year\n        \n  congressional_boundaries_shape_file_location_map[[year]] &lt;- file_path\n\n      }\n    } else {\n       # store file location for given year\n  congressional_boundaries_shape_file_location_map[[year]] &lt;- file_path\n  \n      break\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\nTask 3: Exploration of Vote Count Data Answer the following using the vote count data files from the MIT Election Data Science Lab. You may answer each with a table or plot as you feel is appropriate.\n\n\n\n\n3.1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\nThis identifies the states with the largest gains and losses in seats and visualizes these changes with a bar plot. Firstly, I filtered the data to include seat counts for each state in those years. I calculated the difference in seats (seat_change) for each state and identified the top 10 states with the largest gains and losses Then, I created a bar plot showing these changes, using green bars for gains and red bars for losses, with each bar labeled by the number of seats changed. Please follow the short notes with the code to understand it.\n\n# Load the data from year 1976 to 2022- representing U.S. House elections\nseats_by_state &lt;- house_vote_data |&gt;\n  filter(year %in% c(1976, 2022), office == \"US HOUSE\") |&gt;\n  group_by(state, year) |&gt;\n  summarise(seat_count = n_distinct(district), .groups = \"drop\")\n\n# calculate the change in seats between 1976 and 2022 for each state.\nseat_changes &lt;- seats_by_state |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  pivot_wider(names_from = year, values_from = seat_count, names_prefix = \"year_\")\n\n# Let's create new column to show the difference in seats\ntop_gains_losses &lt;- seat_changes |&gt;\n  mutate(seat_change = year_2022 - year_1976) |&gt;\n  arrange(desc(seat_change)) |&gt;\n  filter(row_number() &lt;= 10 | row_number() &gt; (n() - 10))\n\n# To show the plot, mapping the states (state) to the x-axis and seat_change to the y-axis.\nggplot(top_gains_losses, aes(x = reorder(state, seat_change), y = seat_change, fill = seat_change &gt; 0)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  coord_flip() +\n  geom_text(aes(label = seat_change), hjust = ifelse(top_gains_losses$seat_change &gt; 0, -0.2, 1.2), size = 3) +\n  scale_fill_manual(values = c(\"red\", \"green\"), labels = c(\"Loss\", \"Gain\"), guide = guide_legend(title = \"Seat Change\")) +\n  labs(\n    title = \"Top 10 Seat Gains and Losses in the US House of Representatives (1976-2022)\",\n    x = \"State\", y = \"Seat Change\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 10, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nThis chart shows the top 10 states with the most significant changes in U.S. House seats from 1976 to 2022. Texas and Florida gained the most seats (14 and 13), reflecting population growth in the South and West. In contrast, New York and Ohio lost the most seats (13 and 8), indicating population decline in the Northeast and Midwest. This shift redistributes political influence toward states with growing populations.\n\n\n3.2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nlet’s analyze New York’s U.S. House elections (1976 onward) to see if any outcomes would have differed without the “fusion” voting system. It calculates each candidate’s total votes (across all party lines) and votes from only their major party line (Democrat or Republican). It then compares the actual winner (fusion votes) with a hypothetical winner (major party votes only) and displays cases where the outcome would have changed if fusion votes weren’t counted.\n\nlibrary(dplyr)\n# Filter for New York and US HOUSE elections of  Representatives from 1976 onwards\nny_elections &lt;- house_vote_data |&gt;\n  filter(state == \"NEW YORK\", office == \"US HOUSE\", year &gt;= 1976)\n\n# Identify major party line (Democrat or Republican) and calculate total votes for each candidate\ncandidate_votes &lt;- ny_elections |&gt;\n  mutate(is_major_party = ifelse(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\"), TRUE, FALSE)) |&gt;\n  group_by(year, district, candidate) |&gt;\n  summarise(\n    total_votes = sum(candidatevotes),\n    major_party_votes = sum(candidatevotes[is_major_party]),\n    .groups = \"drop\"\n  )\n\n# Determine the actual winner and hypothetical winner if only major party line votes were counted\ncandidate_votes |&gt;\n  group_by(year, district) |&gt;\n  summarise(\n    fusion_winner = candidate[which.max(total_votes)],\n    major_party_winner = candidate[which.max(major_party_votes)],\n    fusion_votes = max(total_votes),\n    major_party_votes = max(major_party_votes),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(fusion_winner != major_party_winner) |&gt;\n  select(year, district, fusion_winner, major_party_winner, fusion_votes, major_party_votes) |&gt;\n  DT::datatable()\n\n\n\n\n\n“fusion_winner” is the candidate who actually won with combined votes from all party lines. “major_party_winner” is the hypothetical winner who would have won if only votes from major parties were counted.\n\n\n3.3.Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\nlet’s compare the total votes for Democratic and Republican candidates in presidential and congressional elections (U.S. House) during presidential election years (1976 to 2020) and how this trend varies over time.\n\n# Define presidential election years from 1976 to 2020 for our analysis\npresidential_years &lt;- seq(1976, 2020, by = 4)\n\n# let's use filter and group by to sum presidential votes by state, year, and party\npresidential_votes &lt;- presidential_election_data |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\"), year %in% presidential_years) |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  mutate(party = party_simplified) |&gt;\n  select(year, state, party, total_votes)\n\n# Here I am trying to filter for U.S. House elections in presidential years, considering only Democratic and Republican votes\ncongressional_votes &lt;- house_vote_data |&gt;\n  filter(office == \"US HOUSE\", party %in% c(\"DEMOCRAT\", \"REPUBLICAN\"), year %in% presidential_years) |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\")\n\n\n# Rename columns to differentiate between President and Congress election data\npresidential_votes &lt;- presidential_votes |&gt;\n  mutate(election_type = \"President\")\n\ncongressional_votes &lt;- congressional_votes |&gt;\n  mutate(election_type = \"Congress\")\n\n# Combine presidential and congressional votes into a single dataset- combined_votes\ncombined_votes &lt;- bind_rows(presidential_votes, congressional_votes)\n\nvotes_summary &lt;- combined_votes |&gt;\n  group_by(year, party, election_type) |&gt;\n  summarize(total_votes = sum(total_votes), .groups = \"drop\")\n\n# Now lets combine into one interaction_label for plotting\nvotes_summary &lt;- votes_summary |&gt;\n  mutate(interaction_label = factor(interaction(party, election_type),\n                                    levels = c(\"DEMOCRAT.President\", \"DEMOCRAT.Congress\",\n                                               \"REPUBLICAN.President\", \"REPUBLICAN.Congress\")))\n\n# Plot the data \nggplot(votes_summary, aes(x = year, y = total_votes, color = interaction_label)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Total Votes by Party and Election Type (Presidential and Congressional)\",\n    x = \"Year\",\n    y = \"Total Votes\",\n    color = \"Party and Election Type\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"DEMOCRAT.President\" = \"blue\",\n      \"DEMOCRAT.Congress\" = \"darkblue\",\n      \"REPUBLICAN.President\" = \"red\",\n      \"REPUBLICAN.Congress\" = \"darkred\"\n    ),\n    labels = c(\n      \"Democrat - President\",\n      \"Democrat - Congress\",\n      \"Republican - President\",\n      \"Republican - Congress\"\n    )\n  ) +\n  # Use comma format for y-axis labels- for readability\n  scale_y_continuous(labels = scales::comma) + \n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 10),\n    axis.text.x = element_text(angle = 40, hjust = 1)\n  )\n\n\n\n\n\n\n\n\nOver time, total votes for both presidential and congressional races have steadily increased, reflecting higher voter turnout or population growth.\n\n\n\n\n\n\n\n\n\nTask 4: Automate Zip File Extraction Adapt the code after the ##- symbol above into a function read_shp_from_zip() which takes in a file name, pulls out the .shp file contained there in, and reads it into R using read_sf().\n\n\n\nThe purpose of this process is to make it easier to work with shapefiles that are stored in zip files. Instead of manually unzipping the file, locating the shapefile, and loading it into R, this function does it all in one step. This approach is helpful because I will be working with geographic data from various sources.\n\n# Define a function for automated downloading and reading of shapefiles\n\nread_shp_from_zip &lt;- function(zip_file) {\n  \n  # Create a temporary directory to extract files\n  temp_dir &lt;- tempdir()\n\n  # Extract all contents of the zip file to the temporary directory\n  zip_contents &lt;- unzip(zip_file, exdir = temp_dir)\n  \n  # Defining the path where shapefiles should be located, ensures shapefiles are matched  \n  shp_file_location &lt;- paste0(temp_dir, \"/districtShapes/\")\n  shp_files &lt;- list.files(shp_file_location, pattern = \"\\\\.shp$\", full.names = TRUE)\n\n\n  # Check if a .shp file is found \n  if (length(shp_files) == 0) {\n    stop(\"No .shp file found in the zip archive.\")\n  }\n\n  # Read the .shp file as a simple feature collection\n  shp_sf &lt;- suppressMessages(read_sf(shp_files[1])) # Read the first .shp file if multiple are found\n\n  # This will return the spatial data (shp_sf) so it can be used in further analysis or visualization.\n  return(shp_sf)\n}\n\n\n\n\n\nCustom Functions\nLet’s create custom functions that will be useful later for the analysis.\n\n\nCreate function to get election year data.\nThis function will give us all the data related to an election year. Things like total votes, electoral vote count. It will extract data for that year, focusing on the two main parties (Democratic and Republican).\n\n# This represents which election year I wanna analyze\ncreate_election_year_data &lt;- function(year_to_look_for) {\n  \n  # Filter presidential data for the specified year and the two main parties\n  president_data_for_given_year &lt;- presidential_election_data |&gt;\n    filter(\n      year == year_to_look_for,\n      office == \"US PRESIDENT\",\n      party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")\n    ) |&gt;\n    # This groups the data by state\n    group_by(state) |&gt;\n    # Selects candidate with max vote\n    slice_max(candidatevotes, n = 1, with_ties = FALSE) |&gt;\n    # Select the relevant columns\n    select(state, state_po, candidate, party_simplified, candidatevotes)\n\n  # Filter congressional vote count data for the specified year and summarize Electoral College Votes (ECV)\n  ecv_data_for_given_year &lt;- house_vote_data |&gt;\n    filter(year == year_to_look_for, office == \"US HOUSE\") |&gt;\n    group_by(state) |&gt;\n    summarize(\n      num_representatives = n_distinct(district),\n      # +2 is to count 2 senators for each state\n      ecv = num_representatives + 2,\n      .groups = \"drop\"\n    ) |&gt;\n    # Add DC manually with its 3 electoral votes\n    bind_rows(data.frame(state = \"DISTRICT OF COLUMBIA\", ecv = 3))\n\n  # Combine presidential data with ECV data and format state names\n  combined_data &lt;- president_data_for_given_year |&gt;\n    # Left_join will merge \n    left_join(ecv_data_for_given_year, by = \"state\") |&gt;\n    # Formats state names to title case\n    mutate(state = str_to_title(state))\n\n  return(combined_data)\n}\n\nThis code will help to retrieve relevant information for election analysis, comparisons, or visualization.\n\n\nFunction to get shp_data for a given year.\nThis code is designed to prepare state-level shapefile data for a specified year.\n\n# Let's create function to take an argument(year)\ncreate_state_shp_data &lt;- function(year) {\n  # find file location for a given year\n  file_location_for_given_year &lt;- congressional_boundaries_shape_file_location_map[[year]] \n  # This calls the previously defined function (read_shp_from_zip) to extract and load the shapefile data for the specified year\n  shp_data_for_given_year &lt;- read_shp_from_zip(file_location_for_given_year)\n\n  # Standardize STATENAME to title case\n  congressional_districts_for_given_year &lt;- shp_data_for_given_year |&gt;\n    mutate(STATENAME = str_to_title(trimws(STATENAME))) |&gt; # Convert to title case (e.g., \"new york\" becomes \"New York\")\n    rename(state = STATENAME) |&gt; # Rename column to 'state' for consistency\n    select(state, geometry) |&gt; #select the given column\n    st_make_valid()\n\n# Let's aggregate the individual congressional districts into a single geometry for each state\n  state_level_shape_for_given_year &lt;- congressional_districts_for_given_year |&gt;\n    group_by(state) |&gt; # groupby state\n    reframe(geometry = st_union(geometry)) |&gt; # Use reframe to handle ungrouping automatically\n    distinct(state, .keep_all = TRUE) # Ensure unique entries remain for each state \n\n  return(state_level_shape_for_given_year)\n}\n\nThe create_state_shp_data function efficiently retrieves and processes shapefile data, combining individual congressional districts into state-level geometries for a specified year. This function is useful for generating state-level spatial data for maps and analyses where district-level detail is not required, ensuring consistency in state names and valid geometry.\n\n\nDefine function to create and save a map for a specific year\nNow working to create a customized election map for a given year. This map visualizes each state, showing which party won the state’s electoral votes, with color-coded states representing the winning party (Democrat, Republican, or Other). Also, specialized labeling for small northeastern states, and inset maps for Alaska and Hawai is done.\n\ncreate_election_map &lt;- function(election_year_data, shp_data_for_year, year_num, file_name = NULL) {\n  yearly_data &lt;- shp_data_for_year |&gt;\n    inner_join(election_year_data, by = \"state\")\n\n  # Define the small northeastern states to display abbreviations outside\n  yearly_data &lt;- yearly_data |&gt;\n    mutate(\n      state_abbr = ifelse(state %in% c(\n        \"Connecticut\", \"Delaware\", \"Maryland\", \"Massachusetts\",\n        \"New Jersey\", \"New Hampshire\", \"Rhode Island\", \"Vermont\"\n      ), state_po, NA), # Use abbreviations only for small northeastern states\n      label_text = ifelse(!is.na(state_abbr), paste0(state_abbr, \" \", ecv), NA)\n    )\n\n  northeastern_states &lt;- c(\n    \"Connecticut\", \"Delaware\", \"Maryland\", \"Massachusetts\",\n    \"New Jersey\", \"New Hampshire\", \"Rhode Island\", \"Vermont\"\n  )\n\n  if (!inherits(yearly_data, \"sf\")) {\n    yearly_data &lt;- st_as_sf(yearly_data)\n  }\n\n\n  # Prepare the main map excluding Alaska and Hawaii\n  mainland &lt;- yearly_data |&gt; filter(!state %in% c(\"Alaska\", \"Hawaii\"))\n  alaska &lt;- yearly_data |&gt; filter(state == \"Alaska\")\n  hawaii &lt;- yearly_data |&gt; filter(state == \"Hawaii\")\n\n\n  mainland_plot &lt;- ggplot(mainland) +\n    geom_sf(aes(fill = party_simplified)) +\n\n    # Only show electoral votes for non-northeastern states\n    geom_sf_text(\n      data = mainland |&gt; filter(!state %in% northeastern_states),\n      aes(label = ecv), size = 6, color = \"black\", fontface = \"bold\"\n    ) +\n\n    # Add labels for small northeastern states with electoral counts outside\n    geom_text_repel(\n      data = yearly_data |&gt; filter(!is.na(state_abbr)), # Only for small northeastern states\n      aes(\n        geometry = geometry,\n        label = label_text\n      ),\n      stat = \"sf_coordinates\", # Use spatial coordinates for label placement\n      size = 4,\n      color = \"black\",\n      fontface = \"bold\", # Make labels bold\n      nudge_x = 5, # Set a strong rightward nudge to move all labels to the right\n      nudge_y = 0, # Keep y-nudging minimal for alignment\n      hjust = 0, # Left-align labels\n      direction = \"y\", # Keep lines vertically aligned\n      lineheight = 0.9 # Adjust line height for clarity if needed\n    ) +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    labs(\n      title = paste0(year_num, \" Presidential Election - Electoral College Results\"),\n      fill = \"Winner\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 20, face = \"bold\"), # Bold title\n      legend.position = \"bottom\"\n    ) +\n    coord_sf(expand = FALSE)\n\n  # Alaska plot without legend\n  alaska_plot &lt;- ggplot(alaska) +\n    geom_sf(aes(fill = party_simplified)) +\n    geom_sf_text(aes(label = ecv), size = 6, color = \"black\", fontface = \"bold\") +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    theme_void() +\n    theme(legend.position = \"none\") + # Hide legend in Alaska inset\n    coord_sf(xlim = c(-180, -130), ylim = c(50, 72), expand = FALSE)\n\n  # Hawaii plot without legend\n  hawaii_plot &lt;- ggplot(hawaii) +\n    geom_sf(aes(fill = party_simplified)) +\n    geom_sf_text(aes(label = ecv), size = 6, color = \"black\", fontface = \"bold\") +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    theme_void() +\n    theme(legend.position = \"none\") + # Hide legend in Hawaii inset\n    coord_sf(xlim = c(-161, -154), ylim = c(18, 23), expand = FALSE)\n\n  # Combine plots using patchwork with adjusted insets on bottom left\n  combined_plot &lt;- mainland_plot +\n    inset_element(alaska_plot, left = 0.05, bottom = 0.1, right = 0.25, top = 0.3) + # Alaska on bottom left\n    inset_element(hawaii_plot, left = 0.05, bottom = 0.05, right = 0.25, top = 0.15) # Hawaii below Alaska on bottom left\n  # Save plot\n  \n   if (is.null(file_name)) {\n      ggsave(filename = paste0(\"mp03data/election_maps/election_map_\", year_num, \".png\"), plot = combined_plot, width = 16, height = 12, dpi = 300)\n   }else{\n     ggsave(filename = file_name, plot = combined_plot, width = 16, height = 12, dpi = 300)\n   }\n  \n}\n\nNow it efficiently generates an electoral map for a specified year, incorporating color-coded party winners, labels for small northeastern states, and Alaska and Hawaii as inset maps. This automation produces detailed visualizations of election results, making it ideal for presentations or reports on electoral data.\n\n\n\n\n\n\nTask 5: Chloropleth Visualization of the 2000 Presidential Election Electoral College Results\n\n\n\nI will now generate an electoral map for the 2000 presidential election. I have also included a step-by-step explanation of what each line does:\n\n# Calls the function with 2000 as the argument to generate data for the 2000 presidential election\nelection_year_data &lt;- create_election_year_data(2000)\n# The argument to create spatial data for each state in 2000 by combining congressional district geometries into state-level boundaries.\nelection_year_shp_data &lt;- create_state_shp_data(2000)\n# To create and save a map of the 2000 presidential election\ncreate_election_map(election_year_data, election_year_shp_data, 2000, \"election_map_2000.png\")\n# Display the Map in the Document\nknitr::include_graphics(\"election_map_2000.png\")\n\n\n\n\n\n\n\n\nThis map shows the results of the 2000 U.S. Presidential Election by state, indicating the winner in each state along with the electoral college votes (ECV) awarded Where Red: States won by the Republican candidate. Blue: States won by the Democratic candidate.\n\n\n\n\n\n\n\n\nTask 6: Advanced Chloropleth Visualization of Electoral College Results} ### 6. Modify your previous code to make either an animated faceted version showing election results over time. You may want to set facet_wrap or facet_grid to use a single column and adjust the figure size for the best reading experience\n\n\n\nHere I will created an animated, faceted map to visualize U.S. presidential election results over time. I prepared the election and geographic data for each election year from 1976 to 2020, merging shapefiles of U.S. states with election results to color each state by the winning party. Using facet_wrap, I created a one-column layout to display each year individually, while transition_manual provided smooth transitions between years for animation. Finally, I saved the animation as a GIF, adjusting plot settings to produce a clean, focused view of each year’s results, effectively illustrating shifts in electoral outcomes over time.\n\n# Define a function to generate and save an animated, faceted electoral map\ncreate_animated_facet_map &lt;- function(all_year_data) {\n  \n  # Combine data from all years into one data frame for animation\n  combined_data &lt;- bind_rows(all_year_data)\n  \n  # Check if the combined_data is in the correct format (as an sf object)\n  if (!inherits(combined_data, \"sf\")) {\n    combined_data &lt;- st_as_sf(combined_data)\n  }\n  \n  # Set up the plot using ggplot2 with faceting for each year\n  p &lt;- ggplot(combined_data) +\n    geom_sf(aes(fill = party_simplified), color = NA) +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    labs(\n      title = \"U.S. Presidential Election Results by State\",\n      fill = \"Winning Party\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n      legend.position = \"bottom\",\n      axis.text = element_blank(),         # Remove coordinate labels\n      axis.ticks = element_blank(),        # Remove axis ticks\n      panel.grid = element_blank()         # Remove grid lines\n    ) +\n    facet_wrap(~year, ncol = 1, scales = \"fixed\") +  # Single column, fixed scales\n    coord_sf(xlim = c(-125, -66), ylim = c(24, 50), expand = FALSE) +  # Adjusted limits for U.S.\n    transition_manual(year)\n\n  # Animate the plot with increased dimensions\n  animated_plot &lt;- animate(p, nframes = length(unique(combined_data$year)), fps = 1, width = 1000, height = 1200, res = 150)\n  \n  # Save the animation as a GIF\n  anim_save(\"election_results_facet_animation.gif\", animated_plot)\n}\n\n# Loop to prepare the data for each year and call the function to animate\nyears &lt;- seq(1976, 1978, by = 4)\nall_year_data &lt;- list()\n\nfor (year in years) {\n  # Assuming create_state_shp_data and create_election_year_data functions exist as per Task 5\n  shp_data_for_year &lt;- create_state_shp_data(year)\n  election_year_data &lt;- create_election_year_data(year)\n\n  # Combine shapefile data with election data\n  yearly_data &lt;- shp_data_for_year %&gt;%\n    left_join(election_year_data, by = \"state\") %&gt;%\n    mutate(year = year)  # Add year column for facetting\n\n  # Append yearly data to the list for later animation\n  all_year_data[[as.character(year)]] &lt;- yearly_data\n}\n\n# Call the animation function\n# create_animated_facet_map(all_year_data)\n\n# Display the animation\nknitr::include_graphics(\"election_results_facet_animation.gif\")\n\n\n\n\n\n\n\n\nResult shows the animated, faceted visualization highlights the shifting political landscape of the U.S. over several decades, providing a clear view of how electoral outcomes have changed across different presidential elections.\n\n\n\n\n\n\nTask 7: Evaluating Fairness of ECV Allocation Schemes\n\n\n\n\n7.1. State-Wide Winner-Take-All\nLet’s calculate State-Wide Winner-Take-All to examine which party receives all of a state’s ECVs based on a simple majority, we can understand potential biases in the current system and assess its representativeness.\n\n# Filter house_vote_data to only include presidential election years (every 4 years)\nhouse_vote_data_president_election_year &lt;- house_vote_data |&gt;\n  filter(year %% 4 == 0)\n\n# Filter presidential_election_data to only include presidential election years (every 4 years)\npresidential_election_data &lt;- presidential_election_data |&gt;\n  filter(year %% 4 == 0)\n\n# Deduct ECV data from the filtered congressional data and add DC manually\necv_data &lt;- house_vote_data_president_election_year |&gt;\n  filter(office == \"US HOUSE\") |&gt;\n  group_by(state, year) |&gt;\n  summarize(\n    num_representatives = n_distinct(district),\n    ecv = num_representatives + 2,\n    .groups = \"drop\"\n  ) |&gt;\n  bind_rows(data.frame(state = \"DISTRICT OF COLUMBIA\", year = unique(house_vote_data_president_election_year$year), ecv = 3))\n\n# Ensure candidate votes are numeric in presidential_election_data\npresidential_election_data &lt;- presidential_election_data |&gt;\n  mutate(candidatevotes = as.numeric(candidatevotes))\n\n# 1. State-Wide Winner-Take-All Allocation\nwinner_take_all &lt;- presidential_election_data |&gt;\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  left_join(ecv_data, by = c(\"state\", \"year\")) |&gt;\n  mutate(ecv_allocation = ecv, allocation_scheme = \"State-Wide Winner-Take-All\") |&gt;\n  select(year, state, party_simplified, ecv_allocation, allocation_scheme)\n\nwinner_take_all_summary &lt;- winner_take_all |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ecv_allocation), .groups = \"drop\")\n\n# I am sorting and searching\nwinner_take_all_summary |&gt;\n  DT::datatable()\n\n\n\n\n\nThe table displayed here shows the results of the State-Wide Winner-Take-All allocation system for each U.S. presidential election year. The fluctuations in total ECVs show the changing dominance of parties over time and how closely contested certain election years were.\n\n\n7.2. District-Wide Winner-Take-All + State-Wide “At Large” Votes\nlet’s calculate District-Wide Winner-Take-All + State-Wide “At Large” Votes. This will help to evaluate how district-level and at-large ECV allocations could impact the distribution of ECVs in U.S. presidential elections compared to a purely winner-take-all state system.\n\n# 1. Calculate District-Level ECV Allocation for Each State (excluding DC since it has no districts)\ndistrict_winner_take_all &lt;- house_vote_data_president_election_year |&gt;\n  filter(office == \"US HOUSE\", state != \"DISTRICT OF COLUMBIA\") |&gt; # Only states with congressional districts\n  group_by(year, state, district) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes in each district\n  ungroup() |&gt;\n  mutate(ecv_allocation = 1) |&gt; # Each district winner receives 1 ECV\n  mutate(party_simplified = party)\n\n# 2. Calculate Statewide \"At Large\" ECV Allocation for Each State\nat_large_votes &lt;- presidential_election_data |&gt;\n  filter(state != \"DISTRICT OF COLUMBIA\") |&gt; # Exclude DC here for now\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes statewide\n  ungroup() |&gt;\n  mutate(ecv_allocation = 2) # Statewide winner receives 2 \"at-large\" ECVs\n\n# 3. Add DC’s Fixed 3 ECVs with Winner-Take-All Allocation\ndc_winner_take_all &lt;- presidential_election_data |&gt;\n  filter(state == \"DISTRICT OF COLUMBIA\") |&gt; # Only DC\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes in DC\n  ungroup() |&gt;\n  mutate(ecv_allocation = 3) # DC winner receives all 3 ECVs\n\n# 4. Combine District-Level, Statewide At-Large, and DC Allocations for All States\nnationwide_district_allocation &lt;- bind_rows(district_winner_take_all, at_large_votes, dc_winner_take_all) |&gt;\n  mutate(allocation_scheme = \"Nationwide District-Wide Winner-Take-All + At-Large\") |&gt;\n  select(year, state, district, party_simplified, ecv_allocation, allocation_scheme)\n\n# Summarize the results by year and party to see the national impact\ndistrict_allocation_summary &lt;- nationwide_district_allocation |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ecv_allocation), .groups = \"drop\") |&gt;\n  arrange(year, desc(total_ecv))\n\n# Display the national summary to see the overall outcome by party\ndistrict_allocation_summary |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n7.3. State-Wide Proportional Allocation\nThis code calculates Electoral College Votes (ECV) using a State-Wide Proportional Allocation. We first determine each party’s vote share within each state, then allocate ECVs proportionally. Afterward, we adjust for any rounding differences to ensure each state’s total ECV is correct. Finally, we summarize the results nationally to compare total ECVs for each party in each election year, offering a fairer reflection of each party’s support within states.\n\n# Join proportional_state with ecv_data to get total ECVs for each state-year combination\nproportional_state &lt;- presidential_election_data |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarize(total_party_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  left_join(ecv_data, by = c(\"state\", \"year\")) |&gt;\n  group_by(year, state) |&gt;\n  mutate(\n    vote_share = total_party_votes / sum(total_party_votes), # Calculate each party's vote share\n    proportional_ecv = round(vote_share * ecv), # Proportionally allocate ECVs\n    allocation_scheme = \"State-Wide Proportional\"\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, state, party_simplified, ecv, proportional_ecv, allocation_scheme)\n\n# Adjust to ensure each state's total ECV allocation matches exactly\nproportional_state_adjusted &lt;- proportional_state |&gt;\n  group_by(year, state) |&gt;\n  mutate(\n    adjustment = ecv - sum(proportional_ecv), # Calculate any rounding difference\n    proportional_ecv = if_else(row_number() == 1, proportional_ecv + adjustment, proportional_ecv) # Adjust the first row\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, state, party_simplified, proportional_ecv, allocation_scheme)\n\n# Summarize the results by year and party to see the national impact\nproportional_state_summary &lt;- proportional_state_adjusted |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(proportional_ecv), .groups = \"drop\") |&gt;\n  arrange(year, desc(total_ecv))\n\n# Display the national summary to see the overall outcome by party\nproportional_state_summary |&gt;\n  DT::datatable()\n\n\n\n\n\nThe State-Wide Proportional Allocation method more accurately reflects voter support by allocating Electoral College Votes based on each party’s share of votes within each state.\n\n\n7.4. National Proportional\nThis code allocates Electoral College Votes (ECVs) based on each party’s national vote share. It calculates the total votes per party each year, determines each party’s share of the national vote, and then allocates ECVs proportionally.\n\n# Total ECVs for the nation\ntotal_ecv_nationwide &lt;- 538 # Includes 3 for DC\n\n# Step 1: Calculate the national vote totals for each candidate per year\nnational_vote_totals &lt;- presidential_election_data |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(national_votes = sum(candidatevotes), .groups = \"drop\")\n\n# Step 2: Calculate each candidate's national vote share\nnational_vote_shares &lt;- national_vote_totals |&gt;\n  group_by(year) |&gt;\n  mutate(\n    national_vote_share = national_votes / sum(national_votes) # Calculate national vote share\n  ) |&gt;\n  ungroup()\n\n# Step 3: Allocate ECVs based on national vote share and adjust for rounding differences\nnational_proportional_ecv &lt;- national_vote_shares |&gt;\n  mutate(\n    proportional_ecv = round(national_vote_share * total_ecv_nationwide), # Allocate ECVs proportionally\n    allocation_scheme = \"National Proportional\"\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    adjustment = total_ecv_nationwide - sum(proportional_ecv), # Calculate any rounding difference\n    total_ecv = if_else(row_number() == 1, proportional_ecv + adjustment, proportional_ecv) # Adjust the first row\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, party_simplified, total_ecv, allocation_scheme)\n\n\n# Summarize the results by year to see the national distribution by party\nnational_proportional_summary &lt;- national_proportional_ecv |&gt;\n  arrange(year, desc(total_ecv))\n\n# Display the national summary\nnational_proportional_summary |&gt;\n  DT::datatable()\n\n\n\n\n\nThe result is a table showing ECV distribution by party and year under a proportional system, aiming to reflect popular support more accurately.\n\n\nBased on these allocation strategies, compare the winning presidential candidate with the actual historical winner.\nlet’s identify the historical Electoral College winner for each U.S. presidential election year based on actual state-wide, winner-take-all allocations.\n\nstate_wide_winners &lt;- presidential_election_data |&gt;\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes in each state\n  ungroup()\n\n# Step 2: Join with `ecv_data` to get ECVs for each state-year combination\nstate_wide_winners_with_ecv &lt;- state_wide_winners |&gt;\n  left_join(ecv_data, by = c(\"state\", \"year\")) |&gt; # Add ECVs from `ecv_data`\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ecv), .groups = \"drop\") # Sum ECVs per candidate per year\n\n# Step 3: Identify the actual historical winner for each year\nhistorical_winner &lt;- state_wide_winners_with_ecv |&gt;\n  group_by(year) |&gt;\n  slice_max(total_ecv, with_ties = FALSE) |&gt; # Get the candidate with the most ECVs in each year\n  ungroup() |&gt;\n  rename(actual_winner = party_simplified, actual_ecv = total_ecv) |&gt;\n  select(year, actual_winner, actual_ecv)\n\n# Display the historical winners to verify\nhistorical_winner |&gt;\n  DT::datatable()\n\n\n\n\n\nThis output table shows the actual winning party and ECV total per election year, reflecting the historical outcomes in the Electoral College system.\n\n\nConsolidate Results from All Allocation Strategies\nNow,let’s evaluate the consistency of various Electoral College allocation strategies by comparing the winning party in each strategy with the historical winner.\n\n# Combine all strategies into a single dataset and find the winner for each\nwinners_by_strategy &lt;- bind_rows(\n  winner_take_all_summary |&gt; mutate(strategy = \"State-Wide Winner-Take-All\"),\n  district_allocation_summary |&gt; mutate(strategy = \"District-Wide Winner-Take-All + At-Large\"),\n  proportional_state_summary |&gt; mutate(strategy = \"State-Wide Proportional\"),\n  national_proportional_summary |&gt; mutate(strategy = \"National Proportional\")\n) |&gt;\n  group_by(year, strategy) |&gt;\n  slice_max(total_ecv, with_ties = FALSE) |&gt; # Select candidate with the most ECVs per strategy\n  ungroup() |&gt;\n  select(year, strategy, party_simplified, total_ecv) |&gt;\n  rename(winning_party = party_simplified, winning_ecv = total_ecv)\n\n# 4. Compare Each Strategy’s Winner with the Actual Historical Winner\ncomparison_results &lt;- winners_by_strategy |&gt;\n  left_join(historical_winner, by = \"year\") |&gt; # Join with the actual historical winner data\n  mutate(\n    match_with_actual = if_else(winning_party == actual_winner, \"Match\", \"Different\")\n  ) |&gt;\n  select(year, strategy, winning_party, winning_ecv, actual_winner, actual_ecv, match_with_actual)\n\n\n# 5. Summarize Results by Strategy\nsummary_results &lt;- comparison_results |&gt;\n  group_by(strategy, match_with_actual) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = match_with_actual, values_from = count, values_fill = 0)\n\nsummary_results |&gt;\n  DT::datatable()\n\n\n\n\n\nThe table shows that the State-Wide Winner-Take-All strategy matches historical results exactly with 12 matches and no differences. In contrast, the District-Wide Winner-Take-All + At-Large, National Proportional, and State-Wide Proportional strategies each show 10 matches and 2 differences, indicating that these alternative methods could have changed the election outcome in 2 instances.\n\n\nCreate a comparison table that shows the winner for each strategy and the actual historical winner\nThis code creates a comparison table that shows the actual historical election winner alongside the winners predicted by each voting strategy (Winner-Take-All, District-Wide + At-Large, State-Wide Proportional, and National Proportional) for each year. It reshapes the data, joins it with historical results, renames columns for clarity, and displays it as an interactive table, allowing easy side-by-side comparison of each strategy’s outcomes against actual results.\n\n# Step 1: Reshape the winners_by_strategy to have one row per year with each strategy's winner as separate columns\nstrategy_winners_table &lt;- winners_by_strategy |&gt;\n  pivot_wider(\n    names_from = strategy,\n    values_from = c(winning_party, winning_ecv),\n    names_glue = \"{strategy}_{.value}\"\n  )\n\n# Step 2: Join with the actual historical winner\ncomparison_table &lt;- historical_winner |&gt;\n  left_join(strategy_winners_table, by = \"year\") |&gt;\n  select(\n    year,\n    actual_winner,\n    `State-Wide Winner-Take-All_winning_party`,\n    `District-Wide Winner-Take-All + At-Large_winning_party`,\n    `State-Wide Proportional_winning_party`,\n    `National Proportional_winning_party`\n  ) |&gt;\n  rename(\n    `Actual Winner` = actual_winner,\n    `Winner-Take-All` = `State-Wide Winner-Take-All_winning_party`,\n    `District-Wide + At-Large Winner` = `District-Wide Winner-Take-All + At-Large_winning_party`,\n    `State-Wide Proportional` = `State-Wide Proportional_winning_party`,\n    `National Proportional` = `National Proportional_winning_party`,\n  )\n\n# Display the comparison table\ncomparison_table |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFact Check\n\n\n\nOverall, the National proportional strategy seems to be most fair. It would represent will of every American Citizen and remove unfair advantages smaller states have. Politicians would have to try to gather votes from every Citizen rather than just few swing states. Other approach I would have liked to analyze is Ranked choice voting but we do not have data to estimate how the result would have been like.\n\n\n\n\n\n\n\n\nExtra Credit: Advanced Chloropleth Visualization of Electoral College Results Modify your previous code to make either an animated version showing election results over time.\n\n\n\nI couldn’t generate gif from the data at once. So instead, I created multiple images for each year and combined them to form a gif. This approach allows visualizing how electoral results have changed over time, highlighting shifts in political dominance across states, trends in voting patterns, and potential changes in electoral maps due to population changes or other factors. This GIF offers an engaging, visual summary of historical election data.\n\nlibrary(magick)\n\n# Define election years\nyears &lt;- seq(1976, 1978, by = 4)\n\n# Now iterate over each year and retrieve the file location\nfor (year in years) {\n  shp_data_for_year &lt;- create_state_shp_data(year)\n  election_year_data &lt;- create_election_year_data(year)\n\n  yearly_data &lt;- shp_data_for_year |&gt;\n    inner_join(election_year_data, by = \"state\")\n\n  # print(yearly_data)\n  create_election_map(election_year_data, shp_data_for_year, year)\n}\n\n# Load all PNG images and create a GIF\nimages &lt;- list.files(path = \"mp03data/election_maps\", pattern = \"*.png\", full.names = TRUE)\n\ngif &lt;- image_read(images) # Read all images\n\n# Animate the images with desired frame rate (e.g., 1 frame per second)\nanimated_gif &lt;- image_animate(gif, fps = 1)\n\n# Save the animated GIF\n# image_write(animated_gif, \"election_map_animation.gif\")\nknitr::include_graphics(\"election_map_animation.gif\")\n\n\n\n\n\n\n\n\nThis animated election map reflects Advanced Chloropleth Visualization of Electoral College Results Modify your previous code to make either an animated version showing election results over time.\n\n\n\nReflecting on my experience with this project:\nAs someone new to the U.S. election system and experiencing an election here for the first time, working on this project was both eye-opening and challenging. I had to research a lot to understand the Electoral College and how it shapes election results. At times, it felt overwhelming, but watching the real election while working on this helped me connect more deeply with the data. This real-time context gave me a personal perspective on the process, making the project not just an academic task but a truly insightful and memorable experience."
  }
]