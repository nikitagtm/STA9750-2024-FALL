[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Transit Data Analysis",
    "section": "",
    "text": "This analysis will examine the fiscal characteristics of major U.S. public transit systems using publicly available data. For more details on the problem description, please refer to: Mini-Project #01. The primary objective is to answer key questions related to transit agencies, focusing on areas such as farebox recovery performance, ridership trends, and operating expenses. The analysis will involve tasks such as renaming columns, recoding modes, and addressing instructor-specified questions using various transit data sources."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Transit Data Analysis",
    "section": "",
    "text": "Install Required Packages\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nif(!require(\"lubridate\")) install.packages(\"lubridate\")\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\n\n\n\nLoad the packages\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\n\n\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\n\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Agency, City, State, NTD ID, Organization Type, Reporter Type, UZA...\ndbl  (2): Report Year, UACE Code\nnum (10): Primary UZA Population, Agency VOMS, Mode VOMS, Vehicle Operations...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n            group_by(`NTD ID`, `Agency`, `UZA Name`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'UZA Name', 'Mode', '3\nMode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, `UZA Name`, Mode, `3 Mode`,\nmonth)`"
  },
  {
    "objectID": "mp01.html#library-setup",
    "href": "mp01.html#library-setup",
    "title": "Transit Data Analysis",
    "section": "Library Setup",
    "text": "Library Setup\n\nInstall Required Packages\nWe will be analyzing various data from various sources. Following libraries are needed for this analysis. First check if the library is already installed and then install if not installed.\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\n\n\nLoad the packages\nOnce the packages are installed, those will be loaded to the workspace so that they can be used later.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)"
  },
  {
    "objectID": "mp01.html#load-data",
    "href": "mp01.html#load-data",
    "title": "Transit Data Analysis",
    "section": "Load Data",
    "text": "Load Data\nSince we have now setup libraries, we will now download the data to our project so that we can use later fo our analysis. You might get an error when trying to download the file programmatically. If the error persists, download the files manually and rename those and copy them to project folder.\n\nLoading Fare Revenue Data\nWe will first Load Fare revenue data from 2022 Fare Revenue table. This table Contains data on revenues a transit agency earns from carrying passengers, organized by mode and type of service. Reported as funds earned, funds expended on operations, and funds expended on capital.\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif (!file.exists(\"2022_fare_revenue.xlsx\")) {\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n  # directory.\n  download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\",\n    destfile = \"2022_fare_revenue.xlsx\",\n    quiet = FALSE,\n    method = \"wget\"\n  )\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(\n    -`State/Parent NTD ID`,\n    -`Reporter Type`,\n    -`Reporting Module`,\n    -`TOS`,\n    -`Passenger Paid Fares`,\n    -`Organization Paid Fares`\n  ) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(\n    `NTD ID`, # Sum over different `TOS` for the same `Mode`\n    `Agency Name`, # These are direct operated and sub-contracted\n    `Mode`\n  ) |&gt; # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n\n\nNext load Expenses\nThe 2022 Annual dataset containing data on expenses applied to operate public transportation services for each agency, by mode, and type of service operated. Divides expenses among NTD expense functions and object classes.\n\n# Next, expenses\nif (!file.exists(\"2022_expenses.csv\")) {\n  # This should work _in theory_ but in practice it's still a bit finicky\n  # If it doesn't work for you, download this file 'by hand' in your\n  # browser and save it as \"2022_expenses.csv\" in your project\n  # directory.\n  download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\",\n    destfile = \"2022_expenses.csv\",\n    quiet = FALSE,\n    method = \"wget\"\n  )\n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(\n    `NTD ID`,\n    `Agency`,\n    `Total`,\n    `Mode`\n  ) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()"
  },
  {
    "objectID": "mp01.html#tasks",
    "href": "mp01.html#tasks",
    "title": "Transit Data Analysis",
    "section": "Tasks",
    "text": "Tasks\nNow, we will complete the tasks mentioned in this page\n\n\n\n\n\n\nTask 1 - Creating Syntatic Names\n\n\n\nRename a column: UZA Name to metro_area.\n\nUSAGE &lt;- USAGE |&gt; rename(metro_area = \"UZA Name\")\n\nWe will also rename few other columns to make them more readable\n\nUSAGE &lt;- USAGE |&gt;\n  rename(Passenger_Trips = UPT, Vehicle_Miles = VRM)\n\n\n\n\n\n\n\n\n\nTask 2: Recoding the Mode column\n\n\n\nFind Unique Modes and Print.\n\nunique_modes &lt;- USAGE |&gt;\n  distinct(Mode)\n\nprint(unique_modes)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\nNow we will get the meaning of these symbols from NDT website. Once we have the meaning for each Acronyms, we will replace using case-when.\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Motorbus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\n\n\n\n\n\n\n\nTask 3: Answering Instructor Specified Questions with dplyr\n\n\n\n\n1. What transit agency had the most total VRM in this sample?\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_VRM = sum(Vehicle_Miles, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_VRM)) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Total_VRM\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n2. What transit mode had the most total VRM in this sample?\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(Vehicle_Miles, na.rm=TRUE)) |&gt;\n  arrange(desc(Total_VRM)) |&gt;\n  datatable(options = list(pageLength = 1, dom = 't'),  # Only display top row\n          rownames = FALSE) |&gt; \n  formatRound(\"Total_VRM\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n\ntotal_trips &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month == \"2024-05-01\") |&gt;\n  summarize(Total_Trips = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  pull(Total_Trips)\n\nmessage &lt;- sprintf(\n  \"There were %s trips taken on the NYC Subway (Heavy Rail) in May 2024.\",\n  format(total_trips, big.mark = \",\")\n)\n\ncat(message)\n\nThere were 180,458,819 trips taken on the NYC Subway (Heavy Rail) in May 2024.\n\n\n\n\n5. How much did NYC subway ridership fall between April 2019 and April 2020?\nTo solve this, we will first find ridership for 2019 and 2020 separately. Then we will subtract to get change and get percentage.\n\n# Filter and summarize data for April 2019\napril_2019 &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month == \"2019-04-01\") |&gt;\n  summarize(Total_Trips_2019 = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  pull(Total_Trips_2019)\n\n# Filter and summarize data for April 2020\napril_2020 &lt;- USAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\", Mode == \"Heavy Rail\", month == \"2020-04-01\") |&gt;\n  summarize(Total_Trips_2020 = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  pull(Total_Trips_2020)\n\n# Calculate the absolute difference and percentage drop\nridership_difference &lt;- april_2019 - april_2020\npercentage_drop &lt;- (ridership_difference / april_2019) * 100\n\n# Print the custom message with the result and percentage drop\nmessage &lt;- sprintf(\n  \"NYC subway ridership fell by %s trips between April 2019 and April 2020, which is a %.2f%% decrease.\",\n  format(ridership_difference, big.mark = \",\"), percentage_drop\n)\n\ncat(message)\n\nNYC subway ridership fell by 211,969,660 trips between April 2019 and April 2020, which is a 91.28% decrease.\n\n\n\n\n\n\n\n\n\n\n\nTask 4: Explore and Analyze\n\n\n\nFind three more interesting transit facts in this data other than those above.\n\n1. Top 5 Transit Agencies by Total Passenger Trips\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_Trips = sum(Passenger_Trips, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_Trips)) |&gt;\n  head(5) |&gt;\n  datatable(options = list(pageLength = 5, dom = \"t\"), rownames = FALSE) |&gt;\n  formatRound(\"Total_Trips\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n2. Top 5 Transit Modes by Total Vehicle Miles\n\ntop_modes_vrm &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(Vehicle_Miles, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_VRM)) |&gt;\n  head(5)\n\ndatatable(top_modes_vrm, options = list(pageLength = 5, dom = \"t\"), rownames = FALSE) |&gt;\n  formatRound(\"Total_VRM\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n3. Top 5 Agencies with Largest Decrease in Ridership between 2019 and 2020\n\nridership_decline &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(\n    Trips_2019 = sum(ifelse(month == \"2019-04-01\", Passenger_Trips, NA), na.rm = TRUE),\n    Trips_2020 = sum(ifelse(month == \"2020-04-01\", Passenger_Trips, NA), na.rm = TRUE)\n  ) |&gt;\n  mutate(Decline = Trips_2019 - Trips_2020) |&gt;\n  arrange(desc(Decline)) |&gt;\n  head(5)\n\ndatatable(ridership_decline, options = list(pageLength = 5, dom = \"t\"), rownames = FALSE) |&gt;\n  formatRound(c(\"Trips_2019\", \"Trips_2020\", \"Decline\"), digits = 0, mark = \",\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 5: Table Summarization\n\n\n\nCreate a new table from USAGE that has annual total (sum) UPT and VRM for 2022. This will require use of the group_by, summarize, and filter functions. You will also want to use the year function, to extract a year from the month column.\nThe resulting table should have the following columns:\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\n\nMake sure to ungroup your table after creating it.\nName this table USAGE_2022_ANNUAL.\nThis will be done with following command.\n\n# Create the USAGE_2022_ANNUAL table\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  # Extract the year from the month column\n  mutate(year = year(month)) |&gt;\n  # Filter for the year 2022\n  filter(year == 2022) |&gt;\n  # Group by the necessary columns\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt;\n  # Summarize total UPT and VRM for the year\n  summarize(UPT = sum(Passenger_Trips, na.rm = TRUE), VRM = sum(Vehicle_Miles, na.rm = TRUE)) |&gt;\n  # Ungroup the table\n  ungroup()\n\nLet’s verify that all the columns are there.\n\ncat(colnames(USAGE_2022_ANNUAL), sep = \"\\n\")\n\nNTD ID\nAgency\nmetro_area\nMode\nUPT\nVRM\n\n\n\n\nNow, Let’s join with FINANCIALS to create single table and create USAGE_AND_FINANCIALS. However, before joining, let’s view the table Financials:\n\nFINANCIALS |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nHere we notice that Mode is Acronym. However, USAGE_2022_ANNUAL doesn’t have acronyms. To make sure we can join both the table, let’s change FINANCIALS mode to match USAGE_2022_ANNUAL.\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Motorbus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nNow, let’s join to create USAGE_AND_FINANCIALS. We will join NTD ID and Mode as they are present in both the tables to create USAGE_AND_FINANCIALS.\n\n USAGE_AND_FINANCIALS &lt;- left_join(\n  USAGE_2022_ANNUAL,\n  FINANCIALS,\n  join_by(`NTD ID`, Mode)\n) |&gt;\n  drop_na()\n\nLet’s view few records to make sure we have them:\n\nUSAGE_AND_FINANCIALS |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nBefore we answer the questions, we will rename few columns to make them more readable:\n\nUSAGE_AND_FINANCIALS &lt;- USAGE_AND_FINANCIALS |&gt;\n  rename(Passenger_Trips = UPT, Vehicle_Miles = VRM)\n\n\n\n\n\n\n\nTask 6: Farebox Recovery Among Major Systems\n\n\n\nUsing the USAGE_AND_FINANCIALS table, answer the following questions:\n\n1. Which transit system (agency and mode) had the most UPT in 2022?\n\n USAGE_AND_FINANCIALS |&gt;\n  select(Agency, Mode, Passenger_Trips) |&gt;\n  arrange(desc(Passenger_Trips)) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Passenger_Trips\", digits = 0, mark = \",\")\n\n\n\n\n\n\n\n2. Which transit system (agency and mode) had the highest farebox recovery (Total Fares to Expenses)?\n\n    USAGE_AND_FINANCIALS |&gt;\n  mutate(Farebox_Recovery = `Total Fares` / Expenses) |&gt;\n  filter(!is.na(`Expenses`) &`Expenses`&gt;0) |&gt;\n  arrange(desc(Farebox_Recovery)) |&gt;\n  select(Agency, Mode, Farebox_Recovery) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Farebox_Recovery\", mark = \",\")\n\n\n\n\n\n\n\n3 Which transit system (agency and mode) has the lowest expenses per UPT?\n\n  USAGE_AND_FINANCIALS |&gt;\n  mutate(Expenses_per_UPT = Expenses / Passenger_Trips) |&gt;\n  arrange(Expenses_per_UPT) |&gt;\n  select(Agency, Mode, Expenses_per_UPT) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Expenses_per_UPT\", mark = \",\")\n\n\n\n\n\n\n\n4. Which transit system (agency and mode) has the highest total fares per UPT?\n\n  USAGE_AND_FINANCIALS |&gt;\n   mutate(Fares_per_UPT = `Total Fares` / Passenger_Trips) |&gt;\n   arrange(desc(Fares_per_UPT)) |&gt;\n   select(Agency, Mode, Fares_per_UPT) |&gt;\n   datatable(\n     options = list(pageLength = 1, dom = \"t\"), # Only display top row\n     rownames = FALSE\n   ) |&gt;\n   formatRound(\"Fares_per_UPT\", mark = \",\")\n\n\n\n\n\n\n\n5. Which transit system (agency and mode) has the lowest expenses per VRM?\n\n  USAGE_AND_FINANCIALS |&gt;\n   mutate(Expenses_per_VRM = Expenses / Vehicle_Miles) |&gt;\n   arrange(Expenses_per_VRM) |&gt;\n   select(Agency, Mode, Expenses_per_VRM) |&gt;\n   datatable(\n     options = list(pageLength = 1, dom = \"t\"), # Only display top row\n     rownames = FALSE\n   ) |&gt;\n   formatRound(\"Expenses_per_VRM\", mark = \",\")\n\n\n\n\n\n\n\n6. Which transit system (agency and mode) has the highest total fares per VRM?\n\n  USAGE_AND_FINANCIALS |&gt;\n  mutate(Fares_per_VRM = `Total Fares` / Vehicle_Miles) |&gt;\n  arrange(desc(Fares_per_VRM)) |&gt;\n  select(Agency, Mode, Fares_per_VRM) |&gt;\n  datatable(\n    options = list(pageLength = 1, dom = \"t\"), # Only display top row\n    rownames = FALSE\n  ) |&gt;\n  formatRound(\"Fares_per_VRM\", mark = \",\")"
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Transit Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn my view, the Transit Authority of Central Kentucky’s Vanpool stands out as the most efficient transit system due to its farebox recovery ratio exceeding 100%, meaning it generates more fare revenue than its operating costs. This high level of financial self-sufficiency is uncommon in public transit and makes it highly efficient from a financial sustainability perspective.\nOverall, this was an interesting assignment to understand basics DT operations using Transportation data. This analysis provided valuable insights into the financial and operational performance of different transit agencies and modes."
  },
  {
    "objectID": "mp01.html#key-points-from-this-analysis",
    "href": "mp01.html#key-points-from-this-analysis",
    "title": "Transit Data Analysis",
    "section": "Key points from this analysis",
    "text": "Key points from this analysis\n\nMTA New York City Transit had the most passenger trips in 2022.\nNYC Subway saw a 91.28% drop in ridership between April 2019 and April 2020 due to COVID-19.\nTransit Authority of Central Kentucky’s Vanpool had the highest farebox recovery, covering costs effectively through fare revenue.\nNorth Carolina State University’s Motorbus service had the lowest expenses per passenger trip.\nThe Motorbus mode, across all agencies, is the one that collects the most fare revenue for every mile the buses travel while carrying passengers.\nThe transit system with the highest total fares per Vehicle Revenue Mile is the Chicago Water Taxi in the Ferryboat mode."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini Project 02",
    "section": "",
    "text": "IMPD PROJECT\n\n\nIntroduction\nThe goal of this project is to leverage data-driven insights to identify the key characteristics of successful movies and develop a compelling proposal for a new film. By analyzing historical IMDb data on movie ratings, genres, and key personnel, we aim to guide creative decisions with statistical evidence, ultimately proposing a high-potential movie idea that aligns with current industry trends and audience preferences.please refer to: Mini-Project #02\n\n\nData Sources\nData from the The IMDb non-commercial release can be used for this project. Specifically, which are made freely available for non-commercial use, provide comprehensive information about films, including ratings, genres, and key personnel, allowing for analysis of historical trends and the development of a data-driven movie proposal.\nThe IMDb dataset was initially too large, so for this project, a pre-processed and downsized version provided by the professor on GitHub. This version retains the essential information but is optimized for more manageable data analysis in R, allowing us to run the necessary tasks more efficiently without overloading system resources.\n\n\nData Description\nThe data includes comprehensive information about movies, such as ratings, genres, and key personnel. The dataset consists of multiple tables:\nname_basics_small: Contains information about actors, directors, and other personnel.\ntitle_basics_small: Provides basic movie details like title, genre, and release year.\ntitle_ratings_small: Contains IMDb user ratings and vote counts for movies.\ntitle_crew_small: Includes data on directors and writers for each title.\ntitle_principals_small: Details about the key actors and their roles in each movie.\ntitle_episodes_small: Contains data on TV episodes related to series.\n\n\nLoading Packages\nOnce the packages are installed, those will be loaded to the workspace so that they can be used later.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(tidyr)\nlibrary(data.table)\n\n\n\nLoad Data\nUsing the following code, I manage to download and process of the IMDb datasets, including name.basics, title.basics, title.episode, title.ratings, title.crew, and title.principals.\n\nget_imdb_file &lt;- function(fname) {\n  BASE_URL &lt;- \"https://github.com/michaelweylandt/STA9750/tree/main/miniprojects/mini02_preprocessed/\"\n  fname_ext &lt;- paste0(fname, \".csv.zip\")\n  rds_file &lt;- paste0(fname, \".rds\")\n\n  # Check if the .rds file already exists\n  if (file.exists(rds_file)) {\n    return(readRDS(rds_file)) # Load the data from the saved .rds file\n  } else {\n    # Download only if the .tsv.gz file does not exist\n    if (!file.exists(fname_ext)) {\n      FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n      download.file(FILE_URL, destfile = fname_ext, mode = \"wb\") # Ensure binary mode for downloading compressed files\n    }\n    \n    # Use data.table::fread for faster reading\n    df &lt;- as.data.frame(fread(fname_ext))\n    saveRDS(df, rds_file, compress = FALSE)\n    return(df)\n  }\n}\n\n\n# Load IMDb data\nNAME_BASICS &lt;- get_imdb_file(\"name_basics_small\")\nTITLE_BASICS &lt;- get_imdb_file(\"title_basics_small\")\nTITLE_RATINGS &lt;- get_imdb_file(\"title_ratings_small\")\nTITLE_EPISODES &lt;- get_imdb_file(\"title_episodes_small\")\nTITLE_CREW &lt;- get_imdb_file(\"title_crew_small\")\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title_principals_small\")\n\n\n\nData Sampling\nGiven the size of the data, we began by down-selecting to create a more manageable dataset for analysis. We will only select actors who are known for more than 1 movie/show.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\n\n\nData Visualization\nWe visualize key metrics to gain insights. For example, we create histograms of IMDb ratings to see the distribution of ratings across movies.\n\nTITLE_RATINGS |&gt;\n  ggplot(aes(x = numVotes)) +\n  geom_histogram(bins = 30) +\n  xlab(\"Number of IMDB Ratings\") +\n  ylab(\"Number of Titles\") +\n  ggtitle(\"Majority of IMDB Titles Have Less than 1000 Ratings\") +\n  theme_bw() +\n  scale_x_log10(label = scales::comma) +\n  scale_y_continuous(label = scales::comma)\n\n\n\n\n\n\n\n\n\n\nData Cleaning\nNow, let’s filter and join the IMDb datasets, correct column types, and clean the data for analysis by ensuring numeric and logical fields are properly formatted.\n\nTITLE_RATINGS |&gt;\n  pull(numVotes) |&gt;\n  quantile()\n\n     0%     25%     50%     75%    100% \n    100     165     332     970 2942823 \n\n\nNow, let’s only keep records that have atleast 100 records\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  filter(numVotes &gt;= 100)\n\nNow, semi_join will be used to keep records that are present in title_rating. This will help will reducing the size of data.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(parentTconst == tconst)\n  )\n\nTITLE_EPISODES &lt;- bind_rows(\n  TITLE_EPISODES_1,\n  TITLE_EPISODES_2\n) |&gt;\n  distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\n# Following are not used. Removing\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\n\nData Cleaning before Task 1\nWe use mutate and as.numeric to convert string values in the NAME_BASICS birthYear and deathYear columns to numeric format for proper analysis.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(\n    birthYear = as.numeric(birthYear),\n    deathYear = as.numeric(deathYear)\n  )\n\n# Let's view data now. \nglimpse(NAME_BASICS)\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\n\n\n\n\n\n\nTask 1: Column Type Correction\n\n\n\nCorrect the column types of the TITLE tables using a combination of mutate and the coercion functions as.numeric and as.logical.\nNow, we will examine the dataset using glimpse and clean the data by converting columns in TITLE_BASICS and TITLE_EPISODES to numeric and boolean types, and split knownForTitles in NAME_BASICS for further analysis.\n\nglimpse(TITLE_BASICS)\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;chr&gt; \"1894\", \"1892\", \"1892\", \"1892\", \"1893\", \"1894\", \"1894\",…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nAs we can see, isAdult can be boolean. Startyear, endYear, runTimeMinutes can be numbers.\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    startYear = as.numeric(startYear),\n    endYear = as.numeric(endYear),\n    runtimeMinutes = as.numeric(runtimeMinutes),\n    isAdult = as.logical(isAdult)\n  )\n\nNow, let’s do some data cleaning for other tables. Starting with TITLE_EPISODES\n\nglimpse(TITLE_EPISODES)\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nIt can be seen that seasonNumber and Episode Number are numbers. Converting those:\n\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(\n    seasonNumber = as.numeric(seasonNumber),\n    episodeNumber = as.numeric(episodeNumber)\n  )\n\nNAME_BASICS will be analyzed next.\n\nglimpse(NAME_BASICS)\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nIt can be seen that knownForTitles column contains multiple comma separated records. We can use the separate_longer_delim function to break these into multiple rows.\n\nNAME_BASICS |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  slice_head(n = 10)\n\n      nconst     primaryName birthYear deathYear\n1  nm0000001    Fred Astaire      1899      1987\n2  nm0000001    Fred Astaire      1899      1987\n3  nm0000001    Fred Astaire      1899      1987\n4  nm0000001    Fred Astaire      1899      1987\n5  nm0000002   Lauren Bacall      1924      2014\n6  nm0000002   Lauren Bacall      1924      2014\n7  nm0000002   Lauren Bacall      1924      2014\n8  nm0000002   Lauren Bacall      1924      2014\n9  nm0000003 Brigitte Bardot      1934        NA\n10 nm0000003 Brigitte Bardot      1934        NA\n                    primaryProfession knownForTitles\n1        actor,miscellaneous,producer      tt0072308\n2        actor,miscellaneous,producer      tt0050419\n3        actor,miscellaneous,producer      tt0053137\n4        actor,miscellaneous,producer      tt0027125\n5  actress,soundtrack,archive_footage      tt0037382\n6  actress,soundtrack,archive_footage      tt0075213\n7  actress,soundtrack,archive_footage      tt0117057\n8  actress,soundtrack,archive_footage      tt0038355\n9   actress,music_department,producer      tt0057345\n10  actress,music_department,producer      tt0049189\n\n\n\n\n\n\n\n\n\n\nTask 2: Instructor-Provided Questions\n\n\n\n\n2.1. How many movies, TV series, and TV episodes are in the data set?\nWe calculate the total number of movies, TV series, and TV episodes after removing records with less than 100 ratings.\n\n# Count movies, TV series, and TV episodes\nresult &lt;- TITLE_BASICS |&gt;\n  group_by(titleType) |&gt;\n  summarise(count = n())\n\n# Now lets make a message\nmessage &lt;- sprintf(\n  \"The dataset contains %d movies, %d TV series, and %d TV episodes.\",\n  result$count[result$titleType == \"movie\"],\n  result$count[result$titleType == \"tvSeries\"],\n  result$count[result$titleType == \"tvEpisode\"]\n)\n\ncat(message)\n\nThe dataset contains 131662 movies, 29789 TV series, and 155722 TV episodes.\n\n\n\n\n2.2. Who is the oldest living person in our data set?\nWe find the oldest living person by filtering for those with missing death years and sorting by birth year. However, we need to make sure that are not including records before 1900 as they might be missing.\n\n# Find the oldest living person, born after a reasonable cutoff (e.g., 1900)\noldest_person &lt;- NAME_BASICS |&gt;\n  filter(is.na(deathYear)) |&gt; # Only living people\n  filter(!is.na(birthYear)) |&gt; # Exclude missing birth years\n  filter(birthYear &gt;= 1900) |&gt; # Consider only people born after 1900\n  arrange(birthYear) |&gt; # Sort by birth year\n  head(1) # Get the oldest person\n\n# Calculate current age\ncurrent_year &lt;- as.numeric(format(Sys.Date(), \"%Y\"))\noldest_person_age &lt;- current_year - oldest_person$birthYear\n\n# Create a dynamic message\nmessage &lt;- sprintf(\n  \"The oldest living person in the dataset is %s, born in %d. They are currently %d years old.\",\n  oldest_person$primaryName,\n  oldest_person$birthYear,\n  oldest_person_age\n)\n\ncat(message)\n\nThe oldest living person in the dataset is Léonide Azar, born in 1900. They are currently 124 years old.\n\n\n\n\n2.3. Find the TV episode with 10/10 rating and 200,000 ratings\nWe use filtering and joins to identify highly rated TV episodes.\n\n# Step 1: Find the TV episode with a 10/10 rating and at least 200,000 votes\ntop_rated_episode &lt;- TITLE_RATINGS |&gt;\n  filter(averageRating == 10, numVotes &gt;= 200000) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  head(1)\n\n\nmessage &lt;- sprintf(\n    \"The top-rated TV episode is %s with %d votes and a perfect 10/10 rating.\",\n    top_rated_episode$primaryTitle,\n    top_rated_episode$numVotes\n  )\n\n\n# Print the message\ncat(message)\n\nThe top-rated TV episode is Ozymandias with 227589 votes and a perfect 10/10 rating.\n\n\n\n\n2.4. What four projects is the actor Mark Hamill most known for?\n\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  pull(knownForTitles)\n\n\ntconsts &lt;- unlist(strsplit(mark_hamill, \",\"))\n\n\nTITLE_BASICS |&gt;\n  filter(tconst %in% tconsts) |&gt;\n  select(primaryTitle, titleType, startYear) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n2.5. TV series with more than 12 episodes and the highest average rating\n\n# Filter for TV episodes with ratings and join with TITLE_BASICS\nepisode_ratings &lt;- TITLE_EPISODES |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  inner_join(TITLE_BASICS, by = c(\"parentTconst\" = \"tconst\"))\n\n# Count episodes per series and filter for series with more than 12 episodes\ntop_rated_series &lt;- episode_ratings |&gt;\n  group_by(parentTconst, primaryTitle) |&gt;\n  summarise(\n    avg_rating = mean(averageRating, na.rm = TRUE),\n    num_episodes = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(num_episodes &gt; 12) |&gt;\n  arrange(desc(avg_rating)) |&gt;\n  head(1)\n\nmessage &lt;- sprintf(\n    \"The TV series with the highest average rating is %s with an average rating of %.2f across %d episodes.\",\n    top_rated_series$primaryTitle,\n    top_rated_series$avg_rating,\n    top_rated_series$num_episodes\n  )\ncat(message)\n\nThe TV series with the highest average rating is Kavya - Ek Jazbaa, Ek Junoon with an average rating of 9.75 across 113 episodes.\n\n\n\n\n2.6. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\nlibrary(ggplot2)\n\n# Find all episodes of Happy Days\nhappy_days &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\")\n\n# Join with episodes and ratings\nhappy_days_ratings &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst %in% happy_days$tconst) |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  group_by(seasonNumber) |&gt;\n  summarise(avg_rating = mean(averageRating, na.rm = TRUE)) |&gt;\n  arrange(seasonNumber)\n\n# Create a line graph to visualize the average ratings by season\nggplot(happy_days_ratings, aes(x = seasonNumber, y = avg_rating)) +\n  geom_line(color = \"pink\") + \n  geom_point(color = \"red\") +\n  labs(\n    title = \"Average Ratings of Happy Days by Season\",\n    x = \"Season Number\",\n    y = \"Average Rating\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBased on the results, it does appear that later seasons of Happy Days generally have lower average ratings than the earlier seasons:\n\nSeasons 1 through 3 have relatively high ratings, all around or above 7.5.\nStarting from Season 4, there is a noticeable decline, with Season 8 reaching a low of 5.3.\nThere is a small improvement in Seasons 9 to 11, but they remain lower compared to the earlier seasons.\n\nThis confirms that later seasons, particularly after Season 3, tend to have lower average ratings compared to the earlier ones, supporting the hypothesis that the quality of the show may have declined in its later years\n\n\n\n\n\n\n\n\n\nTask 3: Custom Success Metric\n\n\n\n\n3.1. Choose the top 5-10 movies on your m-0[] that combines IMDb ratings with the number of votes. We define success as follows:\nA success metric is a way to measure how well something has performed, in this case, movies. For your project, we want to measure both the quality of a movie (how good people think it is) and its popularity (how many people have seen and rated it).\nWe use two key pieces of information from IMDb:\n\nAverage IMDb Rating: This tells us how good the movie is, based on the ratings it has received.\nNumber of Votes: This tells us how many people rated the movie, which helps us understand how popular it is.\n\nTo create the success metric, we combine these two things in a formula:\nSuccess Metric = Average IMDb Rating * log10(Number of Votes)\n\nThe average rating shows how much people liked the movie.\nThe logarithm of the number of votes is used to make sure that popular movies with lots of ratings get credit for being well-known, but without letting movies with lots of votes (but mediocre ratings) dominate.\n\nOverall, this will look for movies that are both highly rated and widely popular, and this formula helps to rank movies based on both factors. The higher the success metric, the more successful the movie is considered.\nLet’s add a column-success_metric to store the new metric.\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(success_metric = averageRating * log10(numVotes))\n\n\n\n3.1. Choose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\n\nmovies_only &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\")\n\n# Add a custom success metric to the movies_ratings table\nmovies_ratings &lt;- TITLE_RATINGS |&gt;\n  inner_join(movies_only, by = \"tconst\")\n\n\n# View the top 10 movies by success_metric\nmovies_ratings |&gt;\n  arrange(desc(success_metric)) |&gt;\n  head(10) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable()\n\n\n\n\n\nThese are the top 10 movies on my metric and they are commercially successful movies.\n\n\n3.2. Choose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\n\n# Select 3-5 movies with a high number of votes but low success metric\nmovies_ratings |&gt;\n  filter(numVotes &gt; 100000) |&gt; # Filter for popular movies\n  arrange(success_metric) |&gt; # Sort by lowest success metric\n  head(5) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable()\n\n\n\n\n\nThese are top 5 movies with high number of votes but low success metric, indicating low-quality popular movies.\n\n\n3.3. Choose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nFor this question let’s go with famous Director Steven Spielberg.\n\n# Filter for Steven Spielberg in the NAME_BASICS table\nspielberg_nconst &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  pull(nconst)\n\n# Filter TITLE_CREW for movies directed by Steven Spielberg\nspielberg_projects &lt;- TITLE_CREW |&gt;\n  filter(directors == spielberg_nconst) # Spielberg's nconst from the previous step\n\n# Join with TITLE_BASICS to get movie titles\nspielberg_movies &lt;- spielberg_projects |&gt;\n  inner_join(movies_ratings, by = \"tconst\") # Join with the ratings table to get success metric\n\n# Arrange by success metric to see top movies\nspielberg_movies |&gt;\n  arrange(desc(success_metric)) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable()\n\n\n\n\n\nThis table shows a prestige director-Steven Spielberg with many projects having high success metrics, indicating a consistently successful career in terms of popularity and quality.\n\n\n3.4. Perform at least one other form of ‘spot check’ validation.\n\n# Split genres into multiple rows\n# Define blockbusters as movies with over 200,000 votes\nmovies_by_type &lt;- TITLE_RATINGS |&gt;\n  mutate(movie_type = ifelse(numVotes &gt;= 200000, \"Blockbuster\", \"Independent\")) |&gt;\n  group_by(movie_type) |&gt;\n  summarise(\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    num_movies = n(),\n    .groups = \"drop\"\n  )\n\n# View the result\nmovies_by_type |&gt;\n  DT::datatable()\n\n\n\n\n\n\nInterpretation:\nBlockbusters Tend to Have Higher Success: It’s expected that blockbuster movies score higher on the success metric because they usually attract a larger number of votes and typically have bigger production budgets, more marketing, and wider releases, leading to more exposure.The higher score (41.36) for blockbusters reflects their broad popular awareness combined with high ratings.\nIndependent Films: While independent films have a much lower average success score (18.25), they are still in large numbers. Independent films tend to have smaller audiences and therefore fewer ratings, which impacts their success metric.\n\n\n\n3.5. Come up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\nThe logic behind determining successful movies was to first calculate the quantiles of the success_metric to understand its distribution. We selected the 90th percentile as the success threshold, meaning only the top 10% of movies would be considered “successful.” Then, we filtered the dataset based on this threshold, allowing us to focus on high-performing movies for further analysis.\n\n# Determine quantiles for success_metric\nquantile(movies_ratings$success_metric, probs = seq(0, 1, by = 0.05))\n\n       0%        5%       10%       15%       20%       25%       30%       35% \n 2.068186  8.611309 10.187988 11.248161 12.058833 12.761020 13.412522 14.017286 \n      40%       45%       50%       55%       60%       65%       70%       75% \n14.604024 15.214337 15.854272 16.539468 17.309076 18.185141 19.151397 20.321881 \n      80%       85%       90%       95%      100% \n21.707109 23.449321 25.858492 29.826115 60.159507 \n\n\nSince 95% of movies have less than 25.85 rating, let’s consider this as a success threshold(v)\n\nsuccess_threshold &lt;- 25.85\n\n\n\n\n\n\n\n\n\n\nTask 4.Using questions like the following, identify a good “genre” for your next film. You do not need to answer these questions precisely, but these are may help guide your thinking.\n\n\n\n\nPrepare Data for Analysis\nNow, lets store popular movies. First mutate startYear as numeric, after which we will create separate records for separate genre with separate_rows and rename the genres column to genre.\n\nmovies_ratings &lt;- movies_ratings |&gt;\n  mutate(startYear = as.numeric(startYear))\n\n# Separate the genres into individual rows (some movies have multiple genres)\nmovies_genre &lt;- movies_ratings |&gt;\n  separate_rows(genres, sep = \",\")\n\n# Rename the 'genres' column to 'genre'\nmovies_genre &lt;- movies_genre |&gt;\n  rename(genre = genres)\n\nThis table will show the genre with the most successful movies in each decade. For example, “Drama” might dominate many decades, as it’s a consistently successful genre.\n\n\n4.1. What Was the Genre with the Most “Successes” in Each Decade?\n\n# Create a new column for the decade\nmovies_genre &lt;- movies_genre |&gt;\n  mutate(decade = floor(startYear / 10) * 10)\n\n# Filter for successful movies (using the threshold from earlier)\nsuccessful_movies_by_decade &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(decade, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(decade, desc(num_successes))\n\n# Find the top genre in each decade\nsuccessful_movies_by_decade |&gt;\n  group_by(decade) |&gt;\n  slice_max(order_by = num_successes, n = 1) |&gt;\n  DT::datatable()\n\n\n\n\n\nLooking at the answer, it can be said that Drama is the most famous genre each decade.\n\n\n4.2. What Genre Consistently Has the Most “Successes”?\n\n# Total number of successes per genre across all decades\ntotal_successes_by_genre &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(total_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_successes))\n\ntotal_successes_by_genre |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per the analysis result from this Drama Genre seems to have most success with the most numbers of suceesful Titles.\n\n\n4.3. What Genre Used to Reliably Produce “Successes” but Has Fallen Out of Favor?\nFor this, we will analyze success before 2000 and after 2000.\n\n# Successes in earlier decades (before 2000) vs recent decades (2000 and later)\nsuccess_by_era &lt;- movies_genre |&gt;\n  mutate(era = ifelse(decade &lt; 2000, \"Before 2000\", \"2000 and After\")) |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(era, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = era, values_from = num_successes, values_fill = 0) |&gt;\n  mutate(fall_out = `Before 2000` &gt; 0 & `2000 and After` == 0) |&gt;\n  filter(fall_out == TRUE)\n\nsuccess_by_era |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per above result, Film-Noir seems to have zero successes after 2000.\n\n\n4.4. What Genre Has Produced the Most “Successes” Since 2010?\n\n# Filter for movies since 2010 and count successes by genre\nmovies_genre |&gt;\n  filter(startYear &gt;= 2010, success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(num_successes)) |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per above result, Drama seems to have the most Successes after 2010.\n\n\n4.5. Does the Genre with the Most Successes Have the Highest Success Rate?\n\n# Calculate the total number of movies per genre and the number of successes\nmovies_genre |&gt;\n  group_by(genre) |&gt;\n  summarise(\n    total_movies = n(),\n    num_successes = sum(success_metric &gt;= success_threshold),\n    success_rate = num_successes / total_movies * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(success_rate)) |&gt;\n  DT::datatable()\n\n\n\n\n\nAlthough the Drama genre has most successful movies, it doesn’t seem to have the highest success rate. Biography genre has the most success rate. Drama is in 14th place when ranking by success rate.\n\n\n4.6. What Genre Has Become More Popular in Recent Years?\nTo do this analysis, we will compare movies before 2010 and after 2010.\n\nmovies_by_era &lt;- movies_genre |&gt;\n  filter(!is.na(startYear)) |&gt;  # Filter out rows with NA in startYear\n  mutate(era = ifelse(startYear &gt;= 2010, \"Post 2010\", \"Pre 2010\")) |&gt;  # Categorize movies by era\n  group_by(era, genre) |&gt;\n  summarise(num_successful_movies = n(), .groups = \"drop\") |&gt; \n  pivot_wider(names_from = era, values_from = num_successful_movies, values_fill = 0)\n\n# Calculate the absolute difference and percentage increase between eras\nmovies_by_era |&gt;\n  mutate(\n    abs_difference = `Post 2010` - `Pre 2010`,  # Absolute difference\n    percent_increase = (`Post 2010` - `Pre 2010`) / `Pre 2010` * 100  # Percentage increase\n  ) |&gt;\n  # Filter for genres with at least one movie in both eras and significant increase\n  filter(`Pre 2010` &gt; 0 & `Post 2010` &gt; 0) |&gt;\n  filter(percent_increase &gt;= 50)  |&gt;\n  select(genre, `Pre 2010`, `Post 2010`, abs_difference, percent_increase) |&gt;\n  arrange(desc(percent_increase)) |&gt;\n  head(1) |&gt;\n  DT::datatable()\n\n\n\n\n\nNews genre seems to have the highest increase in popularity.\n\n\n\n\n\n\n\n\n\nTask 5: Key Personnel\n\n\n\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\nActors with successful movies\nLet’s list down the actors with their numbers of movies which is categorized successful.\n\n# Join TITLE_PRINCIPALS with movies_ratings to get actors with successful movies\nsuccessful_actors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n# Join with NAME_BASICS to get actor names\nsuccessful_actors &lt;- successful_actors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_actors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nThis result says that most successful actor are Samuel L. Jackson & Robert De Niro with 72 & 71 sucessful movies.\n\n\nDirectors with successful movies\nNow let’s find the most successful director with their numbers of movies which is categorized successful.\n\nsuccessful_directors &lt;- TITLE_CREW |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(!is.na(directors)) |&gt;\n  separate_rows(directors, sep = \",\") |&gt;\n  group_by(directors) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(5)\n\nsuccessful_directors &lt;- successful_directors |&gt;\n  inner_join(NAME_BASICS, by = c(\"directors\" = \"nconst\")) |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_directors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nThis result says that most successful director is Woody Allen with 48 sucessful movies.\n\n\n5.1: Generate a Graphic for Actor/Director Success\n\nlibrary(ggplot2)\n\n# Combine actors and directors for visualization\nkey_talent &lt;- rbind(\n  successful_actors |&gt; head(2),\n  successful_directors |&gt; head(1)\n)\n\n# Create a bar plot for key talent success\nggplot(key_talent, aes(x = reorder(primaryName, -num_successful_movies), y = num_successful_movies)) +\n  geom_bar(stat = \"identity\", fill = \"blue\") +\n  labs(\n    title = \"Number of Successful Movies for Key Talent\",\n    x = \"Talent\",\n    y = \"Number of Successful Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe graph below demonstrates that Samuel L. Jackson and Robert De Nior lead the way with the highest number of successful movies, while Woody Allen stands out as the top director in terms of successful films.\n\n\n5.2: Create a Table for Actor/Director Success\n\nif (!require(\"kableExtra\")) install.packages(\"kableExtra\")\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Add role information for actors and directors\nactors &lt;- successful_actors |&gt;\n  head(2) |&gt;\n  mutate(role = \"Actor\")\ndirectors &lt;- successful_directors |&gt;\n  head(1) |&gt;\n  mutate(role = \"Director\")\n\n# Combine actors and directors into one table\nkey_talent &lt;- rbind(actors, directors)\n\n\n# Create a table showing the role, number of successful movies, and average success metric\nkey_talent |&gt;\n  select(primaryName, role, num_successful_movies, avg_success_metric) |&gt;\n  kable(col.names = c(\"Name\", \"Role\", \"Successful Movies\", \"Average Success Metric\")) |&gt;\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\n\n\n\n\nName\nRole\nSuccessful Movies\nAverage Success Metric\n\n\n\n\nSamuel L. Jackson\nActor\n72\n30.64902\n\n\nRobert De Niro\nActor\n71\n31.50732\n\n\nWoody Allen\nDirector\n48\n32.34535\n\n\n\n\n\n\n\nThe table below demonstrates that Samuel L. Jackson and Robert De Nirolead the way with the highest number of successful movies, while Woody Allen stands out as the top director in terms of successful films.\n\n\n\n\n\n\n\n\n\nTask 6: Finding a Classic Movie to Remake\n\n\n\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\n\n6.1: Filter for Classic Movies\n\n# Define the year cutoff for remakes (25 years ago)\nyear_cutoff &lt;- 1999\n\n# Filter for classic movies that haven't been remade in the last 25 years\nclassic_movies &lt;- movies_ratings |&gt;\n  filter(\n    startYear &lt; year_cutoff,\n    averageRating &gt;= 7.5,\n    numVotes &gt;= 50000\n  ) |&gt;\n  arrange(desc(averageRating))\n\nclassic_movies |&gt;\n  DT::datatable()\n\n\n\n\n\nAs per result, these are the rank for classic movies.\n\n\n6.2: Check if Original Actors Are Still Alive\n\noriginal_movie_tconst &lt;- classic_movies$tconst[1] # Example: select the first movie in the list\n\n# Find actors in the original movie\noriginal_actors &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == original_movie_tconst, category %in% c(\"actor\", \"actress\")) |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, birthYear, deathYear)\n\n# Filter for actors who are still alive\noriginal_actors |&gt;\n  filter(is.na(deathYear)) |&gt;\n  DT::datatable()\n\n\n\n\n\nThis result says that the these top actors are still alive.\n\n\n6.3: Check if Original Directors/writers Are Still Alive\n\n# Find directors and writers from the original movie\noriginal_crew &lt;- TITLE_CREW |&gt;\n  filter(tconst == original_movie_tconst) |&gt;\n  separate_rows(directors, writers, sep = \",\") |&gt;\n  pivot_longer(c(directors, writers), names_to = \"role\", values_to = \"nconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, role, birthYear, deathYear)\n\n# Filter for crew members who are still alive\nalive_crew &lt;- original_crew |&gt;\n  filter(is.na(deathYear))\n\nalive_crew |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nThis result says that the these top directors and writers are still alive.\n\n\n6.4 Once you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\nTo proceed with the remake of “The Shawshank Redemption,” I have confirmed that several key figures from the original are still alive. Morgan Freeman and Tim Robbins, who played iconic roles, are both alive and could potentially be included in the project for “fan service.” Additionally, Frank Darabont, the original director, and Stephen King, the writer, are also still active.\nTo move forward, we will need to contact the legal department to secure the necessary rights from Castle Rock Entertainment for the film and Stephen King’s estate for the novella adaptation. Including classic actors like Freeman and Robbins in cameo roles would not only pay tribute to the original but also help attract loyal fans while building excitement for a new generation of viewers.\n\n\n\n\n\n\n\n\n\nTask 7: Elevator Pitch\n\n\n\nElevator Pitch: Remake of “The Shawshank Redemption”\nI’ve got a proposal that’s going to excite both loyal fans and a whole new audience — a modern remake of “The Shawshank Redemption.” This film has been a staple of cinema since 1994, ranked #1 on IMDb with a near-perfect rating of 9.3/10 and over 2.9 million votes. It’s clear: people love this story of hope, resilience, and friendship, and it’s time to bring it back with a fresh perspective.\nWe have an incredible opportunity to remake this classic with a stunning team. Woody Allen, known for his unique storytelling style, is our director. With 48 successful movies, Woody brings the creative depth needed to honor the original while offering a new artistic take.\nFor casting, we’re aiming high: Samuel L. Jackson as “Red.” With 72 hit films to his name, Jackson has the gravitas and warmth to bring this iconic character to life. And to make it even more dynamic, Robert De Niro, with 71 successful films, will play a key supporting role, ensuring this cast delivers both emotional depth and star power.\nWhy drama? In recent years, drama has consistently been the top-performing genre, producing the most successful films. From 2010 onward, it has led with over 3,290 successful titles, proving that audiences are hungry for emotionally-driven stories. With the powerhouse combination of Woody Allen, Samuel L. Jackson, and Robert De Niro, we’re confident that this remake will not only honor the original but also become a hit for modern viewers.\nTogether, this dream team is set to breathe new life into one of the greatest stories ever told, while maintaining the emotional core that made the original so beloved.\n\n\n\nKey Points for the movie:\nClassic Movie: The Shawshank Redemption is a highly rated film with over 2.9 million IMDb votes and a 9.3/10 rating, making it a prime candidate for a remake.\nStar Power: Samuel L. Jackson and Robert De Niro have over 140 successful films between them, bringing star power and acting prowess to the remake.\nDirector: Woody Allen, with 48 successful films, provides the artistic vision necessary to honor the original while offering a fresh perspective.\nMarket Potential: Drama continues to dominate as the top-performing genre, with over 3,290 successful titles released since 2010, making this remake a perfect fit for current audience preferences.\nNostalgia & Fan Service: Original actors like Morgan Freeman and Tim Robbins can make cameo appearances, blending nostalgia with a modern twist to attract loyal fans and new viewers alike."
  },
  {
    "objectID": "mp02.html#library-setup",
    "href": "mp02.html#library-setup",
    "title": "Mini Project 2",
    "section": "Library Setup",
    "text": "Library Setup\n\nInstall Required Packages\nWe will be analyzing various data from various sources. Following libraries are needed for this analysis. First check if the library is already installed and then install if not installed.\n\nif (!require(\"readr\")) install.packages(\"readr\") \nif (!require(\"dplyr\")) install.packages(\"dplyr\") \nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"stringr\")) install.packages(\"stringr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\n\n\n\nLoad the packages\nOnce the packages are installed, those will be loaded to the workspace so that they can be used later.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(tidyr)"
  },
  {
    "objectID": "mp02.html#load-data",
    "href": "mp02.html#load-data",
    "title": "Mini Project 2",
    "section": "Load Data",
    "text": "Load Data\nSince we have now setup libraries, we will now download the data to our project so that we can use later fo our analysis\n\nget_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\nNAME_BASICS &lt;- get_imdb_file(\"name.basics\")\n\n\nTITLE_BASICS &lt;- get_imdb_file(\"title.basics\")\n\n\nTITLE_RATINGS &lt;- get_imdb_file(\"title.ratings\")\n\n\nTITLE_CREW &lt;- get_imdb_file(\"title.crew\")\n\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")"
  },
  {
    "objectID": "oct17class.html",
    "href": "oct17class.html",
    "title": "oct17class",
    "section": "",
    "text": "library(ggplot2)\n\nScatter plot of price vs carat, faceted by cut\nggplot(diamonds, aes(x = carat, y = price)) + geom_point(alpha = 0.5) + # Add transparency to handle overplotting facet_wrap(~ cut) + # Facet by cut labs(title = “Price vs Carat, Faceted by Cut”, x = “Carat”, y = “Price”) + theme_minimal() ````"
  },
  {
    "objectID": "mp02.html#task-3",
    "href": "mp02.html#task-3",
    "title": "Mini Project 02",
    "section": "Task 3",
    "text": "Task 3\n\n# Add a custom success metric to TITLE_RATINGS\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  mutate(success_metric = averageRating * log10(numVotes))\n\nglimpse(TITLE_RATINGS)\n\nRows: 373,996\nColumns: 4\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ averageRating  &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, …\n$ numVotes       &lt;dbl&gt; 2095, 283, 2102, 183, 2837, 196, 889, 2242, 215, 7724, …\n$ success_metric &lt;dbl&gt; 18.930749, 13.730004, 21.597113, 12.217236, 21.407728, …\n\n\n\nmovies_only &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\")\n\n# Add a custom success metric to the movies_ratings table\nmovies_ratings &lt;- TITLE_RATINGS |&gt;\n  inner_join(movies_only, by = \"tconst\")\n\n\n# View the top 10 movies by success_metric\nmovies_ratings |&gt;\n  arrange(desc(success_metric)) |&gt;\n  head(10) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n                                        primaryTitle averageRating numVotes\n1                           The Shawshank Redemption           9.3  2951083\n2                                    The Dark Knight           9.0  2932304\n3                                      The Godfather           9.2  2057179\n4      The Lord of the Rings: The Return of the King           9.0  2020203\n5                                       Pulp Fiction           8.9  2266176\n6                                          Inception           8.8  2602440\n7  The Lord of the Rings: The Fellowship of the Ring           8.9  2049867\n8                                         Fight Club           8.8  2382865\n9                                       Forrest Gump           8.8  2308847\n10                                  Schindler's List           9.0  1480407\n   success_metric\n1        60.17083\n2        58.20488\n3        58.08210\n4        56.74856\n5        56.56211\n6        56.45535\n7        56.17436\n8        56.11848\n9        55.99788\n10       55.53343"
  },
  {
    "objectID": "mp02.html#low-scoring-popular-movies-validation",
    "href": "mp02.html#low-scoring-popular-movies-validation",
    "title": "Mini Project 02",
    "section": "Low-Scoring Popular Movies Validation",
    "text": "Low-Scoring Popular Movies Validation\n\n# Select 3-5 movies with a high number of votes but low success metric\nmovies_ratings |&gt;\n  filter(numVotes &gt; 100000) |&gt; # Filter for popular movies\n  arrange(success_metric) |&gt; # Sort by lowest success metric\n  head(5) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric)\n\n       primaryTitle averageRating numVotes success_metric\n1             Radhe           1.9   180234        9.98609\n2        Epic Movie           2.4   110309       12.10227\n3         Adipurush           2.7   134356       13.84629\n4 Meet the Spartans           2.8   112308       14.14115\n5          365 Days           3.3   100866       16.51236\n\n\n\n3. Prestige Actor or Director Validation\n\n# Example for a director like Steven Spielberg\n# TITLE_CREW |&gt;\n#    select(directors)\n\n# Filter for Steven Spielberg in the NAME_BASICS table\nspielberg_nconst &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  pull(nconst)\n\n# Filter TITLE_CREW for movies directed by Steven Spielberg\nspielberg_projects &lt;- TITLE_CREW |&gt;\n  filter(directors == spielberg_nconst) # Spielberg's nconst from the previous step\n\n# Join with TITLE_BASICS to get movie titles\nspielberg_movies &lt;- spielberg_projects |&gt;\n  inner_join(movies_ratings, by = \"tconst\") # Join with the ratings table to get success metric\n\n# Arrange by success metric to see top movies\nspielberg_movies |&gt;\n  arrange(desc(success_metric)) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n# spielberg_success &lt;- movies_ratings |&gt;\n#     filter(tconst %in% spielberg_tconst) |&gt;\n#     arrange(desc(success_metric))\n#\n# print(spielberg_success)"
  },
  {
    "objectID": "mp02.html#other-spot-check-validation",
    "href": "mp02.html#other-spot-check-validation",
    "title": "Mini Project 02",
    "section": "4. Other Spot-Check Validation",
    "text": "4. Other Spot-Check Validation\n\n# # Compare success scores across genres\n# genre_success &lt;- movies_ratings |&gt;\n#     group_by(genre) |&gt;\n#     summarise(avg_success = mean(success_metric, na.rm = TRUE)) |&gt;\n#     arrange(desc(avg_success))\n#\n# print(genre_success)\n\nlibrary(tidyr)\n\n# Split genres into multiple rows\nmovies_with_genres &lt;- movies_ratings |&gt;\n  separate_rows(genres, sep = \",\")\n\n# Calculate average success metric by genre\nmovies_with_genres |&gt;\n  group_by(genres) |&gt;\n  summarise(\n    avg_success = mean(success_metric, na.rm = TRUE),\n    num_movies = n()\n  ) |&gt; # Count how many movies are in each genre\n  arrange(desc(avg_success)) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n5. Numerical Threshold for Success\n\n# Determine quantiles for success_metric\nquantile(movies_ratings$success_metric, probs = seq(0, 1, by = 0.05))\n\n       0%        5%       10%       15%       20%       25%       30%       35% \n 2.068186  8.621344 10.187988 11.247041 12.058179 12.759427 13.407628 14.013592 \n      40%       45%       50%       55%       60%       65%       70%       75% \n14.601748 15.212304 15.853725 16.539468 17.308795 18.184748 19.151397 20.320207 \n      80%       85%       90%       95%      100% \n21.703213 23.447132 25.849774 29.816939 60.170827 \n\n\n\n\nFinding movies with success\n\n# Define a threshold for success based on the 90th percentile\nsuccess_threshold &lt;- 25.85\n\n# Filter movies that are considered \"successful\"\nmovies_ratings |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n  head(100) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n###Examining Success by Genre and Decade 1. Prepare Data for Analysis\n\n# Ensure that startYear is numeric\nmovies_ratings &lt;- movies_ratings |&gt;\n  mutate(startYear = as.numeric(startYear))\n\n# Separate the genres into individual rows (some movies have multiple genres)\nmovies_genre &lt;- movies_ratings |&gt;\n  separate_rows(genres, sep = \",\")\n\n# Rename the 'genres' column to 'genre'\nmovies_genre &lt;- movies_genre |&gt;\n  rename(genre = genres)\n\n\nWhat Was the Genre with the Most “Successes” in Each Decade?\n\n\n# Create a new column for the decade\nmovies_genre &lt;- movies_genre |&gt;\n  mutate(decade = floor(startYear / 10) * 10)\n\n# Filter for successful movies (using the threshold from earlier)\nsuccessful_movies_by_decade &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(decade, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(decade, desc(num_successes))\n\n# Find the top genre in each decade\nsuccessful_movies_by_decade |&gt;\n  group_by(decade) |&gt;\n  slice_max(order_by = num_successes, n = 1) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n3. What Genre Consistently Has the Most “Successes”?\n\n# Total number of successes per genre across all decades\ntotal_successes_by_genre &lt;- movies_genre |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(total_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_successes))\n\ntotal_successes_by_genre |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n4. What Genre Used to Reliably Produce “Successes” but Has Fallen Out of Favor?\n\n# Successes in earlier decades (before 2000) vs recent decades (2000 and later)\nsuccess_by_era &lt;- movies_genre |&gt;\n  mutate(era = ifelse(decade &lt; 1980, \"Before 2000\", \"2000 and After\")) |&gt;\n  filter(success_metric &gt;= success_threshold) |&gt;\n  group_by(era, genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = era, values_from = num_successes, values_fill = 0) |&gt;\n  mutate(fall_out = `Before 2000` &gt; 0 & `2000 and After` == 0) |&gt;\n  filter(fall_out == TRUE)\n\nsuccess_by_era |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n5. What Genre Has Produced the Most “Successes” Since 2010?\n\n# Filter for movies since 2010 and count successes by genre\nmovies_genre |&gt;\n  filter(startYear &gt;= 2010, success_metric &gt;= success_threshold) |&gt;\n  group_by(genre) |&gt;\n  summarise(num_successes = n(), .groups = \"drop\") |&gt;\n  arrange(desc(num_successes)) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n6. Does the Genre with the Most Successes Have the Highest Success Rate?\n\n# Calculate the total number of movies per genre and the number of successes\nmovies_genre |&gt;\n  group_by(genre) |&gt;\n  summarise(\n    total_movies = n(),\n    num_successes = sum(success_metric &gt;= success_threshold),\n    success_rate = num_successes / total_movies * 100,\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(success_rate)) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\n7. What Genre Has Become More Popular in Recent Years?\n\n# Compare genre popularity by counting the number of movies in recent years (post-2010) vs earlier\n# Ensure there are no missing startYear values and proceed with the calculation\nmovies_genre |&gt;\n  filter(!is.na(startYear)) |&gt; # Filter out rows with NA in startYear\n  mutate(era = ifelse(startYear &gt;= 2010, \"Post 2010\", \"Pre 2010\")) |&gt;\n  group_by(era, genre) |&gt;\n  summarise(num_movies = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = era, values_from = num_movies, values_fill = 0) |&gt;\n  mutate(popularity_increase = `Post 2010` &gt; `Pre 2010`) |&gt;\n  filter(popularity_increase == TRUE) |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\n\nTask 5: Key Personnel\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\n# Join TITLE_PRINCIPALS with movies_ratings to get actors with successful movies\nsuccessful_actors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n# Join with NAME_BASICS to get actor names\nsuccessful_actors &lt;- successful_actors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_actors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nDirectors: Now let’s find the most successful director:\n\n# Join TITLE_CREW with movies_ratings to get directors with successful movies\nsuccessful_directors &lt;- TITLE_CREW |&gt;\n  inner_join(movies_ratings, by = \"tconst\") |&gt;\n  filter(!is.na(directors)) |&gt;\n  separate_rows(directors, sep = \",\") |&gt;\n  group_by(directors) |&gt;\n  summarise(\n    num_successful_movies = sum(success_metric &gt;= success_threshold),\n    avg_success_metric = mean(success_metric, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(5)\n\n# Join with NAME_BASICS to get director names\nsuccessful_directors &lt;- successful_directors |&gt;\n  inner_join(NAME_BASICS, by = c(\"directors\" = \"nconst\")) |&gt;\n  select(primaryName, num_successful_movies, avg_success_metric)\n\nsuccessful_directors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nStep 2: Generate a Graphic for Actor/Director Success\n\nlibrary(ggplot2)\n\n# Combine actors and directors for visualization\nkey_talent &lt;- rbind(\n  successful_actors |&gt; head(2),\n  successful_directors |&gt; head(1)\n)\n\n# Create a bar plot for key talent success\nggplot(key_talent, aes(x = reorder(primaryName, -num_successful_movies), y = num_successful_movies)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(\n    title = \"Number of Successful Movies for Key Talent\",\n    x = \"Talent\",\n    y = \"Number of Successful Movies\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nStep 3: Create a Table for Actor/Director Success\n\nif (!require(\"kableExtra\")) install.packages(\"kableExtra\")\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Add role information for actors and directors\nactors &lt;- successful_actors |&gt;\n  head(2) |&gt;\n  mutate(role = \"Actor\")\ndirectors &lt;- successful_directors |&gt;\n  head(1) |&gt;\n  mutate(role = \"Director\")\n\n# Combine actors and directors into one table\nkey_talent &lt;- rbind(actors, directors)\n\n\n# Create a table showing the role, number of successful movies, and average success metric\nkey_talent |&gt;\n  select(primaryName, role, num_successful_movies, avg_success_metric) |&gt;\n  kable(col.names = c(\"Name\", \"Role\", \"Successful Movies\", \"Average Success Metric\")) |&gt;\n  kable_styling(bootstrap_options = \"striped\", full_width = F)\n\n\n\n\nName\nRole\nSuccessful Movies\nAverage Success Metric\n\n\n\n\nSamuel L. Jackson\nActor\n72\n30.51962\n\n\nRobert De Niro\nActor\n71\n31.50986\n\n\nWoody Allen\nDirector\n48\n32.34295\n\n\n\n\n\n\n\n\n\nTask 6: Finding a Classic Movie to Remake\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.4\nOnce you have found your classic movie to remake, confirm whether key actors, directors, or writers from the original are still alive. If so, you need to contact your legal department to ensure they can secure the rights to the project. You may also want to include the classic actors as “fan service.”\nStep 1: Filter for Classic Movies\n\n# Define the year cutoff for remakes (25 years ago)\nyear_cutoff &lt;- 1999\n\n# Filter for classic movies that haven't been remade in the last 25 years\nclassic_movies &lt;- movies_ratings |&gt;\n  filter(\n    startYear &lt; year_cutoff,\n    averageRating &gt;= 7.5,\n    numVotes &gt;= 50000\n  ) |&gt;\n  arrange(desc(averageRating))\n\nclassic_movies |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nStep 2: Check if Original Actors Are Still Alive\n\n# Get key actors from the original movie\noriginal_movie_tconst &lt;- classic_movies$tconst[1] # Example: select the first movie in the list\n\n# Find actors in the original movie\noriginal_actors &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == original_movie_tconst, category %in% c(\"actor\", \"actress\")) |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, birthYear, deathYear)\n\n# Filter for actors who are still alive\nalive_actors &lt;- original_actors |&gt;\n  filter(is.na(deathYear))\n\nprint(alive_actors)\n\n        primaryName birthYear deathYear\n1       Tim Robbins      1958        NA\n2    Morgan Freeman      1937        NA\n3        Bob Gunton      1945        NA\n4    William Sadler      1950        NA\n5      Clancy Brown      1959        NA\n6       Gil Bellows      1967        NA\n7      Mark Rolston      1956        NA\n8    Jeffrey DeMunn      1947        NA\n9 Larry Brandenburg      1948        NA\n\n\nStep 2: Check if Original Directors/writers Are Still Alive\n\n# Find directors and writers from the original movie\noriginal_crew &lt;- TITLE_CREW |&gt;\n  filter(tconst == original_movie_tconst) |&gt;\n  separate_rows(directors, writers, sep = \",\") |&gt;\n  pivot_longer(c(directors, writers), names_to = \"role\", values_to = \"nconst\") |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, role, birthYear, deathYear)\n\n# Filter for crew members who are still alive\nalive_crew &lt;- original_crew |&gt;\n  filter(is.na(deathYear))\n\nalive_crew |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\nStep 4: Consider Including Original Talent and Legal Clearance"
  },
  {
    "objectID": "mp02.html#key-points-for-the-movie",
    "href": "mp02.html#key-points-for-the-movie",
    "title": "Mini Project 02",
    "section": "Key Points for the movie:",
    "text": "Key Points for the movie:\nClassic Movie: The Shawshank Redemption is a highly rated film with over 2.9 million IMDb votes and a 9.3/10 rating, making it a prime candidate for a remake.\nStar Power: Samuel L. Jackson and Robert De Niro have over 140 successful films between them, bringing star power and acting prowess to the remake.\nDirector: Woody Allen, with 48 successful films, provides the artistic vision necessary to honor the original while offering a fresh perspective.\nMarket Potential: Drama continues to dominate as the top-performing genre, with over 3,290 successful titles released since 2010, making this remake a perfect fit for current audience preferences.\nNostalgia & Fan Service: Original actors like Morgan Freeman and Tim Robbins can make cameo appearances, blending nostalgia with a modern twist to attract loyal fans and new viewers alike."
  },
  {
    "objectID": "Mini03.html",
    "href": "Mini03.html",
    "title": "Mini Project 03",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "ocr24.html",
    "href": "ocr24.html",
    "title": "oct24",
    "section": "",
    "text": "library(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n``` ::: {.cell}\n:::"
  },
  {
    "objectID": "Mini03.html#installment",
    "href": "Mini03.html#installment",
    "title": "Mini Project 03",
    "section": "Installment",
    "text": "Installment\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\n\n###Load data"
  },
  {
    "objectID": "TransitDATA.html#installment",
    "href": "TransitDATA.html#installment",
    "title": "Mini Project 03",
    "section": "Installment",
    "text": "Installment\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(httr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\nLoad data\n\nurl &lt;- \"https://data.ny.gov/api/views/vtvh-gimj/rows.csv?accessType=DOWNLOAD\"\n\n\nmta_data &lt;- read.csv(url)\n\n# View the first few rows\nhead(mta_data)\n\n       month   division line day_type num_on_time_trips num_sched_trips\n1 2020-01-01 A DIVISION    1        1              9035           10554\n2 2020-01-01 A DIVISION    1        2              2866            3050\n3 2020-01-01 A DIVISION    2        1              6058            7330\n4 2020-01-01 A DIVISION    2        2              1674            2340\n5 2020-01-01 A DIVISION    3        1              5732            6603\n6 2020-01-01 A DIVISION    3        2              1527            1875\n  terminal_on_time_performance\n1                    0.8560735\n2                    0.9396721\n3                    0.8264666\n4                    0.7153846\n5                    0.8680903\n6                    0.8144000\n\n\n\nglimpse(mta_data)\n\nRows: 2,665\nColumns: 7\n$ month                        &lt;chr&gt; \"2020-01-01\", \"2020-01-01\", \"2020-01-01\",…\n$ division                     &lt;chr&gt; \"A DIVISION\", \"A DIVISION\", \"A DIVISION\",…\n$ line                         &lt;chr&gt; \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"…\n$ day_type                     &lt;int&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,…\n$ num_on_time_trips            &lt;int&gt; 9035, 2866, 6058, 1674, 5732, 1527, 6552,…\n$ num_sched_trips              &lt;int&gt; 10554, 3050, 7330, 2340, 6603, 1875, 8124…\n$ terminal_on_time_performance &lt;dbl&gt; 0.8560735, 0.9396721, 0.8264666, 0.715384…\n\n\n\n\nWhich subway lines maintain the highest schedule?\n\n# Calculate the average terminal on-time performance for each line\nlibrary(dplyr)\n\ntop_lines &lt;- mta_data |&gt;\n  group_by(line)|&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# View the top lines with the highest average on-time performance\nhead(top_lines)\n\n# A tibble: 6 × 2\n  line   avg_on_time_performance\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 S 42nd                   0.996\n2 GS                       0.995\n3 S Fkln                   0.994\n4 FS                       0.993\n5 S Rock                   0.967\n6 H                        0.965\n\n\n###How does the terminal on-time performance vary across different subway lines from 2020 to 2024?\n\n# Assuming mta_data is already loaded in R\n\n# Convert 'month' column to Date type if not already\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Filter for data from 2020 to 2024\nmta_data_filtered &lt;- subset(mta_data, month &gt;= as.Date(\"2020-01-01\") & month &lt;= as.Date(\"2024-12-31\"))\n\n# Plotting\nggplot(mta_data_filtered, aes(x = month, y = terminal_on_time_performance, color = line)) +\n  geom_line() +\n  labs(title = \"On-Time Performance by Subway Line (2020-2024)\",\n       x = \"Month\",\n       y = \"On-Time Performance (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nLine with the maximum and minimum number of on-time trips\n\n# Exclude rows where line is \"Systemwide\" becuase that is overall all\nfiltered_data &lt;- subset(mta_data, line != \"Systemwide\")\n\n# Calculate the total number of on-time trips for each line\nline_on_time_trips &lt;- aggregate(num_on_time_trips ~ line, data = filtered_data, sum)\n\n# Find the line with the maximum number of on-time trips\nmax_on_time_line &lt;- line_on_time_trips[which.max(line_on_time_trips$num_on_time_trips), ]\n\n# Find the line with the minimum number of on-time trips\nmin_on_time_line &lt;- line_on_time_trips[which.min(line_on_time_trips$num_on_time_trips), ]\n\n# Display the results\nprint(paste(\"The line with the maximum on-time trips is Line\", max_on_time_line$line,\n            \"with\", max_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n[1] \"The line with the maximum on-time trips is Line 7 with 862955 on-time trips.\"\n\nprint(paste(\"The line with the minimum on-time trips is Line\", min_on_time_line$line,\n            \"with\", min_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n[1] \"The line with the minimum on-time trips is Line S Rock with 4721 on-time trips.\"\n\n\n\n\nwhich Month has most and least on-time performance?\n\n# Convert the 'month' column to Date type if it's not already\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Calculate the average terminal on-time performance for each month\nmonthly_performance &lt;- aggregate(terminal_on_time_performance ~ format(month, \"%Y-%m\"), data = mta_data, mean)\n\n# Rename columns for clarity\ncolnames(monthly_performance) &lt;- c(\"month\", \"avg_terminal_on_time_performance\")\n\n# Find the month with the highest terminal on-time performance\nmax_performance_month &lt;- monthly_performance[which.max(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Find the month with the lowest terminal on-time performance\nmin_performance_month &lt;- monthly_performance[which.min(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Display the results\nprint(paste(\"The month with the highest terminal on-time performance is\", max_performance_month$month,\n            \"with an average on-time performance of\", round(max_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n[1] \"The month with the highest terminal on-time performance is 2020-05 with an average on-time performance of 0.94 %.\"\n\nprint(paste(\"The month with the lowest terminal on-time performance is\", min_performance_month$month,\n            \"with an average on-time performance of\", round(min_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n[1] \"The month with the lowest terminal on-time performance is 2024-02 with an average on-time performance of 0.78 %.\"\n\n\n\n\nYearly on-time performance trend from 2020 to 2024\n\n# Analyze the yearly on-time performance trend from 2020 to 2024\nmta_data |&gt;\n  # Convert 'month' to Date format if not already in Date format\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = format(month, \"%Y\")) |&gt;\n  \n  # Calculate the average on-time performance for each year\n  group_by(year) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  \n  # Plot the trend over years\n  ggplot(aes(x = as.numeric(year), y = avg_terminal_on_time_performance)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Yearly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Year\",\n       y = \"Average On-Time Performance (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Calculate the average on-time performance for each subway line and select the top 5\ntop_5_lines &lt;- mta_data |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) # Select the top 5 lines\n\n# Display the result\ntop_5_lines\n\n# A tibble: 5 × 2\n  line   avg_on_time_performance\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 S 42nd                   0.996\n2 GS                       0.995\n3 S Fkln                   0.994\n4 FS                       0.993\n5 S Rock                   0.967\n\n\n\n# Calculate the total number of on-time trips for each subway line\nline_on_time_trips &lt;- mta_data |&gt;\n  group_by(line) |&gt;\n  summarize(total_on_time_trips = sum(num_on_time_trips, na.rm = TRUE))\n\n# Get the top 5 lines with the maximum on-time trips\ntop_5_max_trips &lt;- line_on_time_trips |&gt;\n  arrange(desc(total_on_time_trips)) |&gt;\n  slice_head(n = 5)\n\n# Get the top 5 lines with the minimum on-time trips\ntop_5_min_trips &lt;- line_on_time_trips |&gt;\n  arrange(total_on_time_trips) |&gt;\n  slice_head(n = 5)\n\n# Display the results\nlist(Top_5_Max_On_Time_Trips = top_5_max_trips, Top_5_Min_On_Time_Trips = top_5_min_trips)\n\n$Top_5_Max_On_Time_Trips\n# A tibble: 5 × 2\n  line       total_on_time_trips\n  &lt;chr&gt;                    &lt;int&gt;\n1 Systemwide            10417622\n2 7                       862955\n3 L                       780783\n4 GS                      702897\n5 6                       663427\n\n$Top_5_Min_On_Time_Trips\n# A tibble: 5 × 2\n  line   total_on_time_trips\n  &lt;chr&gt;                &lt;int&gt;\n1 S Rock                4721\n2 S Fkln                6752\n3 JZ                    7667\n4 S 42nd               14897\n5 B                   163207\n\n\n\n# Convert 'month' column to Date format if it is not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = format(month, \"%Y\"))\n\n# Calculate the average terminal on-time performance for each year\nyearly_performance &lt;- mta_data |&gt;\n  group_by(year) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE))\n\n# Plot the trend over years\nggplot(yearly_performance, aes(x = as.numeric(year), y = avg_terminal_on_time_performance)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Yearly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Year\",\n       y = \"Average On-Time Performance (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Convert 'month' column to Date format if not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = format(month, \"%Y\"))\n\n# Calculate the average on-time performance for each year\nyearly_performance &lt;- mta_data |&gt;\n  group_by(year) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  mutate(period = ifelse(year %in% c(\"2020\", \"2021\"), \"COVID Period (2020-2021)\", \"Post-COVID Recovery (2022-2024)\"))\n\n# Plot the trends for each period\nggplot(yearly_performance, aes(x = as.numeric(year), y = avg_terminal_on_time_performance, color = period)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Terminal On-Time Performance: COVID vs. Post-COVID Recovery\",\n       x = \"Year\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Period\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"COVID Period (2020-2021)\" = \"red\", \"Post-COVID Recovery (2022-2024)\" = \"green\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = as.numeric(format(month, \"%Y\")))\n\n# Separate data into COVID period (2020-2021) and post-COVID recovery (2022-2024)\ncovid_period &lt;- mta_data |&gt;\n  filter(year %in% c(2020, 2021)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) # Select top 5 lines\n\npost_covid_period &lt;- mta_data |&gt;\n  filter(year %in% c(2022, 2023, 2024)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) # Select top 5 lines\n\n# Display the results\nlist(COVID_Period_Top_5_Lines = covid_period, Post_COVID_Recovery_Top_5_Lines = post_covid_period)\n\n$COVID_Period_Top_5_Lines\n# A tibble: 5 × 2\n  line  avg_on_time_performance\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 FS                      0.994\n2 GS                      0.993\n3 H                       0.973\n4 L                       0.941\n5 7                       0.917\n\n$Post_COVID_Recovery_Top_5_Lines\n# A tibble: 5 × 2\n  line   avg_on_time_performance\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 S 42nd                   0.996\n2 GS                       0.996\n3 S Fkln                   0.994\n4 FS                       0.992\n5 S Rock                   0.967\n\n\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = as.numeric(format(month, \"%Y\")))\n\n# Calculate top 5 lines in 2020, filtering out NAs\ntop_5_2020 &lt;- mta_data |&gt;\n  filter(year == 2020 & !is.na(terminal_on_time_performance)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) |&gt;\n  mutate(year = \"2020\")\n\n# Calculate top 5 lines in 2022, filtering out NAs\ntop_5_2022 &lt;- mta_data |&gt;\n  filter(year == 2022 & !is.na(terminal_on_time_performance)) |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance)) |&gt;\n  slice_head(n = 5) |&gt;\n  mutate(year = \"2022\")\n\n# Combine both years into one data frame for plotting\ntop_lines_combined &lt;- bind_rows(top_5_2020, top_5_2022)\n\n# Plotting the top 5 lines in each year\nggplot(top_lines_combined, aes(x = reorder(line, avg_on_time_performance), \n                               y = avg_on_time_performance, fill = year)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Top 5 Reliable Subway Lines for 2020 and 2022\",\n       x = \"Subway Line\",\n       y = \"Average On-Time Performance (%)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"2020\" = \"blue\", \"2022\" = \"orange\")) +\n  coord_flip() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n# Load the necessary library\nlibrary(readr)\n\n# Read the data from the URL\nurl &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/candy-power-ranking/candy-data.csv\"\ncandy_data &lt;- read_csv(url)\n\nRows: 85 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): competitorname\ndbl (12): chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewaf...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Display the first few rows of the data\nhead(candy_data)\n\n# A tibble: 6 × 13\n  competitorname chocolate fruity caramel peanutyalmondy nougat crispedricewafer\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;  &lt;dbl&gt;            &lt;dbl&gt;\n1 100 Grand              1      0       1              0      0                1\n2 3 Musketeers           1      0       0              0      1                0\n3 One dime               0      0       0              0      0                0\n4 One quarter            0      0       0              0      0                0\n5 Air Heads              0      1       0              0      0                0\n6 Almond Joy             1      0       0              1      0                0\n# ℹ 6 more variables: hard &lt;dbl&gt;, bar &lt;dbl&gt;, pluribus &lt;dbl&gt;,\n#   sugarpercent &lt;dbl&gt;, pricepercent &lt;dbl&gt;, winpercent &lt;dbl&gt;\n\n\n\nglimpse(candy_data)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ fruity           &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ caramel          &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ peanutyalmondy   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ nougat           &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ crispedricewafer &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ hard             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,…\n$ bar              &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ pluribus         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…\n\n\n\ncorrelation &lt;- cor(candy_data$sugarpercent, candy_data$winpercent)\nprint(paste(\"Correlation between sugar content and win percentage:\", correlation))\n\n[1] \"Correlation between sugar content and win percentage: 0.229150657128007\"\n\n\n\n# Calculate the correlation between pricepercent and winpercent\ncorrelation_price &lt;- cor(candy_data$pricepercent, candy_data$winpercent)\nprint(paste(\"Correlation between price and win percentage:\", correlation_price))\n\n[1] \"Correlation between price and win percentage: 0.345325409967685\"\n\n\n\n# Find the candy with the highest win percentage\nmost_popular_candy &lt;- candy_data[which.max(candy_data$winpercent), ]\n\n# Display the name and win percentage of the most popular candy\nmost_popular_candy_name &lt;- most_popular_candy$competitorname\nmost_popular_candy_winpercent &lt;- most_popular_candy$winpercent\nprint(paste(\"The most popular candy during Halloween is:\", most_popular_candy_name, \n            \"with a win percentage of\", round(most_popular_candy_winpercent, 2), \"%\"))\n\n[1] \"The most popular candy during Halloween is: Reese's Peanut Butter cup with a win percentage of 84.18 %\"\n\n\n\n# Load necessary library\nlibrary(dplyr)\n\n# Find the 5 least popular candies based on win percentage\nleast_popular_5 &lt;- candy_data |&gt;\n  arrange(winpercent) |&gt;\n  slice(1:5)\n\n# Plot the win percentage of the 5 least popular candies\nbarplot(least_popular_5$winpercent,\n        names.arg = least_popular_5$competitorname,\n        main = \"5 Least Popular Candies\",\n        xlab = \"Candy\",\n        ylab = \"Win Percentage (%)\",\n        las = 2, \n        col = \"pink\")\n\n\n\n\n\n\n\n\n\n\nHow has the overall terminal on-time performance trended each month from 2020 to 2024?\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Calculate the average on-time performance for each month across all lines\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Calculate the average on-time performance for each month across all lines\nmonthly_performance &lt;- mta_data |&gt;\n  group_by(month = format(month, \"%Y-%m\")) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  mutate(month = as.Date(paste0(month, \"-01\"), format = \"%Y-%m-%d\"))\n\n# Add a column to indicate the period (COVID vs Post-COVID)\nmonthly_performance &lt;- monthly_performance |&gt;\n  mutate(period = ifelse(format(month, \"%Y\") %in% c(\"2020\", \"2021\"), \"COVID Period (2020-2021)\", \"Post-COVID Recovery (2022-2024)\"))\n\n# Plot the monthly trend with different colors for each period\nggplot(monthly_performance, aes(x = month, y = avg_terminal_on_time_performance, color = period)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Monthly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Period\") +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"3 months\") +\n  scale_color_manual(values = c(\"COVID Period (2020-2021)\" = \"pink\", \"Post-COVID Recovery (2022-2024)\" = \"purple\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Extract month name, month number, and assign a season based on month\nmta_data &lt;- mta_data |&gt;\n  mutate(month_name = format(month, \"%B\"),\n         month_num = as.numeric(format(month, \"%m\")),\n         season = case_when(\n           month_num %in% c(12, 1, 2) ~ \"Winter\",\n           month_num %in% c(3, 4, 5) ~ \"Spring\",\n           month_num %in% c(6, 7, 8) ~ \"Summer\",\n           month_num %in% c(9, 10, 11) ~ \"Fall\"\n         ))\n\n# Calculate the average on-time performance for each month across all years\nseasonal_performance &lt;- mta_data |&gt;\n  group_by(month_name, month_num, season) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(month_num)  # Arrange by month number for chronological order\n\n`summarise()` has grouped output by 'month_name', 'month_num'. You can override\nusing the `.groups` argument.\n\n# Plot the seasonal/monthly trend with colors by season\nggplot(seasonal_performance, aes(x = reorder(month_name, month_num), y = avg_on_time_performance, color = season, group = 1)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Seasonal/Monthly Terminal On-Time Performance Trend Across Years\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Season\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Winter\" = \"skyblue\", \"Spring\" = \"green\", \"Summer\" = \"orange\", \"Fall\" = \"brown\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini project 03",
    "section": "",
    "text": "TOPIC : Do Proportional Electoral College Allocations Yield a More Representative Presidency?\n\n\nIntroduction\nFor this project I will analyze whether the U.S. Electoral College provides a fair and representative outcome in presidential elections, particularly when different allocation methods are used. The findings will be presented in the form of a fact-checking report, analyzing claims of bias within the Electoral College and assessing how allocation rules influence election results.\n\n\nsetup libraries needed for analysis\nTo prepare for the analysis in this project, I am setting up and loading several R libraries that are essential for data manipulation, visualization, spatial analysis, and animation. This will ensure I have the tools needed for a comprehensive analysis of electoral data and effective, dynamic visualizations.\n\n# Let's Install the libraries \nif (!require(tidyverse)) install.packages(\"tidyverse\")\nif (!require(readr)) install.packages(\"readr\")\nif (!require(sf)) install.packages(\"sf\")\nif (!require(httr)) install.packages(\"httr\")\nif (!require(zip)) install.packages(\"zip\")\nif (!require(patchwork)) install.packages(\"patchwork\")\nif (!require(ggrepel)) install.packages(\"ggrepel\")\nif (!require(gganimate)) install.packages(\"gganimate\")\nif (!require(scales)) install.packages(\"scales\")\nif (!require(magick)) install.packages(\"magick\")\n\n\n# Let's Load the Libraries\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(sf)\nlibrary(httr)\nlibrary(zip)\nlibrary(patchwork)\nlibrary(ggrepel)\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(magick)\n\n\n\nManually Download us_house_votes.csv and president.csv.\nFollowing the project instructions, I manually downloaded two key datasets from the MIT Election Data Science Lab using my web browser. The us_house_votes.csv for U.S. House election votes (1976-2022) and president.csv for presidential votes (1976-2020). I saved these locally and loaded them into R for the analysis. I also used suppressMessages for cleaner output.\n\n# Load votes data and election data\nhouse_vote_data &lt;- suppressMessages(read_csv(\"mp03data/1976-2022-house.csv\"))\npresidential_election_data &lt;- suppressMessages(read_csv(\"mp03data/1976-2020-president.csv\"))\n\n\n\n\n\n\n\nTask 1: Download Congressional Shapefiles 1976-2012\n\n\n\nDownload data from 1976-2012\nNow I will automate download and extraction of shapefiles that contain the boundaries of U.S. congressional districts for each election year from 1976 to 2012. The data is sourced from Here For instance, the 94th Congress, spanning January 14, 1975, to October 1, 1976, is labeled ‘094’ in file names so I’ll use sprintf to format each Congress number as a three-digit code (e.g., ‘094’ for the 94th Congress), applying the same approach for all election years. Additionally, I will store the file location for each year.\n\n# Create a empty list - congressional_boundaries_shape_file_location_map to store, to store the mapping of each election year to the download location of its corresponding shapefile.\n\ncongressional_boundaries_shape_file_location_map &lt;- list()\n\n# Now I’m setting the starting year as 1976, as specified in the project guidelines.\nstarting_year &lt;- 1976\n\n# I created the get_congressional_boundaries_shape_files() function to automate downloading, storing, and extracting shapefiles for each congressional session based on the specified Congress number. Also,use sprintf.\n\nget_congressional_boundaries_shape_files &lt;- function(congress) {\n  congress_number &lt;- sprintf(\"%03d\", congress)\n\n  \n# Set base URL and file name pattern\n  base_url &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  file_name &lt;- paste0(\"districts\", congress_number, \".zip\")\n  \n# file_download_location is a local path where each zip file will be saved.\n  file_download_location &lt;- paste0(\"mp03data/zip_files/\", congress_number, \".zip\")\n  \n# To calculate the election year based on the Congress number.\n  election_year &lt;- starting_year + (congress - 94) * 2\n\n# store file location for given year\n  congressional_boundaries_shape_file_location_map[[election_year]] &lt;&lt;- file_download_location\n  \n\n# Unzip and load the shape file\n  unzip_dir &lt;- paste0(\"mp03data/shp_data/\", congress_number, \"/\")\n\n\n# Only download if the file does not already exist\n  if (!file.exists(file_download_location)) {\n    file_url &lt;- paste0(base_url, file_name)\n    dir.create(\"mp03data/zip_files\", recursive = TRUE)\n    download.file(file_url, destfile = file_download_location, mode = \"wb\")\n  }\n\n  unzip(file_download_location, exdir = unzip_dir)\n}\n\n# This loop iterates each Congress from 1976 to 2012, covering the period from 1976 (94th Congress) to 2012 (112th Congress). \nfor (congress in 94:112) { \n  get_congressional_boundaries_shape_files(congress)\n}\n\n\n\n\n\n\n\n\n\nTask 2: Download Congressional Shapefiles 2014-2022\n\n\n\nDownload Congressional data from 2014-2022\nlet’s ensure that the data for congressional boundaries from 2014 to 2022 is downloaded and organized efficiently, ready for further use in analysis. Also, the name of the files Here are ending with these numbers- “118”, “116”, “115”, “114” so, I checked each version in sequence until it finds one that works for a specific year.\n\n# This will be the template for the URL where the shapefiles are stored\nbase_url_template &lt;- \"https://www2.census.gov/geo/tiger/TIGER%d/CD/tl_%d_us_cd%s.zip\"\n\n# The code will try each version in sequence until it finds one that works for a specific year.\nversions &lt;- c(\"118\", \"116\", \"115\", \"114\")  \n\n# Specified the folder where the files will be saved.\ndownload_dir &lt;- \"mp03data\"\n\n# Create directory for saving shapefiles if it doesn't exist\nif (!dir.exists(download_dir)) {\n  dir.create(download_dir)\n}\n# Now Looping through years 2014:2022\nfor (year in 2014:2022) {\n  file_downloaded &lt;- FALSE\n\n# Try each version until one succeeds\n  for (version in versions) {\n# Construct the file name and URL using sprintf\n    file_name &lt;- paste0(\"congressional_districts_\", year, \"_cd\", version, \".zip\")\n    file_path &lt;- file.path(download_dir, file_name)\n    url &lt;- sprintf(base_url_template, year, year, version)\n    \n    # Check if file already exists to avoid re-downloading\n    if (!file.exists(file_path)) {\n      # Perform the download\n      response &lt;- GET(url)\n\n      if (status_code(response) == 200) {\n        # Write the downloaded content to a file\n        writeBin(content(response, \"raw\"), file_path)\n\n           # store file location for given year\n        \n  congressional_boundaries_shape_file_location_map[[year]] &lt;- file_path\n\n      }\n    } else {\n       # store file location for given year\n  congressional_boundaries_shape_file_location_map[[year]] &lt;- file_path\n  \n      break\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\nTask 3: Exploration of Vote Count Data Answer the following using the vote count data files from the MIT Election Data Science Lab. You may answer each with a table or plot as you feel is appropriate.\n\n\n\n\n3.1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\nThis identifies the states with the largest gains and losses in seats and visualizes these changes with a bar plot. Firstly, I filtered the data to include seat counts for each state in those years. I calculated the difference in seats (seat_change) for each state and identified the top 10 states with the largest gains and losses Then, I created a bar plot showing these changes, using green bars for gains and red bars for losses, with each bar labeled by the number of seats changed. Please follow the short notes with the code to understand it.\n\n# Load the data from year 1976 to 2022- representing U.S. House elections\nseats_by_state &lt;- house_vote_data |&gt;\n  filter(year %in% c(1976, 2022), office == \"US HOUSE\") |&gt;\n  group_by(state, year) |&gt;\n  summarise(seat_count = n_distinct(district), .groups = \"drop\")\n\n# calculate the change in seats between 1976 and 2022 for each state.\nseat_changes &lt;- seats_by_state |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  pivot_wider(names_from = year, values_from = seat_count, names_prefix = \"year_\")\n\n# Let's create new column to show the difference in seats\ntop_gains_losses &lt;- seat_changes |&gt;\n  mutate(seat_change = year_2022 - year_1976) |&gt;\n  arrange(desc(seat_change)) |&gt;\n  filter(row_number() &lt;= 10 | row_number() &gt; (n() - 10))\n\n# To show the plot, mapping the states (state) to the x-axis and seat_change to the y-axis.\nggplot(top_gains_losses, aes(x = reorder(state, seat_change), y = seat_change, fill = seat_change &gt; 0)) +\n  geom_bar(stat = \"identity\", color = \"black\") +\n  coord_flip() +\n  geom_text(aes(label = seat_change), hjust = ifelse(top_gains_losses$seat_change &gt; 0, -0.2, 1.2), size = 3) +\n  scale_fill_manual(values = c(\"red\", \"green\"), labels = c(\"Loss\", \"Gain\"), guide = guide_legend(title = \"Seat Change\")) +\n  labs(\n    title = \"Top 10 Seat Gains and Losses in the US House of Representatives (1976-2022)\",\n    x = \"State\", y = \"Seat Change\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 10, face = \"bold\"),\n    axis.text.y = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nThis chart shows the top 10 states with the most significant changes in U.S. House seats from 1976 to 2022. Texas and Florida gained the most seats (14 and 13), reflecting population growth in the South and West. In contrast, New York and Ohio lost the most seats (13 and 8), indicating population decline in the Northeast and Midwest. This shift redistributes political influence toward states with growing populations.\n\n\n3.2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nlet’s analyze New York’s U.S. House elections (1976 onward) to see if any outcomes would have differed without the “fusion” voting system. It calculates each candidate’s total votes (across all party lines) and votes from only their major party line (Democrat or Republican). It then compares the actual winner (fusion votes) with a hypothetical winner (major party votes only) and displays cases where the outcome would have changed if fusion votes weren’t counted.\n\nlibrary(dplyr)\n# Filter for New York and US HOUSE elections of  Representatives from 1976 onwards\nny_elections &lt;- house_vote_data |&gt;\n  filter(state == \"NEW YORK\", office == \"US HOUSE\", year &gt;= 1976)\n\n# Identify major party line (Democrat or Republican) and calculate total votes for each candidate\ncandidate_votes &lt;- ny_elections |&gt;\n  mutate(is_major_party = ifelse(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\"), TRUE, FALSE)) |&gt;\n  group_by(year, district, candidate) |&gt;\n  summarise(\n    total_votes = sum(candidatevotes),\n    major_party_votes = sum(candidatevotes[is_major_party]),\n    .groups = \"drop\"\n  )\n\n# Determine the actual winner and hypothetical winner if only major party line votes were counted\ncandidate_votes |&gt;\n  group_by(year, district) |&gt;\n  summarise(\n    fusion_winner = candidate[which.max(total_votes)],\n    major_party_winner = candidate[which.max(major_party_votes)],\n    fusion_votes = max(total_votes),\n    major_party_votes = max(major_party_votes),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(fusion_winner != major_party_winner) |&gt;\n  select(year, district, fusion_winner, major_party_winner, fusion_votes, major_party_votes) |&gt;\n  DT::datatable()\n\n\n\n\n\n“fusion_winner” is the candidate who actually won with combined votes from all party lines. “major_party_winner” is the hypothetical winner who would have won if only votes from major parties were counted.\n\n\n3.3.Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\nlet’s compare the total votes for Democratic and Republican candidates in presidential and congressional elections (U.S. House) during presidential election years (1976 to 2020) and how this trend varies over time.\n\n# Define presidential election years from 1976 to 2020 for our analysis\npresidential_years &lt;- seq(1976, 2020, by = 4)\n\n# let's use filter and group by to sum presidential votes by state, year, and party\npresidential_votes &lt;- presidential_election_data |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\"), year %in% presidential_years) |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  mutate(party = party_simplified) |&gt;\n  select(year, state, party, total_votes)\n\n# Here I am trying to filter for U.S. House elections in presidential years, considering only Democratic and Republican votes\ncongressional_votes &lt;- house_vote_data |&gt;\n  filter(office == \"US HOUSE\", party %in% c(\"DEMOCRAT\", \"REPUBLICAN\"), year %in% presidential_years) |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\")\n\n\n# Rename columns to differentiate between President and Congress election data\npresidential_votes &lt;- presidential_votes |&gt;\n  mutate(election_type = \"President\")\n\ncongressional_votes &lt;- congressional_votes |&gt;\n  mutate(election_type = \"Congress\")\n\n# Combine presidential and congressional votes into a single dataset- combined_votes\ncombined_votes &lt;- bind_rows(presidential_votes, congressional_votes)\n\nvotes_summary &lt;- combined_votes |&gt;\n  group_by(year, party, election_type) |&gt;\n  summarize(total_votes = sum(total_votes), .groups = \"drop\")\n\n# Now lets combine into one interaction_label for plotting\nvotes_summary &lt;- votes_summary |&gt;\n  mutate(interaction_label = factor(interaction(party, election_type),\n                                    levels = c(\"DEMOCRAT.President\", \"DEMOCRAT.Congress\",\n                                               \"REPUBLICAN.President\", \"REPUBLICAN.Congress\")))\n\n# Plot the data \nggplot(votes_summary, aes(x = year, y = total_votes, color = interaction_label)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Total Votes by Party and Election Type (Presidential and Congressional)\",\n    x = \"Year\",\n    y = \"Total Votes\",\n    color = \"Party and Election Type\"\n  ) +\n  scale_color_manual(\n    values = c(\n      \"DEMOCRAT.President\" = \"blue\",\n      \"DEMOCRAT.Congress\" = \"darkblue\",\n      \"REPUBLICAN.President\" = \"red\",\n      \"REPUBLICAN.Congress\" = \"darkred\"\n    ),\n    labels = c(\n      \"Democrat - President\",\n      \"Democrat - Congress\",\n      \"Republican - President\",\n      \"Republican - Congress\"\n    )\n  ) +\n  # Use comma format for y-axis labels- for readability\n  scale_y_continuous(labels = scales::comma) + \n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 10),\n    axis.text.x = element_text(angle = 40, hjust = 1)\n  )\n\n\n\n\n\n\n\n\nOver time, total votes for both presidential and congressional races have steadily increased, reflecting higher voter turnout or population growth.\n\n\n\n\n\n\n\n\n\nTask 4: Automate Zip File Extraction Adapt the code after the ##- symbol above into a function read_shp_from_zip() which takes in a file name, pulls out the .shp file contained there in, and reads it into R using read_sf().\n\n\n\nThe purpose of this process is to make it easier to work with shapefiles that are stored in zip files. Instead of manually unzipping the file, locating the shapefile, and loading it into R, this function does it all in one step. This approach is helpful because I will be working with geographic data from various sources.\n\n# Define a function for automated downloading and reading of shapefiles\n\nread_shp_from_zip &lt;- function(zip_file) {\n  \n  # Create a temporary directory to extract files\n  temp_dir &lt;- tempdir()\n\n  # Extract all contents of the zip file to the temporary directory\n  zip_contents &lt;- unzip(zip_file, exdir = temp_dir)\n  \n  # Defining the path where shapefiles should be located, ensures shapefiles are matched  \n  shp_file_location &lt;- paste0(temp_dir, \"/districtShapes/\")\n  shp_files &lt;- list.files(shp_file_location, pattern = \"\\\\.shp$\", full.names = TRUE)\n\n\n  # Check if a .shp file is found \n  if (length(shp_files) == 0) {\n    stop(\"No .shp file found in the zip archive.\")\n  }\n\n  # Read the .shp file as a simple feature collection\n  shp_sf &lt;- suppressMessages(read_sf(shp_files[1])) # Read the first .shp file if multiple are found\n\n  # This will return the spatial data (shp_sf) so it can be used in further analysis or visualization.\n  return(shp_sf)\n}\n\n\n\n\n\nCustom Functions\nLet’s create custom functions that will be useful later for the analysis.\n\n\nCreate function to get election year data.\nThis function will give us all the data related to an election year. Things like total votes, electoral vote count. It will extract data for that year, focusing on the two main parties (Democratic and Republican).\n\n# This represents which election year I wanna analyze\ncreate_election_year_data &lt;- function(year_to_look_for) {\n  \n  # Filter presidential data for the specified year and the two main parties\n  president_data_for_given_year &lt;- presidential_election_data |&gt;\n    filter(\n      year == year_to_look_for,\n      office == \"US PRESIDENT\",\n      party_simplified %in% c(\"DEMOCRAT\", \"REPUBLICAN\")\n    ) |&gt;\n    # This groups the data by state\n    group_by(state) |&gt;\n    # Selects candidate with max vote\n    slice_max(candidatevotes, n = 1, with_ties = FALSE) |&gt;\n    # Select the relevant columns\n    select(state, state_po, candidate, party_simplified, candidatevotes)\n\n  # Filter congressional vote count data for the specified year and summarize Electoral College Votes (ECV)\n  ecv_data_for_given_year &lt;- house_vote_data |&gt;\n    filter(year == year_to_look_for, office == \"US HOUSE\") |&gt;\n    group_by(state) |&gt;\n    summarize(\n      num_representatives = n_distinct(district),\n      # +2 is to count 2 senators for each state\n      ecv = num_representatives + 2,\n      .groups = \"drop\"\n    ) |&gt;\n    # Add DC manually with its 3 electoral votes\n    bind_rows(data.frame(state = \"DISTRICT OF COLUMBIA\", ecv = 3))\n\n  # Combine presidential data with ECV data and format state names\n  combined_data &lt;- president_data_for_given_year |&gt;\n    # Left_join will merge \n    left_join(ecv_data_for_given_year, by = \"state\") |&gt;\n    # Formats state names to title case\n    mutate(state = str_to_title(state))\n\n  return(combined_data)\n}\n\nThis code will help to retrieve relevant information for election analysis, comparisons, or visualization.\n\n\nFunction to get shp_data for a given year.\nThis code is designed to prepare state-level shapefile data for a specified year.\n\n# Let's create function to take an argument(year)\ncreate_state_shp_data &lt;- function(year) {\n  # find file location for a given year\n  file_location_for_given_year &lt;- congressional_boundaries_shape_file_location_map[[year]] \n  # This calls the previously defined function (read_shp_from_zip) to extract and load the shapefile data for the specified year\n  shp_data_for_given_year &lt;- read_shp_from_zip(file_location_for_given_year)\n\n  # Standardize STATENAME to title case\n  congressional_districts_for_given_year &lt;- shp_data_for_given_year |&gt;\n    mutate(STATENAME = str_to_title(trimws(STATENAME))) |&gt; # Convert to title case (e.g., \"new york\" becomes \"New York\")\n    rename(state = STATENAME) |&gt; # Rename column to 'state' for consistency\n    select(state, geometry) |&gt; #select the given column\n    st_make_valid()\n\n# Let's aggregate the individual congressional districts into a single geometry for each state\n  state_level_shape_for_given_year &lt;- congressional_districts_for_given_year |&gt;\n    group_by(state) |&gt; # groupby state\n    reframe(geometry = st_union(geometry)) |&gt; # Use reframe to handle ungrouping automatically\n    distinct(state, .keep_all = TRUE) # Ensure unique entries remain for each state \n\n  return(state_level_shape_for_given_year)\n}\n\nThe create_state_shp_data function efficiently retrieves and processes shapefile data, combining individual congressional districts into state-level geometries for a specified year. This function is useful for generating state-level spatial data for maps and analyses where district-level detail is not required, ensuring consistency in state names and valid geometry.\n\n\nDefine function to create and save a map for a specific year\nNow working to create a customized election map for a given year. This map visualizes each state, showing which party won the state’s electoral votes, with color-coded states representing the winning party (Democrat, Republican, or Other). Also, specialized labeling for small northeastern states, and inset maps for Alaska and Hawai is done.\n\ncreate_election_map &lt;- function(election_year_data, shp_data_for_year, year_num, file_name = NULL) {\n  yearly_data &lt;- shp_data_for_year |&gt;\n    inner_join(election_year_data, by = \"state\")\n\n  # Define the small northeastern states to display abbreviations outside\n  yearly_data &lt;- yearly_data |&gt;\n    mutate(\n      state_abbr = ifelse(state %in% c(\n        \"Connecticut\", \"Delaware\", \"Maryland\", \"Massachusetts\",\n        \"New Jersey\", \"New Hampshire\", \"Rhode Island\", \"Vermont\"\n      ), state_po, NA), # Use abbreviations only for small northeastern states\n      label_text = ifelse(!is.na(state_abbr), paste0(state_abbr, \" \", ecv), NA)\n    )\n\n  northeastern_states &lt;- c(\n    \"Connecticut\", \"Delaware\", \"Maryland\", \"Massachusetts\",\n    \"New Jersey\", \"New Hampshire\", \"Rhode Island\", \"Vermont\"\n  )\n\n  if (!inherits(yearly_data, \"sf\")) {\n    yearly_data &lt;- st_as_sf(yearly_data)\n  }\n\n\n  # Prepare the main map excluding Alaska and Hawaii\n  mainland &lt;- yearly_data |&gt; filter(!state %in% c(\"Alaska\", \"Hawaii\"))\n  alaska &lt;- yearly_data |&gt; filter(state == \"Alaska\")\n  hawaii &lt;- yearly_data |&gt; filter(state == \"Hawaii\")\n\n\n  mainland_plot &lt;- ggplot(mainland) +\n    geom_sf(aes(fill = party_simplified)) +\n\n    # Only show electoral votes for non-northeastern states\n    geom_sf_text(\n      data = mainland |&gt; filter(!state %in% northeastern_states),\n      aes(label = ecv), size = 6, color = \"black\", fontface = \"bold\"\n    ) +\n\n    # Add labels for small northeastern states with electoral counts outside\n    geom_text_repel(\n      data = yearly_data |&gt; filter(!is.na(state_abbr)), # Only for small northeastern states\n      aes(\n        geometry = geometry,\n        label = label_text\n      ),\n      stat = \"sf_coordinates\", # Use spatial coordinates for label placement\n      size = 4,\n      color = \"black\",\n      fontface = \"bold\", # Make labels bold\n      nudge_x = 5, # Set a strong rightward nudge to move all labels to the right\n      nudge_y = 0, # Keep y-nudging minimal for alignment\n      hjust = 0, # Left-align labels\n      direction = \"y\", # Keep lines vertically aligned\n      lineheight = 0.9 # Adjust line height for clarity if needed\n    ) +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    labs(\n      title = paste0(year_num, \" Presidential Election - Electoral College Results\"),\n      fill = \"Winner\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 20, face = \"bold\"), # Bold title\n      legend.position = \"bottom\"\n    ) +\n    coord_sf(expand = FALSE)\n\n  # Alaska plot without legend\n  alaska_plot &lt;- ggplot(alaska) +\n    geom_sf(aes(fill = party_simplified)) +\n    geom_sf_text(aes(label = ecv), size = 6, color = \"black\", fontface = \"bold\") +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    theme_void() +\n    theme(legend.position = \"none\") + # Hide legend in Alaska inset\n    coord_sf(xlim = c(-180, -130), ylim = c(50, 72), expand = FALSE)\n\n  # Hawaii plot without legend\n  hawaii_plot &lt;- ggplot(hawaii) +\n    geom_sf(aes(fill = party_simplified)) +\n    geom_sf_text(aes(label = ecv), size = 6, color = \"black\", fontface = \"bold\") +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    theme_void() +\n    theme(legend.position = \"none\") + # Hide legend in Hawaii inset\n    coord_sf(xlim = c(-161, -154), ylim = c(18, 23), expand = FALSE)\n\n  # Combine plots using patchwork with adjusted insets on bottom left\n  combined_plot &lt;- mainland_plot +\n    inset_element(alaska_plot, left = 0.05, bottom = 0.1, right = 0.25, top = 0.3) + # Alaska on bottom left\n    inset_element(hawaii_plot, left = 0.05, bottom = 0.05, right = 0.25, top = 0.15) # Hawaii below Alaska on bottom left\n  # Save plot\n  \n   if (is.null(file_name)) {\n      ggsave(filename = paste0(\"mp03data/election_maps/election_map_\", year_num, \".png\"), plot = combined_plot, width = 16, height = 12, dpi = 300)\n   }else{\n     ggsave(filename = file_name, plot = combined_plot, width = 16, height = 12, dpi = 300)\n   }\n  \n}\n\nNow it efficiently generates an electoral map for a specified year, incorporating color-coded party winners, labels for small northeastern states, and Alaska and Hawaii as inset maps. This automation produces detailed visualizations of election results, making it ideal for presentations or reports on electoral data.\n\n\n\n\n\n\nTask 5: Chloropleth Visualization of the 2000 Presidential Election Electoral College Results\n\n\n\nI will now generate an electoral map for the 2000 presidential election. I have also included a step-by-step explanation of what each line does:\n\n# Calls the function with 2000 as the argument to generate data for the 2000 presidential election\nelection_year_data &lt;- create_election_year_data(2000)\n# The argument to create spatial data for each state in 2000 by combining congressional district geometries into state-level boundaries.\nelection_year_shp_data &lt;- create_state_shp_data(2000)\n# To create and save a map of the 2000 presidential election\ncreate_election_map(election_year_data, election_year_shp_data, 2000, \"election_map_2000.png\")\n# Display the Map in the Document\nknitr::include_graphics(\"election_map_2000.png\")\n\n\n\n\n\n\n\n\nThis map shows the results of the 2000 U.S. Presidential Election by state, indicating the winner in each state along with the electoral college votes (ECV) awarded Where Red: States won by the Republican candidate. Blue: States won by the Democratic candidate.\n\n\n\n\n\n\n\n\nTask 6: Advanced Chloropleth Visualization of Electoral College Results} ### 6. Modify your previous code to make either an animated faceted version showing election results over time. You may want to set facet_wrap or facet_grid to use a single column and adjust the figure size for the best reading experience\n\n\n\nHere I will created an animated, faceted map to visualize U.S. presidential election results over time. I prepared the election and geographic data for each election year from 1976 to 2020, merging shapefiles of U.S. states with election results to color each state by the winning party. Using facet_wrap, I created a one-column layout to display each year individually, while transition_manual provided smooth transitions between years for animation. Finally, I saved the animation as a GIF, adjusting plot settings to produce a clean, focused view of each year’s results, effectively illustrating shifts in electoral outcomes over time.\n\n# Define a function to generate and save an animated, faceted electoral map\ncreate_animated_facet_map &lt;- function(all_year_data) {\n  \n  # Combine data from all years into one data frame for animation\n  combined_data &lt;- bind_rows(all_year_data)\n  \n  # Check if the combined_data is in the correct format (as an sf object)\n  if (!inherits(combined_data, \"sf\")) {\n    combined_data &lt;- st_as_sf(combined_data)\n  }\n  \n  # Set up the plot using ggplot2 with faceting for each year\n  p &lt;- ggplot(combined_data) +\n    geom_sf(aes(fill = party_simplified), color = NA) +\n    scale_fill_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\", \"Other\" = \"gray\")) +\n    labs(\n      title = \"U.S. Presidential Election Results by State\",\n      fill = \"Winning Party\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n      legend.position = \"bottom\",\n      axis.text = element_blank(),         # Remove coordinate labels\n      axis.ticks = element_blank(),        # Remove axis ticks\n      panel.grid = element_blank()         # Remove grid lines\n    ) +\n    facet_wrap(~year, ncol = 1, scales = \"fixed\") +  # Single column, fixed scales\n    coord_sf(xlim = c(-125, -66), ylim = c(24, 50), expand = FALSE) +  # Adjusted limits for U.S.\n    transition_manual(year)\n\n  # Animate the plot with increased dimensions\n  animated_plot &lt;- animate(p, nframes = length(unique(combined_data$year)), fps = 1, width = 1000, height = 1200, res = 150)\n  \n  # Save the animation as a GIF\n  anim_save(\"election_results_facet_animation.gif\", animated_plot)\n}\n\n# Loop to prepare the data for each year and call the function to animate\nyears &lt;- seq(1976, 1978, by = 4)\nall_year_data &lt;- list()\n\nfor (year in years) {\n  # Assuming create_state_shp_data and create_election_year_data functions exist as per Task 5\n  shp_data_for_year &lt;- create_state_shp_data(year)\n  election_year_data &lt;- create_election_year_data(year)\n\n  # Combine shapefile data with election data\n  yearly_data &lt;- shp_data_for_year %&gt;%\n    left_join(election_year_data, by = \"state\") %&gt;%\n    mutate(year = year)  # Add year column for facetting\n\n  # Append yearly data to the list for later animation\n  all_year_data[[as.character(year)]] &lt;- yearly_data\n}\n\n# Call the animation function\n# create_animated_facet_map(all_year_data)\n\n# Display the animation\nknitr::include_graphics(\"election_results_facet_animation.gif\")\n\n\n\n\n\n\n\n\nResult shows the animated, faceted visualization highlights the shifting political landscape of the U.S. over several decades, providing a clear view of how electoral outcomes have changed across different presidential elections.\n\n\n\n\n\n\nTask 7: Evaluating Fairness of ECV Allocation Schemes\n\n\n\n\n7.1. State-Wide Winner-Take-All\nLet’s calculate State-Wide Winner-Take-All to examine which party receives all of a state’s ECVs based on a simple majority, we can understand potential biases in the current system and assess its representativeness.\n\n# Filter house_vote_data to only include presidential election years (every 4 years)\nhouse_vote_data_president_election_year &lt;- house_vote_data |&gt;\n  filter(year %% 4 == 0)\n\n# Filter presidential_election_data to only include presidential election years (every 4 years)\npresidential_election_data &lt;- presidential_election_data |&gt;\n  filter(year %% 4 == 0)\n\n# Deduct ECV data from the filtered congressional data and add DC manually\necv_data &lt;- house_vote_data_president_election_year |&gt;\n  filter(office == \"US HOUSE\") |&gt;\n  group_by(state, year) |&gt;\n  summarize(\n    num_representatives = n_distinct(district),\n    ecv = num_representatives + 2,\n    .groups = \"drop\"\n  ) |&gt;\n  bind_rows(data.frame(state = \"DISTRICT OF COLUMBIA\", year = unique(house_vote_data_president_election_year$year), ecv = 3))\n\n# Ensure candidate votes are numeric in presidential_election_data\npresidential_election_data &lt;- presidential_election_data |&gt;\n  mutate(candidatevotes = as.numeric(candidatevotes))\n\n# 1. State-Wide Winner-Take-All Allocation\nwinner_take_all &lt;- presidential_election_data |&gt;\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  left_join(ecv_data, by = c(\"state\", \"year\")) |&gt;\n  mutate(ecv_allocation = ecv, allocation_scheme = \"State-Wide Winner-Take-All\") |&gt;\n  select(year, state, party_simplified, ecv_allocation, allocation_scheme)\n\nwinner_take_all_summary &lt;- winner_take_all |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ecv_allocation), .groups = \"drop\")\n\n# I am sorting and searching\nwinner_take_all_summary |&gt;\n  DT::datatable()\n\n\n\n\n\nThe table displayed here shows the results of the State-Wide Winner-Take-All allocation system for each U.S. presidential election year. The fluctuations in total ECVs show the changing dominance of parties over time and how closely contested certain election years were.\n\n\n7.2. District-Wide Winner-Take-All + State-Wide “At Large” Votes\nlet’s calculate District-Wide Winner-Take-All + State-Wide “At Large” Votes. This will help to evaluate how district-level and at-large ECV allocations could impact the distribution of ECVs in U.S. presidential elections compared to a purely winner-take-all state system.\n\n# 1. Calculate District-Level ECV Allocation for Each State (excluding DC since it has no districts)\ndistrict_winner_take_all &lt;- house_vote_data_president_election_year |&gt;\n  filter(office == \"US HOUSE\", state != \"DISTRICT OF COLUMBIA\") |&gt; # Only states with congressional districts\n  group_by(year, state, district) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes in each district\n  ungroup() |&gt;\n  mutate(ecv_allocation = 1) |&gt; # Each district winner receives 1 ECV\n  mutate(party_simplified = party)\n\n# 2. Calculate Statewide \"At Large\" ECV Allocation for Each State\nat_large_votes &lt;- presidential_election_data |&gt;\n  filter(state != \"DISTRICT OF COLUMBIA\") |&gt; # Exclude DC here for now\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes statewide\n  ungroup() |&gt;\n  mutate(ecv_allocation = 2) # Statewide winner receives 2 \"at-large\" ECVs\n\n# 3. Add DC’s Fixed 3 ECVs with Winner-Take-All Allocation\ndc_winner_take_all &lt;- presidential_election_data |&gt;\n  filter(state == \"DISTRICT OF COLUMBIA\") |&gt; # Only DC\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes in DC\n  ungroup() |&gt;\n  mutate(ecv_allocation = 3) # DC winner receives all 3 ECVs\n\n# 4. Combine District-Level, Statewide At-Large, and DC Allocations for All States\nnationwide_district_allocation &lt;- bind_rows(district_winner_take_all, at_large_votes, dc_winner_take_all) |&gt;\n  mutate(allocation_scheme = \"Nationwide District-Wide Winner-Take-All + At-Large\") |&gt;\n  select(year, state, district, party_simplified, ecv_allocation, allocation_scheme)\n\n# Summarize the results by year and party to see the national impact\ndistrict_allocation_summary &lt;- nationwide_district_allocation |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ecv_allocation), .groups = \"drop\") |&gt;\n  arrange(year, desc(total_ecv))\n\n# Display the national summary to see the overall outcome by party\ndistrict_allocation_summary |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n7.3. State-Wide Proportional Allocation\nThis code calculates Electoral College Votes (ECV) using a State-Wide Proportional Allocation. We first determine each party’s vote share within each state, then allocate ECVs proportionally. Afterward, we adjust for any rounding differences to ensure each state’s total ECV is correct. Finally, we summarize the results nationally to compare total ECVs for each party in each election year, offering a fairer reflection of each party’s support within states.\n\n# Join proportional_state with ecv_data to get total ECVs for each state-year combination\nproportional_state &lt;- presidential_election_data |&gt;\n  group_by(year, state, party_simplified) |&gt;\n  summarize(total_party_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  left_join(ecv_data, by = c(\"state\", \"year\")) |&gt;\n  group_by(year, state) |&gt;\n  mutate(\n    vote_share = total_party_votes / sum(total_party_votes), # Calculate each party's vote share\n    proportional_ecv = round(vote_share * ecv), # Proportionally allocate ECVs\n    allocation_scheme = \"State-Wide Proportional\"\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, state, party_simplified, ecv, proportional_ecv, allocation_scheme)\n\n# Adjust to ensure each state's total ECV allocation matches exactly\nproportional_state_adjusted &lt;- proportional_state |&gt;\n  group_by(year, state) |&gt;\n  mutate(\n    adjustment = ecv - sum(proportional_ecv), # Calculate any rounding difference\n    proportional_ecv = if_else(row_number() == 1, proportional_ecv + adjustment, proportional_ecv) # Adjust the first row\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, state, party_simplified, proportional_ecv, allocation_scheme)\n\n# Summarize the results by year and party to see the national impact\nproportional_state_summary &lt;- proportional_state_adjusted |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(proportional_ecv), .groups = \"drop\") |&gt;\n  arrange(year, desc(total_ecv))\n\n# Display the national summary to see the overall outcome by party\nproportional_state_summary |&gt;\n  DT::datatable()\n\n\n\n\n\nThe State-Wide Proportional Allocation method more accurately reflects voter support by allocating Electoral College Votes based on each party’s share of votes within each state.\n\n\n7.4. National Proportional\nThis code allocates Electoral College Votes (ECVs) based on each party’s national vote share. It calculates the total votes per party each year, determines each party’s share of the national vote, and then allocates ECVs proportionally.\n\n# Total ECVs for the nation\ntotal_ecv_nationwide &lt;- 538 # Includes 3 for DC\n\n# Step 1: Calculate the national vote totals for each candidate per year\nnational_vote_totals &lt;- presidential_election_data |&gt;\n  group_by(year, party_simplified) |&gt;\n  summarize(national_votes = sum(candidatevotes), .groups = \"drop\")\n\n# Step 2: Calculate each candidate's national vote share\nnational_vote_shares &lt;- national_vote_totals |&gt;\n  group_by(year) |&gt;\n  mutate(\n    national_vote_share = national_votes / sum(national_votes) # Calculate national vote share\n  ) |&gt;\n  ungroup()\n\n# Step 3: Allocate ECVs based on national vote share and adjust for rounding differences\nnational_proportional_ecv &lt;- national_vote_shares |&gt;\n  mutate(\n    proportional_ecv = round(national_vote_share * total_ecv_nationwide), # Allocate ECVs proportionally\n    allocation_scheme = \"National Proportional\"\n  ) |&gt;\n  group_by(year) |&gt;\n  mutate(\n    adjustment = total_ecv_nationwide - sum(proportional_ecv), # Calculate any rounding difference\n    total_ecv = if_else(row_number() == 1, proportional_ecv + adjustment, proportional_ecv) # Adjust the first row\n  ) |&gt;\n  ungroup() |&gt;\n  select(year, party_simplified, total_ecv, allocation_scheme)\n\n\n# Summarize the results by year to see the national distribution by party\nnational_proportional_summary &lt;- national_proportional_ecv |&gt;\n  arrange(year, desc(total_ecv))\n\n# Display the national summary\nnational_proportional_summary |&gt;\n  DT::datatable()\n\n\n\n\n\nThe result is a table showing ECV distribution by party and year under a proportional system, aiming to reflect popular support more accurately.\n\n\nBased on these allocation strategies, compare the winning presidential candidate with the actual historical winner.\nlet’s identify the historical Electoral College winner for each U.S. presidential election year based on actual state-wide, winner-take-all allocations.\n\nstate_wide_winners &lt;- presidential_election_data |&gt;\n  group_by(year, state) |&gt;\n  slice_max(candidatevotes, with_ties = FALSE) |&gt; # Get the candidate with the most votes in each state\n  ungroup()\n\n# Step 2: Join with `ecv_data` to get ECVs for each state-year combination\nstate_wide_winners_with_ecv &lt;- state_wide_winners |&gt;\n  left_join(ecv_data, by = c(\"state\", \"year\")) |&gt; # Add ECVs from `ecv_data`\n  group_by(year, party_simplified) |&gt;\n  summarize(total_ecv = sum(ecv), .groups = \"drop\") # Sum ECVs per candidate per year\n\n# Step 3: Identify the actual historical winner for each year\nhistorical_winner &lt;- state_wide_winners_with_ecv |&gt;\n  group_by(year) |&gt;\n  slice_max(total_ecv, with_ties = FALSE) |&gt; # Get the candidate with the most ECVs in each year\n  ungroup() |&gt;\n  rename(actual_winner = party_simplified, actual_ecv = total_ecv) |&gt;\n  select(year, actual_winner, actual_ecv)\n\n# Display the historical winners to verify\nhistorical_winner |&gt;\n  DT::datatable()\n\n\n\n\n\nThis output table shows the actual winning party and ECV total per election year, reflecting the historical outcomes in the Electoral College system.\n\n\nConsolidate Results from All Allocation Strategies\nNow,let’s evaluate the consistency of various Electoral College allocation strategies by comparing the winning party in each strategy with the historical winner.\n\n# Combine all strategies into a single dataset and find the winner for each\nwinners_by_strategy &lt;- bind_rows(\n  winner_take_all_summary |&gt; mutate(strategy = \"State-Wide Winner-Take-All\"),\n  district_allocation_summary |&gt; mutate(strategy = \"District-Wide Winner-Take-All + At-Large\"),\n  proportional_state_summary |&gt; mutate(strategy = \"State-Wide Proportional\"),\n  national_proportional_summary |&gt; mutate(strategy = \"National Proportional\")\n) |&gt;\n  group_by(year, strategy) |&gt;\n  slice_max(total_ecv, with_ties = FALSE) |&gt; # Select candidate with the most ECVs per strategy\n  ungroup() |&gt;\n  select(year, strategy, party_simplified, total_ecv) |&gt;\n  rename(winning_party = party_simplified, winning_ecv = total_ecv)\n\n# 4. Compare Each Strategy’s Winner with the Actual Historical Winner\ncomparison_results &lt;- winners_by_strategy |&gt;\n  left_join(historical_winner, by = \"year\") |&gt; # Join with the actual historical winner data\n  mutate(\n    match_with_actual = if_else(winning_party == actual_winner, \"Match\", \"Different\")\n  ) |&gt;\n  select(year, strategy, winning_party, winning_ecv, actual_winner, actual_ecv, match_with_actual)\n\n\n# 5. Summarize Results by Strategy\nsummary_results &lt;- comparison_results |&gt;\n  group_by(strategy, match_with_actual) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = match_with_actual, values_from = count, values_fill = 0)\n\nsummary_results |&gt;\n  DT::datatable()\n\n\n\n\n\nThe table shows that the State-Wide Winner-Take-All strategy matches historical results exactly with 12 matches and no differences. In contrast, the District-Wide Winner-Take-All + At-Large, National Proportional, and State-Wide Proportional strategies each show 10 matches and 2 differences, indicating that these alternative methods could have changed the election outcome in 2 instances.\n\n\nCreate a comparison table that shows the winner for each strategy and the actual historical winner\nThis code creates a comparison table that shows the actual historical election winner alongside the winners predicted by each voting strategy (Winner-Take-All, District-Wide + At-Large, State-Wide Proportional, and National Proportional) for each year. It reshapes the data, joins it with historical results, renames columns for clarity, and displays it as an interactive table, allowing easy side-by-side comparison of each strategy’s outcomes against actual results.\n\n# Step 1: Reshape the winners_by_strategy to have one row per year with each strategy's winner as separate columns\nstrategy_winners_table &lt;- winners_by_strategy |&gt;\n  pivot_wider(\n    names_from = strategy,\n    values_from = c(winning_party, winning_ecv),\n    names_glue = \"{strategy}_{.value}\"\n  )\n\n# Step 2: Join with the actual historical winner\ncomparison_table &lt;- historical_winner |&gt;\n  left_join(strategy_winners_table, by = \"year\") |&gt;\n  select(\n    year,\n    actual_winner,\n    `State-Wide Winner-Take-All_winning_party`,\n    `District-Wide Winner-Take-All + At-Large_winning_party`,\n    `State-Wide Proportional_winning_party`,\n    `National Proportional_winning_party`\n  ) |&gt;\n  rename(\n    `Actual Winner` = actual_winner,\n    `Winner-Take-All` = `State-Wide Winner-Take-All_winning_party`,\n    `District-Wide + At-Large Winner` = `District-Wide Winner-Take-All + At-Large_winning_party`,\n    `State-Wide Proportional` = `State-Wide Proportional_winning_party`,\n    `National Proportional` = `National Proportional_winning_party`,\n  )\n\n# Display the comparison table\ncomparison_table |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFact Check\n\n\n\nOverall, the National proportional strategy seems to be most fair. It would represent will of every American Citizen and remove unfair advantages smaller states have. Politicians would have to try to gather votes from every Citizen rather than just few swing states. Other approach I would have liked to analyze is Ranked choice voting but we do not have data to estimate how the result would have been like.\n\n\n\n\n\n\n\n\nExtra Credit: Advanced Chloropleth Visualization of Electoral College Results Modify your previous code to make either an animated version showing election results over time.\n\n\n\nI couldn’t generate gif from the data at once. So instead, I created multiple images for each year and combined them to form a gif. This approach allows visualizing how electoral results have changed over time, highlighting shifts in political dominance across states, trends in voting patterns, and potential changes in electoral maps due to population changes or other factors. This GIF offers an engaging, visual summary of historical election data.\n\nlibrary(magick)\n\n# Define election years\nyears &lt;- seq(1976, 1978, by = 4)\n\n# Now iterate over each year and retrieve the file location\nfor (year in years) {\n  shp_data_for_year &lt;- create_state_shp_data(year)\n  election_year_data &lt;- create_election_year_data(year)\n\n  yearly_data &lt;- shp_data_for_year |&gt;\n    inner_join(election_year_data, by = \"state\")\n\n  # print(yearly_data)\n  create_election_map(election_year_data, shp_data_for_year, year)\n}\n\n# Load all PNG images and create a GIF\nimages &lt;- list.files(path = \"mp03data/election_maps\", pattern = \"*.png\", full.names = TRUE)\n\ngif &lt;- image_read(images) # Read all images\n\n# Animate the images with desired frame rate (e.g., 1 frame per second)\nanimated_gif &lt;- image_animate(gif, fps = 1)\n\n# Save the animated GIF\n# image_write(animated_gif, \"election_map_animation.gif\")\nknitr::include_graphics(\"election_map_animation.gif\")\n\n\n\n\n\n\n\n\nThis animated election map reflects Advanced Chloropleth Visualization of Electoral College Results Modify your previous code to make either an animated version showing election results over time.\n\n\n\nReflecting on my experience with this project:\nAs someone new to the U.S. election system and experiencing an election here for the first time, working on this project was both eye-opening and challenging. I had to research a lot to understand the Electoral College and how it shapes election results. At times, it felt overwhelming, but watching the real election while working on this helped me connect more deeply with the data. This real-time context gave me a personal perspective on the process, making the project not just an academic task but a truly insightful and memorable experience."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "Introduction:\nRetirement planning is a critical aspect of financial security, and selecting the most suitable retirement plan can significantly impact the financial well-being of employees post-retirement. This project explores the decision-making process between two popular retirement plans offered at CUNY (City University of New York): the Teachers’ Retirement System (TRS) and the Optional Retirement Program (ORP). By leveraging historical economic data, advanced statistical techniques, and Monte Carlo simulations, this project aims to provide an analytical framework to assess the financial outcomes of these plans under varying economic conditions.\n\n\n\nRetirement Planning\n\n\n\n\nObjective of this project:\nCalculate the retirement contributions and simulate investment growth. This project compares retirement benefits and develop a framework by combining historical data and probabilistic modeling. Also, understand the retirement planning. Ultimately, the objective is to empower employees to make informed decisions by presenting a clear and data-driven comparison of the plans, considering both financial stability and market uncertainty.\n\n\nConclusion of the project:\nThe comparison between the Teachers’ Retirement System (TRS) and the Optional Retirement Program (ORP) shows clear differences. TRS offers steady, guaranteed income for life, adjusted for inflation, making it a great choice for people who want security and predictability in retirement. On the other hand, ORP has the potential for higher payouts because it invests in the market, but it also carries more risk, including a 75% chance of running out of money if withdrawals aren’t carefully managed. ORP provides a higher average monthly income ($344.25 compared to $271.13 for TRS) but depends heavily on market performance and good financial planning.\nIn simple terms, TRS is the safer option for those who want steady income with no surprises, while ORP works better for people willing to take risks for potentially greater rewards. To make ORP work long-term, retirees might need to lower withdrawal rates or adjust their investments. The choice between the two plans depends on whether someone values guaranteed income or is comfortable with market risks for a chance at higher returns. This analysis gives CUNY employees the tools to choose the plan that fits their needs and goals.\n\n\nProject analysis process:\nThe following section outlines the analysis and visualization steps taken to evaluate and compare the TRS and ORP retirement plans. This report walks you through the libraries used, methodologies, simulations, and visual insights used to determine the best plan based on individual financial goals and risk tolerance. Let’s dive into the details of the analysis\n\n\nLoad Libraries:\nFirstly, to streamline the analysis and ensure efficient data handling, I used a variety of R libraries throughout the project. Below is a breakdown of the libraries I loaded and their specific purposes:\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(httr2)\nlibrary(lubridate)\nlibrary(tidyverse) \nlibrary(patchwork) # For combining plots\nlibrary(\"knitr\")\n\n\n\n\nSimulating Future Value of Retirement Savings\nIn this section, this simulation demonstrates the future value calculation of a retirement savings plan over a period of 24 months. The model assumes a fixed monthly contribution of $100 and variable monthly returns, simulated as random normal values with a mean of 0.5% per month. The code applies a compound return formula to calculate the growth of contributions over time.\n\n\nCode\nRETIREMENT &lt;- data.frame(\n  r = rnorm(24, mean = 0.5) / 100, # Monthly returns\n  C = rep(100, 24), # Monthly savings: 100 per month\n  period = 1:24 # Period ID (# of months)\n)\n\nRETIREMENT |&gt;\n  mutate(net_total_return = order_by(\n    desc(period),\n    cumprod(1 + lead(r, default = 0))\n  )) |&gt;\n  summarize(future_value = sum(C * net_total_return)) |&gt;\n  DT::datatable()\n\n\n\n\n\n\nThe conclusion of the simulation reveals that the future value of the retirement savings over the 24-month period. This result takes into account the monthly contributions of $100 and the compounding effects of random monthly returns (simulated with a mean of 0.5%). This value represents the total amount accumulated in the retirement account after 24 months under the given assumptions.\n\n\n\n\n\n\nTask 1: Register and Load AlphaVantage API Key\n\n\n\nCreate your AlphaVantage free API key at https://www.alphavantage.co/support/#api-key.\nDo Not Include This Key in Your Submission.6 It is your personal account and linked to whatever email you use to create your account.\nI recommend creating a new file on your computer and storing your key there. You can read it into R using the readLines function and use it as needed. Just make sure not to print it.\nOnce you store your AlphaVantage key in a plain text file, make sure to add that file to your .gitignore to make sure you don’t accidentally include it in your git history.\nDocumentation for the AlphaVantage API can be found at (https://www.alphavantage.co/documentation/)\n\n\nThis section involves obtaining and securely loading the AlphaVantage API key, which is used for accessing financial and economic data. This key enables access to financial datasets necessary for the project.\n\n\nCode\nALPHA_ADVANTAGE_API_KEY &lt;- readLines(\"alphavantage_key.txt\")\nALPHA_URL &lt;- \"https://www.alphavantage.co/query\"\n\n\n\n\n\n\n\n\nTask 2: Register and Load FRED API Key\n\n\n\nCreate your FRED free API key at https://fredaccount.stlouisfed.org/login/secure/.\nDo Not Include This Key in Your Submission. It is your personal account and linked to whatever email you use to create your account.\nI recommend creating a new file on your computer and storing your key there. You can read it into R using the readLines function and use it as needed. Just make sure not to print it.\nOnce you store your FRED key in a plain text file, make sure to add that file to your .gitignore to make sure you don’t accidentally include it in your git history.\nDocumentation for the FRED API is available at https://fred.stlouisfed.org/docs/api/fred/.\n\n\nThis section involves obtaining and securely loading the FRED API key for accessing Federal Reserve Economic Data (FRED). The FRED API key allows us to query macroeconomic data, such as inflation rates and CPI, which are essential for the retirement plan analysis.\n\n\nCode\nFRED_API_KEY &lt;- readLines(\"fred_key.txt\")\nFRED_URL &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n\n\n\nVariance Estimation of the Sample Median\nIn this section, I used three approaches are used to estimate the variance of the sample median.This analysis estimates the variance of the sample median using three methods: bootstrap resampling, asymptotic theory, and a simulation-based approach.\n\n\nCode\nset.seed(100)\nDATA &lt;- rchisq(250, df = pi, ncp = exp(2))\nSAMPLE_MEDIAN &lt;- median(DATA)\n\nTRUE_MEDIAN &lt;- median(rchisq(5e7, df = pi, ncp = exp(2)))\n\n\nB &lt;- 500 # Number of boostrap samples to create\nn &lt;- length(DATA) # Original data size\n\nvariance &lt;-expand_grid(\n  B = 1:B,\n  n = 1:n\n) |&gt;\n  # Notice here we sample _with replacement_ from DATA\n  mutate(x = sample(DATA, n(), replace = TRUE)) |&gt;\n  group_by(B) |&gt;\n  summarize(f_boot = median(x)) |&gt;\n  summarize(var_f = var(f_boot)) |&gt;\n  pull(var_f)\n\ncat(\"The variance estimated is \", variance)\n\n\nThe variance estimated is  0.1911888\n\n\n\n\nCode\n# calculate the asymptotic variance of the sample median\nasymptotic_variance &lt;-1 / (4 * n * dchisq(TRUE_MEDIAN, df = pi, ncp = exp(2))^2)\ncat(\"The asymptotic variance estimated is \", asymptotic_variance)\n\n\nThe asymptotic variance estimated is  0.2121155\n\n\n\n\nCode\n# estimate the variance of the sample median\nvariance_sample_medianvar &lt;- (replicate(10000, {\n  median(rchisq(250, df = pi, ncp = exp(2)))\n\n}))\n\n\nThe bootstrap variance estimate is 0.1912, and the asymptotic variance is 0.2121, showing close agreement. Additionally, a simulation approach using 10,000 replications was implemented to estimate the sample median variance. While its results aren’t shown numerically, it complements the other methods by reinforcing the robustness of variance estimates. This demonstrates the reliability of combining empirical, theoretical, and simulation-based approaches for variance estimation\n\n\nBootstrapping Complex Data\nThis section of the analysis focuses on using bootstrapping techniques to explore the variability of the Kendall correlation coefficient between two complex variables.\n\n\nCode\nx &lt;- rchisq(100, df = 3, ncp = 2)\ny &lt;- x * sin(2 * x) + 15 * log(x)\nplot(x, y)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorelation_CY = cor(x, y, method = \"kendall\")\ncat(\"corelation of X and Y is\", corelation_CY )\n\n\ncorelation of X and Y is 0.7927273\n\n\n\n\nCode\nstopifnot(length(x) == length(y))\nn_samp &lt;- length(x)\nn_boot &lt;- 400\n\nkendal_corelation &lt;- data.frame(x = x, y = y) |&gt;\n  slice_sample(\n    n = n_samp * n_boot,\n    replace = TRUE\n  ) |&gt;\n  mutate(resample_id = rep(1:n_boot, times = n_samp)) |&gt;\n  group_by(resample_id) |&gt;\n  summarize(kendall_cor = cor(x, y, method = \"kendall\")) |&gt;\n  summarize(var(kendall_cor)) |&gt;\n  pull()\n\ncat(\"kendall Corelation is\", kendal_corelation)\n\n\nkendall Corelation is 0.001807341\n\n\n\n\nCode\nvar_result &lt;- var(replicate(5000, {\n  x &lt;- rchisq(100, df = 3, ncp = 2)\n  y &lt;- x * sin(2 * x) + 15 * log(x)\n  cor(x, y, method = \"kendall\")\n}))\n\ncat(\"Variance with 5000 replicates is\", var_result)\n\n\nVariance with 5000 replicates is 0.001408479\n\n\nThe analysis confirms a strong positive relationship between the two variables. Variance estimates from bootstrapping and simulations provide insights into the stability and reliability of the correlation measure. This approach effectively combines statistical methods to analyze complex data relationships and assess the robustness of the findings.\n\n\n\n\n\n\nTask 3: Data Acquisition\n\n\n\nIdentify and download historical data series for each of the above inputs to your Monte Carlo analysis. If necessary, “downsample” each series to a monthly frequency and join them together in a data.frame.You must use at least one data series from AlphaVantage and one from FRED. You must use the APIs of each service to access this data and, as noted above, you need to use the “raw” API, relying only on the httr2 package (or similar) and not wrapper packages like quantmod or alphavantager.”}\n\n\n\n\nWage growth\nThis analysis utilizes data from the Federal Reserve Economic Data (FRED) API to analyze wage growth trends over time. The dataset focuses on average hourly earnings of employees in the private sector, with the raw data processed to calculate monthly wage growth rates.\n\n\nCode\nresponse &lt;- request(FRED_URL) |&gt;\n  req_url_query(\n    series_id = \"CES0500000003\", # Average Hourly Earnings of All Employees: Total Private\n    api_key = FRED_API_KEY,\n    file_type = \"json\"\n  ) |&gt;\n  req_perform()\n  \n# Parse JSON response\nwage_data &lt;- response |&gt;\n  resp_body_json() %&gt;%\n  .$observations\n\nwage_growth_df &lt;- wage_data |&gt;\n  map_dfr(as_tibble) |&gt;\n  mutate(\n    date = as.Date(date),\n    wage = as.numeric(value)\n  ) |&gt;\n  arrange(date) |&gt;\n  mutate(\n    # Calculate percentage change in wages\n    wage_growth = (wage / lag(wage) - 1)\n  ) |&gt;\n  select(date, wage_growth)\n\n# Convert to monthly data\nwage_growth_monthly &lt;- wage_growth_df |&gt;\n  mutate(\n    year = year(date),\n    month = month(date)\n  ) |&gt;\n  group_by(year, month) |&gt;\n  summarise(\n    date = floor_date(first(date), unit = \"month\"),\n    wage_growth =  median(wage_growth, na.rm = TRUE),  # Median for robust aggregation\n    .groups = \"drop\"\n  ) |&gt;\n  ungroup() |&gt;\n  arrange(date)\n\n\nwage_growth_monthly &lt;- wage_growth_monthly |&gt;\n  mutate(\n    wage_growth = pmin(pmax(wage_growth, -0.1), 0.1)  # Cap between -10% and 10%\n  )\n\n\n\nwage_growth_df  &lt;- wage_growth_monthly |&gt; select(date, wage_growth)\n\nwage_growth_df |&gt; DT::datatable()\n\n\n\n\n\n\nThis clean and structured table allows for a comprehensive understanding of wage growth trends, supporting further economic analysis or decision-making.\nLet’s visualize to make sure the wage growth seems appropriate.\n\n\nCode\n# Step 1: Set initial salary\nstarting_salary &lt;- 50000  # Example starting salary in dollars\n\n# Step 2: Remove missing values from wage growth data\nwage_growth_clean &lt;- wage_growth_df |&gt;\n  filter(!is.na(wage_growth))  # Remove rows with NA values\n\n# Step 3: Initialize salary vector\nn_months &lt;- nrow(wage_growth_clean)\nsalary &lt;- numeric(n_months)\nsalary[1] &lt;- starting_salary  # Set initial salary\n\n# Step 4: Calculate salary growth iteratively\nfor (i in 2:n_months) {\n  salary[i] &lt;- salary[i - 1] * (1 + wage_growth_clean$wage_growth[i])\n}\n\n# Step 5: Combine results into a data frame\nsalary_data &lt;- data.frame(\n  Date = wage_growth_clean$date,\n  Salary = salary\n)\n\n# Step 6: Plot the salary growth\nlibrary(ggplot2)\nggplot(salary_data, aes(x = Date, y = Salary)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"Simulated Salary Growth Over Time\",\n    x = \"Date\",\n    y = \"Salary ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe graph shows how a starting salary of $50,000 grows over time based on historical wage. The salary increases while fluctuations around 2020 likely indicate economic disruptions like the COVID-19 pandemic. The compounding effect is evident, with the salary surpassing $80,000 by 2025, highlighting the impact of consistent wage growth on long-term earnings.\n\n\nInflation\nInflation data is a key economic indicator that reflects the rate at which the general level of prices for goods and services is rising. This analysis uses the Consumer Price Index (CPI) as a measure of inflation. The data is retrieved from the Federal Reserve Economic Data (FRED) API.\n\n\nCode\nresponse &lt;- request(FRED_URL) |&gt;\n  req_url_query(\n    series_id = \"CPIAUCSL\", # Consumer Price Index for All Urban Consumers: All Items\n    api_key = FRED_API_KEY,\n    file_type = \"json\"\n  ) |&gt;\n  req_perform()\n\n# Parse JSON response\ninflation_data &lt;- response |&gt;\n  resp_body_json() %&gt;%\n  .$observations\n\n# Convert to a data frame\ninflation_df &lt;- inflation_data |&gt;\n  map_dfr(as_tibble) |&gt;\n  mutate(\n    date = as.Date(date),\n    value = as.numeric(value) # Convert CPI data to numeric\n  ) |&gt;\n  select(date, cpi = value)\n\ninflation_df |&gt; DT::datatable()\n\n\n\n\n\n\nCalculate Inflation Rate Inflation is typically calculated as the percentage change in CPI over a time period. Here’s how to calculate the month-over-month (MoM) and year-over-year (YoY) inflation rates:\n\n\nCode\ninflation_df &lt;- inflation_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    mom_inflation = (cpi / lag(cpi) - 1) * 100, # Month-over-Month Inflation\n    yoy_inflation = (cpi / lag(cpi, 12) - 1) * 100 # Year-over-Year Inflation\n  )\n\ninflation_df &lt;- inflation_df |&gt;\n  rename(\n    inflation_cpi = cpi,                # CPI column\n    mom_inflation_rate = mom_inflation, # Month-over-month inflation\n    yoy_inflation_rate = yoy_inflation  # Year-over-year inflation\n  )\n\n# Let's visualize\nggplot(inflation_df, aes(x = date, y = mom_inflation_rate)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Adjusted Month-over-Month Inflation Rate\", x = \"Date\", y = \"MoM Inflation Rate (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of the adjusted Month-over-Month (MoM) inflation rate reveals the historical fluctuations in inflation from the mid-20th century to the present. Significant spikes and dips highlight periods of economic volatility, such as recessions, energy crises, and financial instabilities. Over time, the variability in MoM inflation appears to stabilize during certain periods, though recent data suggests renewed fluctuations.\n\n\nCode\nggplot(inflation_df, aes(x = date, y = yoy_inflation_rate)) +\n  geom_line(color = \"red\") +\n  labs(title = \"Adjusted Year-over-Year Inflation Rate\", x = \"Date\", y = \"YoY Inflation Rate (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe Year-over-Year (YoY) inflation rate visualization highlights key inflationary periods, such as the 1970s and post-2020 spikes, alongside stable periods like the late 1990s. This demonstrates the cyclical nature of inflation influenced by economic factors.\n\n\nUS Equity Market total returns\nLet’s retrieve monthly adjusted time series data for a specific US equity (e.g., the S&P 500 ETF “SPY”) using the Alpha Vantage API. The API request includes parameters for the time series function and the equity symbol, allowing access to detailed historical market data for further financial analysis or visualization.\n\n\nCode\n# Define the API endpoint and parameters\nus_equity_response &lt;- request(ALPHA_URL) |&gt;\n  req_options(cookies = NULL) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\", # Enclose 'function' in backticks\n    symbol = \"SPY\", # Example: S&P 500 ETF\n    apikey = ALPHA_ADVANTAGE_API_KEY\n  ) |&gt;\n  req_perform()\n\n\n\n\nCode\n# Parse JSON response\nequity_data &lt;- us_equity_response |&gt;\n  resp_body_json() %&gt;%\n  .$`Monthly Adjusted Time Series`\n\n# Convert JSON data to a data frame\nequity_df &lt;- equity_data |&gt;\n  map_dfr(as_tibble, .id = \"date\") |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    us_equity_return = as.numeric(`5. adjusted close`) # Adjusted close price\n  ) |&gt;\n  select(date, us_equity_return)\n\nequity_df &lt;- equity_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    us_equity_monthly_return = (us_equity_return - lag(us_equity_return)) / lag(us_equity_return) * 100\n  )\n\nequity_df &lt;- equity_df |&gt;\n  rename(\n    us_equity_adjusted = us_equity_return,  # Adjusted close column\n  )\n\n\nequity_df &lt;- equity_df |&gt;\n  mutate(date = date |&gt;\n           floor_date(unit = \"month\") |&gt;  # Get the first of the current month\n           add_with_rollback(months(1)))  # Add one month\n\n\n# Remove Empty Values\nequity_df &lt;- equity_df |&gt;\n  filter(!is.na(us_equity_monthly_return))\n\n# View the updated data frame\nequity_df |&gt; DT:::datatable()\n\n\n\n\n\n\nThe analysis focuses on preparing monthly adjusted equity data for trend and performance evaluation. It includes converting raw data into a structured format, calculating monthly percentage returns, and ensuring clean, consistent data by addressing date formatting and removing missing values.\n\nPlot Adjusted Close Prices\nNow, we will plot clear and visually appealing line chart showing how adjusted close prices for the US equity market have changed over time.\n\n\nCode\nggplot(equity_df, aes(x = date, y = us_equity_adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"US Equity Market Adjusted Close Prices\", x = \"Date\", y = \"Adjusted Close ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of adjusted close prices for the US equity market demonstrates a clear upward trend over the past two decades. Periods of volatility, such as the 2008 financial crisis and the 2020 pandemic, are evident, followed by robust recoveries. This long-term growth reflects the resilience of the equity market and its ability to adapt to economic challenges.\n\n\nPlot Monthly Returns\nlet’s create a line chart showing US Equity Market Monthly Returns (%) over time. The x-axis represents the date, and the y-axis shows the percentage returns.\n\n\nCode\nggplot(equity_df, aes(x = date, y = us_equity_monthly_return)) +\n  geom_line(color = \"red\") +\n  labs(title = \"US Equity Market Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of US equity market monthly returns highlights significant volatility over the past two decades. Periods of extreme fluctuations, such as during the 2008 financial crisis and the 2020 pandemic, are clearly visible. Despite regular variability, the overall market demonstrates resilience with a balance of positive and negative returns over time, underscoring the dynamic nature of equity investments.\n\n\n\nInternational Equity Market total returns\nThis provides a clear picture of historical international equity market trends, essential for comparing performance with other asset classes in financial analyses.\n\n\nCode\n# Request monthly adjusted data for VEU ETF\nintl_equity_response &lt;- request(ALPHA_URL) |&gt;\n  req_options(cookies = NULL) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\", # Monthly adjusted time series\n    symbol = \"VEU\",                           # Vanguard FTSE All-World ex-US ETF\n    apikey = ALPHA_ADVANTAGE_API_KEY\n  ) |&gt;\n  req_perform()\n\n\n\n\nCode\nintl_equity_data &lt;-intl_equity_response |&gt;\n  resp_body_json() %&gt;%\n  .$`Monthly Adjusted Time Series`\n\n\n# Convert JSON data to a data frame\nintl_equity_df &lt;- intl_equity_data |&gt;\n  map_dfr(as_tibble, .id = \"date\") |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    intl_equity_return = as.numeric(`5. adjusted close`) # Adjusted close price\n  ) |&gt;\n  select(date, intl_equity_return)\n\nintl_equity_df &lt;- intl_equity_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    monthly_return = (intl_equity_return - lag(intl_equity_return)) / lag(intl_equity_return) * 100\n  )\n\nintl_equity_df &lt;- intl_equity_df |&gt;\n  rename(\n    intl_equity_adjusted = intl_equity_return,  # Adjusted close column\n    intl_equity_monthly_return = monthly_return # Monthly returns\n  )\n\nintl_equity_df &lt;- intl_equity_df |&gt;\n  mutate(date = date |&gt;\n           floor_date(unit = \"month\") |&gt;  # Get the first of the current month\n           add_with_rollback(months(1)))  # Add one month\n\n# View the updated data frame\nintl_equity_df |&gt; DT::datatable()\n\n\n\n\n\n\nThe data includes adjusted close prices and calculates monthly percentage returns, providing insights into the performance of international equity markets. The structured dataset is prepared for further exploration, allowing for trend analysis and comparison with other markets.\n\n\nVisualize the International Equity Market Data\nPlot Adjusted Close Prices: This helps verify long-term trends and detect anomalies: This graph displays the International Equity Market Adjusted Close Prices over time.\n\n\nCode\nggplot(intl_equity_df, aes(x = date, y = intl_equity_adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"International Equity Market Adjusted Close Prices\", x = \"Date\", y = \"Adjusted Close ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe graph of International Equity Market Adjusted Close Prices highlights the long-term growth and resilience of international markets, despite periods of volatility. . This upward trend highlights the resilience and recovery of international equity markets, reflecting long-term opportunities for investors outside the US.\n\n\nPlot Monthly Returns:\nThis analysis focuses on the monthly returns of the international equity market:\n\n\nCode\nggplot(intl_equity_df, aes(x = date, y = intl_equity_monthly_return)) +\n  geom_line(color = \"red\") +\n  labs(title = \"International Equity Market Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of international equity market monthly returns highlights significant volatility over time, with sharp declines during major global economic crises such as 2008 and 2020. Despite regular fluctuations, the market shows resilience, with periods of recovery and stability, reflecting the dynamic nature of international investments.\n\n\nBond Market Total Returns\nThis analysis retrieves monthly adjusted time series data for the iShares Core US Aggregate Bond ETF (AGG), a key benchmark for the US bond market. By examining this data, we aim to understand trends and performance in the fixed-income market, providing valuable insights into bond market dynamics over time.\n\n\nCode\n# Request monthly adjusted data for AGG ETF\nbond_response &lt;- request(ALPHA_URL) |&gt;\n  req_options(cookies = NULL) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\", # Monthly adjusted time series\n    symbol = \"AGG\", # iShares Core US Aggregate Bond ETF\n    apikey = ALPHA_ADVANTAGE_API_KEY\n  ) |&gt;\n  req_perform()\n\n\nNow, let’s parse the data\n\n\nCode\n# Parse JSON response\nbond_data &lt;- bond_response |&gt;\n  resp_body_json() %&gt;%\n  .$`Monthly Adjusted Time Series`\n\n# Convert JSON data to a data frame\nbond_df &lt;- bond_data |&gt;\n  map_dfr(as_tibble, .id = \"date\") |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    bond_return = as.numeric(`5. adjusted close`) # Adjusted close price\n  ) |&gt;\n  select(date, bond_return)\n\nbond_df &lt;- bond_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    monthly_return = (bond_return - lag(bond_return)) / lag(bond_return) * 100\n  )\n\nbond_df &lt;- bond_df |&gt;\n  rename(\n    bond_adjusted = bond_return,  # Adjusted close column\n    bond_monthly_return = monthly_return # Monthly returns\n  )\n\n\nbond_df &lt;- bond_df |&gt;\n  mutate(date = date |&gt;\n           floor_date(unit = \"month\") |&gt;  # Get the first of the current month\n           add_with_rollback(months(1)))  # Add one month\n\n# View the updated data frame\nbond_df |&gt; DT::datatable()\n\n\n\n\n\n\nThis structured data enables analysis of bond market performance over time, highlighting trends and volatility in the fixed-income market. It serves as a valuable resource for assessing the stability and dynamics of bonds in a broader portfolio context.\n\nAnalyze the data\nThe analysis visualizes the performance of the bond market using the adjusted close prices.\n\n\nCode\nbond_df &lt;- bond_df |&gt;\n  filter(!is.na(bond_monthly_return))\n\nggplot(bond_df, aes(x = date, y = bond_adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Bond Market Adjusted Close Prices\", x = \"Date\", y = \"Adjusted Close ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization shows steady growth in bond prices over the years, reflecting the bond market’s stability. Periods of decline, such as during economic downturns, are evident but followed by recoveries, emphasizing the resilience of the bond market and its importance as a relatively stable investment option.\n\n\nCode\nggplot(bond_df, aes(x = date, y = bond_monthly_return)) +\n  geom_line(color = \"green\") +\n  labs(title = \"Bond Market Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe bond market’s monthly returns exhibit relative stability compared to equities, with occasional spikes during periods of economic volatility, such as the 2008 financial crisis and the 2020 pandemic. The data underscores bonds’ role as a lower-risk investment, offering consistent returns with minimal drastic fluctuations over time.\n\n\n\nShort Term Debt Returns\nTo fetch Short-Term Debt Returns, we will use the 2-Year US Treasury Yield from the FRED API, which is a widely recognized benchmark for short-term debt.\n\n\nCode\n# Fetch 2-Year Treasury Yield data\nresponse &lt;- request(FRED_URL) |&gt;\n  req_url_query(\n    series_id = \"DGS2\", # 2-Year Treasury Yield\n    api_key = FRED_API_KEY, # Your FRED API key\n    file_type = \"json\", # File format\n    observation_end = Sys.Date() # Today's date\n  ) |&gt;\n  req_perform()\n\n# Parse JSON response\ndebt_data &lt;- response %&gt;%\n  resp_body_json() %&gt;%\n  .$observations\n\n# Convert JSON data to a data frame\ndebt_df &lt;- debt_data |&gt;\n  map_dfr(as_tibble) |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    short_term_yield = as.numeric(value), # Convert yield data to numeric\n    .groups = \"drop\"\n  ) |&gt;\n  select(date, short_term_yield)\n\ndebt_df &lt;- debt_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    monthly_return = (((1 + short_term_yield/100)^(1/12)) - 1) * 100\n  )\n\n\ndebt_df &lt;- debt_df |&gt;\n  rename(\n    short_term_yield_percent = short_term_yield, # Original yield in %\n    short_term_monthly_return = monthly_return  # Monthly return approximation\n  )\n\n\ndebt_df_monthly &lt;- debt_df |&gt;\n  group_by(\n    year = year(date),\n    month = month(date)\n  ) |&gt;\n  summarise(\n    date = floor_date(first(date), unit = \"month\"),\n    short_term_yield_percent = mean(short_term_yield_percent, na.rm = TRUE),\n    short_term_monthly_return = mean(short_term_monthly_return, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  ungroup() |&gt;\n  arrange(date)\n\ndebt_df &lt;- debt_df_monthly |&gt;\n  select(date, short_term_yield_percent, short_term_monthly_return)\n\n\n\nLet’s analyze the results\nThis analysis focuses on short-term debt returns, an important component of financial planning and investment strategies.\n\n\nCode\n# Plot Short-Term Yields Over Time:\nggplot(debt_df, aes(x = date, y = short_term_yield_percent)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Short-Term Treasury Yield (2-Year)\", x = \"Date\", y = \"Yield (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of the 2-Year Treasury Yield highlights significant fluctuations over time, reflecting changes in economic conditions, monetary policy, and market expectations. Peaks in the yield correspond to periods of high inflation and tight monetary policy, such as in the 1980s.\n\n\n\nPlot Monthly Returns Over Time\n\n\nCode\nggplot(debt_df, aes(x = date, y = short_term_monthly_return)) +\n  geom_line(color = \"green\") +\n  labs(title = \"Short-Term Debt Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of short-term debt monthly returns illustrates the impact of economic cycles on returns. Peaks, such as those in the late 1970s and early 1980s, align with periods of high inflation and elevated interest rates. The steady decline and stabilization in subsequent decades reflect accommodative monetary policies and economic recovery phases. Recent upticks highlight the response to changing interest rate environments, emphasizing the sensitivity of short-term debt to macroeconomic conditions.\n\n\n\n\n\n\nTask 4: Initial Analysis\n\n\n\nIdentify and download historical data series for each of the above inputs to your Monte Carlo analysis. If necessary, “downsample” each series to a monthly frequency and join them together in a data.frame.\nYou must use at least one data series from AlphaVantage and one from FRED. You must use the APIs of each service to access this data and, as noted above, you need to use the “raw” API, relying only on the httr2 package (or similar) and not wrapper packages like quantmod or alphavantager.\n\n\nOnce we have acquired and cleaned the input data, the next step is to conduct a basic exploratory data analysis (EDA) to identify key characteristics of the dataset. In this stage, we will examine factors such as correlations, long-term averages, and variances. This will provide us with an understanding of the relationships and trends within the data.\nAs part of our analysis, we will compute the long-run monthly average value for each series, which will be used in subsequent tasks.\nTo begin, we will combine the cleaned and processed data from all sources into a single data frame. This consolidated dataset will allow us to perform comprehensive EDA, providing insights into the overall structure and properties of the data.\n\n\nStep 1: Combine Data\nIn this step, we integrated multiple datasets related to economic and financial indicators into a single, unified dataset:\n\n\nCode\n# Ensure all datasets have a common date format and merge them\ncombined_data &lt;- reduce(\n  list(\n    inflation_df |&gt; select(date, inflation_cpi, mom_inflation_rate, yoy_inflation_rate),\n    wage_growth_df |&gt; select(date, wage_growth),\n    equity_df |&gt; select(date, us_equity_monthly_return),\n    intl_equity_df |&gt; select(date, intl_equity_monthly_return),\n    bond_df |&gt; select(date, bond_monthly_return),\n    debt_df |&gt; select(date, short_term_monthly_return)\n  ),\n  full_join,\n  by = \"date\"\n) \n\ncombined_data &lt;- combined_data %&gt;%\n  filter(complete.cases(.))\n\ncombined_data |&gt; DT::datatable()\n\n\n\n\n\n\nThe resulting dataset is a comprehensive, time-aligned dataset containing multiple economic and financial indicators, ready for analysis and comparison.\n\n\nStep 2: Compute Summary Statistics\nIn this step, we calculated summary statistics for the key economic and financial indicators in the combined dataset:\n\n\nCode\nsummary_stats &lt;- combined_data |&gt;\n  summarize(\n    avg_wage_growth = mean(wage_growth, na.rm = TRUE),\n    avg_mom_inflation = mean(mom_inflation_rate, na.rm = TRUE),\n    avg_yoy_inflation = mean(yoy_inflation_rate, na.rm = TRUE),\n    avg_us_equity_return = mean(us_equity_monthly_return, na.rm = TRUE),\n    avg_intl_equity_return = mean(intl_equity_monthly_return, na.rm = TRUE),\n    avg_bond_return = mean(bond_monthly_return, na.rm = TRUE),\n    avg_short_term_return = mean(short_term_monthly_return, na.rm = TRUE)\n  )\n\n# View summary statistics\nsummary_stats |&gt; DT::datatable()\n\n\n\n\n\n\nThese results allow for a comprehensive comparison across indicators, helping to identify relationships between economic trends, market behaviors, and investment performance.\n\n\nStep 3: Analyze Correlations Among Factors\nlet’s analyze the relationships between various financial metrics by computing a correlation matrix.\n\n\nCode\ncorrelation_matrix &lt;- combined_data |&gt;\n  select(-date) |&gt;\n  drop_na() |&gt;  # Remove rows with NA values\n  cor()\n\n# Print the correlation matrix\ncorrelation_matrix |&gt; DT::datatable()\n\n\n\n\n\n\nThis matrix provides a detailed understanding of how indicators relate to one another, revealing patterns and dependencies for deeper analysis.\n\n\nStep 4: Create Figures Time-Series Visualization of Each Variable:\nNow let’s create a cohesive visualization of multiple time series related to economic and market indicators. Rhis analysis provides a clear view of how different economic and market factors have evolved over time, enabling comparative insights and trend analysis.\n\n\nCode\n# Plot individual time series\nplot1 &lt;- ggplot(combined_data, aes(x = date, y = wage_growth)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Wage Growth Over Time\", x = \"Date\", y = \"Wage Growth (%)\")\n\nplot2 &lt;- ggplot(combined_data, aes(x = date, y = mom_inflation_rate)) +\n  geom_line(color = \"red\") +\n  labs(title = \"MoM Inflation Rate Over Time\", x = \"Date\", y = \"Inflation (%)\")\n\nplot3 &lt;- ggplot(combined_data, aes(x = date, y = us_equity_monthly_return)) +\n  geom_line(color = \"green\") +\n  labs(title = \"US Equity Market Returns Over Time\", x = \"Date\", y = \"Monthly Return (%)\")\n\nplot4 &lt;- ggplot(combined_data, aes(x = date, y = intl_equity_monthly_return)) +\n  geom_line(color = \"orange\") +\n  labs(title = \"International Equity Market Returns Over Time\", x = \"Date\", y = \"Monthly Return (%)\")\n\n# Combine the plots\nplot1 + plot2 + plot3 + plot4 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\nThe visualizations collectively highlight key economic and financial trends over time. Wage growth shows a generally steady pattern with notable disruptions, such as during the pandemic. Inflation rates exhibit fluctuations, with spikes during periods of economic instability. US and international equity markets reflect consistent volatility, underscoring the dynamic nature of investments, while revealing similar patterns of recovery and growth post-crisis. Together, these trends provide a comprehensive view of the interconnectedness of labor, inflation, and financial markets, emphasizing their collective impact on economic stability and performance.\n\n\nStep 5: Compute Long-Run Monthly Averages\nThis analysis calculates the long-run monthly averages, medians, and standard deviations for key economic and financial series.\n\n\nCode\n# Calculate long-run monthly averages for each series\nlong_run_averages &lt;- tibble(\n  Series = c(\"Wage Growth\", \"MoM Inflation Rate\", \"US Equity Market Returns\", \n             \"International Equity Market Returns\", \"Bond Market Returns\", \n             \"Short-Term Debt Returns\"),\n  Mean = c(\n    mean(wage_growth_df$wage_growth, na.rm = TRUE),\n    mean(inflation_df$mom_inflation_rate, na.rm = TRUE),\n    mean(equity_df$us_equity_monthly_return, na.rm = TRUE),\n    mean(intl_equity_df$intl_equity_monthly_return, na.rm = TRUE),\n    mean(bond_df$bond_monthly_return, na.rm = TRUE),\n    mean(debt_df$short_term_monthly_return, na.rm = TRUE)\n  ),\n  Median = c(\n    median(wage_growth_df$wage_growth, na.rm = TRUE),\n    median(inflation_df$mom_inflation_rate, na.rm = TRUE),\n    median(equity_df$us_equity_monthly_return, na.rm = TRUE),\n    median(intl_equity_df$intl_equity_monthly_return, na.rm = TRUE),\n    median(bond_df$bond_monthly_return, na.rm = TRUE),\n    median(debt_df$short_term_monthly_return, na.rm = TRUE)\n  ),\n  SD = c(\n    sd(wage_growth_df$wage_growth, na.rm = TRUE),\n    sd(inflation_df$mom_inflation_rate, na.rm = TRUE),\n    sd(equity_df$us_equity_monthly_return, na.rm = TRUE),\n    sd(intl_equity_df$intl_equity_monthly_return, na.rm = TRUE),\n    sd(bond_df$bond_monthly_return, na.rm = TRUE),\n    sd(debt_df$short_term_monthly_return, na.rm = TRUE)\n  )\n)\n\n# View the long-run monthly averages\nlong_run_averages |&gt; DT::datatable()\n\n\n\n\n\n\nThe long-run averages, medians, and standard deviations provide a clear picture of the behavior and variability of key economic and financial indicators.\n\n\n\n\n\n\nTask 5: Historical Comparison\n\n\n\nNow that you have acquired data, implement the TRS and ORP formulas above and compare the value of each of them for the first month of retirement. To do this, you may assume that your hypothetical employee:\nJoined CUNY in the first month of the historical data Retired from CUNY at the end of the final month of data You will need to select a starting salary for your employee. Use historical data for wage growth and inflation and assume that the TRS and ORP parameters did not change over time. (That is, the employee contribution “brackets” are not inflation adjusted; the employee will have to make larger contributions as income rises over the span of a 20+ year career.)\n\n\n\n\nStep 1: Simulate Salary Growth\nThis simulation models the growth of a hypothetical salary over time based on observed wage growth rates from the dataset. Starting with an initial salary, the model iteratively applies the monthly wage growth rates to project the salary progression\n\n\nCode\n# Step 1: Set initial salary\nstarting_salary &lt;- 50000  # Hypothetical starting salary in dollars\n\n# Step 2: Initialize salary vector\nn_months &lt;- nrow(combined_data)\nsalary &lt;- numeric(n_months)\nsalary[1] &lt;- starting_salary  # Set initial salary\n\n# Step 3: Calculate salary growth iteratively\nfor (i in 2:n_months) {\n  salary[i] &lt;- salary[i - 1] * (1 + combined_data$wage_growth[i])\n}\n\n# Step 4: Combine results into a data frame\nsalary_simulation &lt;- data.frame(\n  Date = combined_data$date,\n  Salary = salary\n)\n\n# Step 5: Visualize the salary growth\nlibrary(ggplot2)\nggplot(salary_simulation, aes(x = Date, y = Salary)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"Simulated Salary Growth Over Time\",\n    x = \"Date\",\n    y = \"Salary ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe simulated salary growth over time demonstrates a steady upward trajectory, reflecting consistent annual increases in earnings. The notable inflection point aligns with a period of economic disruption, such as the COVID-19 pandemic, showing a temporary impact on salary trends. This chart underscores the resilience of salary growth in the long term despite short-term challenges, providing insights into financial stability and progression over time.\n\n\nStep 2: Calculate TRS Contributions\nFor TRS contributions, we simulated the growth of the retirement fund by assuming the employee contributes 5% of their monthly salary throughout their career. The TRS fund grows steadily with a guaranteed annual return rate of 4%, compounded monthly. By the time of retirement, the total fund value is calculated based on these contributions and the compounded growth, providing a clear picture of the cumulative savings achieved under the TRS plan.\n\n\nCode\n# Step 1: Define TRS parameters\ntrs_contribution_rate &lt;- 0.05  # 5% of salary\ntrs_annual_return &lt;- 0.04      # 4% annual return\ntrs_monthly_return &lt;- trs_annual_return / 12  # Monthly return\n\n# Step 2: Calculate monthly contributions\nsalary_simulation &lt;- salary_simulation |&gt;\n  mutate(\n    Monthly_Contribution = (Salary / 12) * trs_contribution_rate  # Divide annual salary by 12\n  )\n\n# Step 3: Simulate TRS fund growth\nn_months &lt;- nrow(salary_simulation)\ntrs_fund &lt;- numeric(n_months)\ntrs_fund[1] &lt;- salary_simulation$Monthly_Contribution[1]  # Initial contribution\n\nfor (i in 2:n_months) {\n  trs_fund[i] &lt;- trs_fund[i - 1] * (1 + trs_monthly_return) + \n                 salary_simulation$Monthly_Contribution[i]\n}\n\n# Step 4: Add TRS fund values to the dataset\nsalary_simulation &lt;- salary_simulation |&gt;\n  mutate(TRS_Fund_Value = trs_fund)\n\n# Step 5: Visualize TRS fund growth over time\nggplot(salary_simulation, aes(x = Date, y = TRS_Fund_Value)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"TRS Fund Value Over Time\",\n    x = \"Date\",\n    y = \"Cumulative TRS Fund Value ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe chart highlights steady TRS fund growth over time, driven by consistent contributions and compounded returns, illustrating the power of long-term saving.\n\n\nStep 3: Calculate ORP Contributions\nThis analysis simulates the growth of an Optional Retirement Plan (ORP) fund over time, integrating weighted returns from equities, bonds, and short-term debt based on a defined portfolio allocation. It combines salary data and monthly contributions with market performance to project the ORP fund value, offering insights into its cumulative growth.\n\n\nCode\n# Step 1: Define ORP parameters\norp_contribution_rate &lt;- 0.05  # 5% of salary\norp_weights &lt;- c(0.6, 0.3, 0.1)  # Portfolio weights: 60% equity, 30% bonds, 10% short-term debt\n\n# Step 2: Merge returns data with salary data\nreturns_combined &lt;- combined_data |&gt;\n  select(date, us_equity_monthly_return, bond_monthly_return, short_term_monthly_return) |&gt;\n  right_join(salary_simulation, by = c(\"date\" = \"Date\"))\n\n# Step 3: Calculate weighted monthly returns\nreturns_combined &lt;- returns_combined |&gt;\n  mutate(\n    Weighted_Return = orp_weights[1] * us_equity_monthly_return +\n                      orp_weights[2] * bond_monthly_return +\n                      orp_weights[3] * short_term_monthly_return,\n    Monthly_Contribution = (Salary / 12) * orp_contribution_rate\n  )\n\n# Step 4: Simulate ORP fund growth\nn_months &lt;- nrow(returns_combined)\norp_fund &lt;- numeric(n_months)\norp_fund[1] &lt;- returns_combined$Monthly_Contribution[1]  # Initial contribution\n\nfor (i in 2:n_months) {\n  orp_fund[i] &lt;- orp_fund[i - 1] * (1 + returns_combined$Weighted_Return[i] / 100) +\n                 returns_combined$Monthly_Contribution[i]\n}\n\n# Step 5: Add ORP fund values to the dataset\nreturns_combined &lt;- returns_combined |&gt;\n  mutate(ORP_Fund_Value = orp_fund)\n\n# Step 6: Visualize ORP fund growth\nggplot(returns_combined, aes(x = date, y = ORP_Fund_Value)) +\n  geom_line(color = \"red\") +\n  labs(\n    title = \"ORP Fund Value Over Time\",\n    x = \"Date\",\n    y = \"Cumulative ORP Fund Value ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ORP simulation shows consistent growth, emphasizing the benefits of regular contributions and a diversified portfolio for long-term financial stability.\n\n\nStep 4: Inflation Adjustment\nThis section introduces inflation-adjusted analysis to provide a realistic comparison of the TRS and ORP fund values over time.\n\n\nCode\n# Step 1: Normalize CPI data to create a cumulative inflation factor\ninflation_adjustment &lt;- combined_data |&gt;\n  arrange(date) |&gt;\n  mutate(\n    cumulative_inflation_factor = inflation_cpi / inflation_cpi[1]  # Normalize to first month's CPI\n  ) |&gt;\n  select(date, cumulative_inflation_factor)\n\n# Step 2: Merge inflation adjustment data with TRS and ORP datasets\nadjusted_data &lt;- returns_combined |&gt;\n  left_join(inflation_adjustment, by = \"date\") |&gt;\n  mutate(\n    TRS_Inflation_Adjusted = TRS_Fund_Value / cumulative_inflation_factor,\n    ORP_Inflation_Adjusted = ORP_Fund_Value / cumulative_inflation_factor\n  )\n\n# Step 3: Visualize the inflation-adjusted TRS and ORP fund values\nggplot(adjusted_data) +\n  geom_line(aes(x = date, y = TRS_Inflation_Adjusted, color = \"TRS Inflation-Adjusted\")) +\n  geom_line(aes(x = date, y = ORP_Inflation_Adjusted, color = \"ORP Inflation-Adjusted\")) +\n  labs(\n    title = \"Inflation-Adjusted TRS and ORP Fund Values Over Time\",\n    x = \"Date\",\n    y = \"Inflation-Adjusted Fund Value ($)\",\n    color = \"Plan\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ORP plan outperforms TRS in inflation-adjusted terms due to its diversified investments, offering higher growth potential but with increased risk.\n\n\nNext Step: Calculate Monthly Retirement Payouts\nTo compare first-month retirement payouts, we calculated inflation-adjusted monthly payments for both TRS and ORP. TRS offers a predictable payout at a fixed annual rate (5% of the final fund value, divided monthly) with inflation adjustments. ORP, on the other hand, allows withdrawals at a fixed annual rate (e.g., 3.5%) while adjusting for inflation to preserve purchasing power. A comparison table was created to summarize the inflation-adjusted final fund values and the corresponding monthly payouts for both plans, highlighting their financial outcomes at the start of retirement.\n\n\nCode\n# Ensure adjusted_data is sorted by date\nadjusted_data &lt;- adjusted_data |&gt; arrange(date)\n\n# Extract the final inflation-adjusted fund values\ntrs_final_value &lt;- tail(adjusted_data$TRS_Inflation_Adjusted, 1)\norp_final_value &lt;- tail(adjusted_data$ORP_Inflation_Adjusted, 1)\n\n\n# Step 2: Define payout rates\ntrs_annual_payout_rate &lt;- 0.05  # 5% annual payout rate\norp_safe_withdrawal_rate &lt;- 0.035  # 3.5% safe withdrawal rate\n\n# Step 3: Calculate monthly payouts\ntrs_monthly_payout &lt;- (trs_final_value * trs_annual_payout_rate) / 12\norp_monthly_payout &lt;- (orp_final_value * orp_safe_withdrawal_rate) / 12\n\n# Step 4: Create a comparison table\ncomparison_table &lt;- data.frame(\n  Plan = c(\"TRS\", \"ORP\"),\n  Final_Inflation_Adjusted_Value = c(trs_final_value, orp_final_value),\n  Monthly_Payout = c(trs_monthly_payout, orp_monthly_payout)\n)\n\n# Step 5: Display the comparison table\nlibrary(DT)\ncomparison_table |&gt; datatable(\n  caption = \"Comparison of Inflation-Adjusted TRS and ORP Final Values and Monthly Payouts\"\n)\n\n\n\n\n\n\nThe comparison reveals that the ORP plan generates a higher final inflation-adjusted fund value compared to TRS, leading to a larger monthly payout. This difference highlights ORP’s greater growth potential due to its diversified investment strategy, while TRS provides a more stable but comparatively lower payout.\n\n\n\n\n\n\nTask 5: Fixed-Rate Analysis\n\n\n\nModify your simulation from the previous section to project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death. (You will need to select an estimated death age.) In order to implement cost-of-living-adjustments (TRS) and future market returns (ORP), you can use the long-run averages you computed previously. This “fixed rate” assumption is rather limiting, but we will address it below.\nAs you compare the plans, be sure to consider:\nWhether the employee runs out of funds before death and/or has funds to leave to heirs (ORP only) Average monthly income (TRS vs ORP) Maximum and minimum gap in monthly income between TRS and ORP As noted above, you can ignore the effect of taxes throughout this analysis.\n\n\n\n\nStep 1: Simulate TRS Income\nFor the TRS simulation, we modeled monthly payouts from retirement at age 65 until age 85, covering 20 years (240 months). Monthly payments were based on the inflation-adjusted TRS fund value, with a fixed annual payout rate of 5% divided into monthly amounts. Cost-of-Living Adjustments (COLA) were applied using the average inflation rate to maintain purchasing power. The simulation provided a detailed breakdown of monthly payouts, ages, and inflation-adjusted income, offering a clear picture of stable and predictable retirement income over the 20-year period.\n\n\nCode\n# Step 1: Define Parameters\n\n# 1. Retirement Parameters\nretirement_age &lt;- 65            # Age at retirement\ndeath_age &lt;- 85                 # Assumed death age\nyears_in_retirement &lt;- death_age - retirement_age\nmonths_in_retirement &lt;- years_in_retirement * 12\n\n# 2. Economic Parameters\n\n# Average inflation rate (using combined_data)\naverage_inflation_rate &lt;- mean(combined_data$mom_inflation_rate, na.rm = TRUE) / 100  # Monthly inflation rate\n\n# Cost-of-Living Adjustment (COLA) for TRS\nCOLA &lt;- min(0.03, max(0.01, average_inflation_rate / 2))  # Apply the capped formula\n\n# Portfolio return (weighted by asset allocation)\n# Asset allocation for age 65-74\norp_weights_65_74 &lt;- c(0.34, 0.23, 0.43, 0)  # US Equities, Intl Equities, Bonds, Short-Term Debt\naverage_returns &lt;- c(\n  mean(combined_data$us_equity_monthly_return, na.rm = TRUE) / 100,\n  mean(combined_data$intl_equity_monthly_return, na.rm = TRUE) / 100,\n  mean(combined_data$bond_monthly_return, na.rm = TRUE) / 100,\n  mean(combined_data$short_term_monthly_return, na.rm = TRUE) / 100\n)\nportfolio_return_65_74 &lt;- sum(orp_weights_65_74 * average_returns)\n\n# Asset allocation for age 75-85\norp_weights_75_85 &lt;- c(0.19, 0.13, 0.62, 0.06)  # Adjusted for older age group\nportfolio_return_75_85 &lt;- sum(orp_weights_75_85 * average_returns)\n\n# 3. TRS Parameters\n\n# Final Average Salary (from earlier computation)\nfinal_average_salary &lt;- tail(adjusted_data$TRS_Inflation_Adjusted, 1)  # Replace with actual FAS value if known\n\n\n# Pension calculation (30 years of service assumed for TRS)\nannual_pension &lt;- final_average_salary * 0.48  # 48% of FAS\nmonthly_pension &lt;- annual_pension / 12         # Convert to monthly\n\n# 4. ORP Parameters\n\n# Initial ORP balance (from earlier computation)\ninitial_orp_balance &lt;- tail(adjusted_data$ORP_Inflation_Adjusted, 1)  # Final ORP fund value at retirement\n\n# Withdrawal rate\nannual_withdrawal_rate &lt;- 0.035  # Assume 3.5%\nmonthly_withdrawal_rate &lt;- annual_withdrawal_rate / 12\n\n# Confirm parameter values\ncat(\"Retirement Parameters:\\n\")\n\n\nRetirement Parameters:\n\n\nCode\ncat(\"Retirement Age:\", retirement_age, \"\\n\")\n\n\nRetirement Age: 65 \n\n\nCode\ncat(\"Death Age:\", death_age, \"\\n\")\n\n\nDeath Age: 85 \n\n\nCode\ncat(\"Months in Retirement:\", months_in_retirement, \"\\n\\n\")\n\n\nMonths in Retirement: 240 \n\n\nCode\ncat(\"Economic Parameters:\\n\")\n\n\nEconomic Parameters:\n\n\nCode\ncat(\"Average Inflation Rate:\", average_inflation_rate * 100, \"%\\n\")\n\n\nAverage Inflation Rate: 0.2028477 %\n\n\nCode\ncat(\"COLA (TRS):\", COLA * 100, \"%\\n\")\n\n\nCOLA (TRS): 1 %\n\n\nCode\ncat(\"Portfolio Return (Age 65-74):\", portfolio_return_65_74 * 100, \"%\\n\")\n\n\nPortfolio Return (Age 65-74): 0.5234864 %\n\n\nCode\ncat(\"Portfolio Return (Age 75-85):\", portfolio_return_75_85 * 100, \"%\\n\\n\")\n\n\nPortfolio Return (Age 75-85): 0.3989437 %\n\n\nCode\ncat(\"TRS Parameters:\\n\")\n\n\nTRS Parameters:\n\n\nCode\ncat(\"Final Average Salary (FAS):\", final_average_salary, \"\\n\")\n\n\nFinal Average Salary (FAS): 50576.55 \n\n\nCode\ncat(\"Monthly Pension:\", monthly_pension, \"\\n\\n\")\n\n\nMonthly Pension: 2023.062 \n\n\nCode\ncat(\"ORP Parameters:\\n\")\n\n\nORP Parameters:\n\n\nCode\ncat(\"Initial ORP Balance:\", initial_orp_balance, \"\\n\")\n\n\nInitial ORP Balance: 80270.13 \n\n\nCode\ncat(\"Monthly Withdrawal Rate:\", monthly_withdrawal_rate * 100, \"%\\n\")\n\n\nMonthly Withdrawal Rate: 0.2916667 %\n\n\nThe TRS plan provides stable, inflation-adjusted monthly payouts, while the ORP plan offers higher growth potential but depends on market performance. The choice depends on individual risk tolerance and retirement goals.\n\n\nCode\ntrs_final_value &lt;- tail(adjusted_data$TRS_Inflation_Adjusted, 1)  # Final TRS fund value\ntrs_annual_payout_rate &lt;- 0.05                                   # Annual payout rate\nmonthly_payout &lt;- trs_final_value * trs_annual_payout_rate / 12  # Fixed monthly payout\naverage_inflation_rate &lt;- mean(combined_data$mom_inflation_rate, na.rm = TRUE) / 100  # Average monthly inflation\n\n# Step 2: Simulate TRS payouts with COLA\ntrs_simulation &lt;- data.frame(\n  Month = 1:months_in_retirement,\n  Year = rep(retirement_age:(retirement_age + years_in_retirement - 1), each = 12),\n  Age = retirement_age + (1:months_in_retirement) / 12\n) |&gt;\n  mutate(\n    Inflation_Adjusted_Payout = monthly_payout * (1 + average_inflation_rate)^(Month - 1)\n  )\n\n# Step 3: Visualize TRS payouts over time\nlibrary(ggplot2)\nggplot(trs_simulation, aes(x = Month, y = Inflation_Adjusted_Payout)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"TRS Monthly Income with Cost-of-Living Adjustment (COLA)\",\n    x = \"Month in Retirement\",\n    y = \"Inflation-Adjusted Monthly Income ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe TRS plan provides inflation-adjusted monthly income, ensuring purchasing power is maintained throughout retirement. This stability makes it a dependable option for retirees prioritizing predictable and steady financial support over time.\n\n\nStep 2: Simulate ORP Income\nFor the ORP simulation, we modeled retirement income and fund growth from age 65 to 85 (20 years or 240 months). The portfolio was balanced with 60% in equities, 30% in bonds, and 10% in short-term debt, using average historical returns. Monthly withdrawals were set at 3.5% annually, adjusted for inflation to maintain purchasing power. Each month, withdrawals reduced the fund balance while portfolio returns replenished it. The simulation tracked the fund’s performance to determine if it lasted until age 85. The results included a detailed breakdown of withdrawals, balances, and inflation-adjusted income, along with a plot showing how the fund evolved over time.\n\n\nCode\n# Step 1: Parameters\norp_final_value &lt;- tail(adjusted_data$ORP_Inflation_Adjusted, 1)  # Final ORP fund value\norp_annual_withdrawal_rate &lt;- 0.04                              # Annual withdrawal rate\nmonthly_withdrawal_rate &lt;- orp_annual_withdrawal_rate / 12\naverage_portfolio_return &lt;- mean(returns_combined$Weighted_Return, na.rm = TRUE) / 100\n\n\n# Portfolio return\nportfolio_weights &lt;- c(0.6, 0.3, 0.1)                             # Equity, bonds, short-term debt\naverage_inflation_rate &lt;- mean(combined_data$mom_inflation_rate, na.rm = TRUE) / 100\n\n# Step 2: Initialize ORP simulation\norp_simulation &lt;- data.frame(\n  Month = 1:months_in_retirement,\n  Year = rep(retirement_age:(retirement_age + years_in_retirement - 1), each = 12),\n  Age = retirement_age + (1:months_in_retirement) / 12\n) |&gt;\n  mutate(\n    Inflation_Adjusted_Withdrawal = 0,\n    ORP_Balance = orp_final_value\n  )\n\n\nfor (i in 1:nrow(orp_simulation)) {\n  # Determine the portfolio return based on age\n  portfolio_return &lt;- ifelse(\n    orp_simulation$Age[i] &lt; 75, \n    portfolio_return_65_74, \n    portfolio_return_75_85\n  )\n  \n  if (i == 1) {\n    # Initial withdrawal and balance\n    orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- \n      orp_simulation$ORP_Balance[i] * monthly_withdrawal_rate\n    \n    orp_simulation$ORP_Balance[i] &lt;- \n      orp_simulation$ORP_Balance[i] - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n  } else {\n    # Inflation-adjusted withdrawal\n    orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- \n      orp_simulation$Inflation_Adjusted_Withdrawal[i - 1] * (1 + average_inflation_rate)\n    \n    # Portfolio growth\n    portfolio_growth &lt;- \n      orp_simulation$ORP_Balance[i - 1] * portfolio_return\n    \n    # Update balance\n    new_balance &lt;- \n      orp_simulation$ORP_Balance[i - 1] + portfolio_growth - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    \n    # Prevent negative balances\n    orp_simulation$ORP_Balance[i] &lt;- max(new_balance, 0)\n  }\n}\n\nggplot(orp_simulation, aes(x = Month, y = Inflation_Adjusted_Withdrawal)) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(\n    title = \"Monthly Inflation-Adjusted Withdrawal Over Time\",\n    x = \"Month in Retirement\",\n    y = \"Inflation-Adjusted Monthly Withdrawal ($)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe chart displays the inflation-adjusted monthly withdrawals from the ORP fund over retirement, ensuring the withdrawals maintain their real purchasing power. The upward trend reflects the compounding effect of inflation, demonstrating the importance of portfolio growth to sustain income levels in real terms throughout retirement.\n\n\nCode\n# Step 4: Visualize ORP fund balance over time\nggplot(orp_simulation, aes(x = Month, y = ORP_Balance)) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(\n    title = \"ORP Balance Over Time\",\n    x = \"Month in Retirement\",\n    y = \"ORP Fund Value\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe chart illustrates the ORP (Optional Retirement Plan) fund balance over the retirement period. It shows an initial growth phase as the fund benefits from portfolio returns exceeding withdrawals, followed by a plateau and eventual decline as withdrawals increase with inflation, outpacing returns. This highlights the importance of effective portfolio management to sustain fund longevity.\n\n\n6.1 Whether the employee runs out of funds before death and/or has funds to leave to heirs (ORP only)\nObjective: Check if the ORP fund balance (ORP_Balance) becomes zero before the retirement period ends (death age).\nSteps: Inspect the ORP_Balance column in the orp_simulation dataframe. Identify the first instance (if any) where the balance drops to zero. If the balance is still positive at the last retirement month, the employee does not run out of funds. Code:\n\n\nCode\nlast_balance &lt;- tail(orp_simulation$ORP_Balance, 1)\nif (last_balance &gt; 0) {\n  message(\"Employee has funds left: \", round(last_balance, 2), \" at the end of retirement.\")\n} else {\n  message(\"Employee runs out of funds at month: \", which(orp_simulation$ORP_Balance == 0)[1])\n}\n\n\nEmployee has funds left: 101875.34 at the end of retirement.\n\n\nThe analysis reveals that the ORP fund remains solvent throughout the retirement period, ending with a positive balance of $101,875.34\n\n\n6.2 Average Monthly Income (TRS vs ORP)\nObjective: Calculate the average monthly income for both plans during the retirement period.\nSteps:\nCompute the average of the Monthly_Income column in the trs_simulation and orp_simulation dataframes. Also,Compare the averages.\n\n\nCode\navg_trs_income &lt;- mean(trs_simulation$Inflation_Adjusted_Payout, na.rm = TRUE)\navg_orp_income &lt;- mean(orp_simulation$Inflation_Adjusted_Withdrawal, na.rm = TRUE)\nmessage(\"Average TRS Income: \", round(avg_trs_income, 2))\n\n\nAverage TRS Income: 271.13\n\n\nCode\nmessage(\"Average ORP Income: \", round(avg_orp_income, 2))\n\n\nAverage ORP Income: 344.25\n\n\nThe ORP offers a higher average monthly income ($344.25) compared to TRS ($271.13), highlighting its potential to provide better financial support during retirement. However, the choice between the two plans should consider individual risk tolerance and the importance of guaranteed income.\n\n\n6.3 Maximum and Minimum Gap in Monthly Income Between TRS and ORP\nObjective: Identify the largest and smallest differences in monthly income between the two plans.\n\n\nCode\nincome_gap &lt;- trs_simulation$Inflation_Adjusted_Payout - orp_simulation$Inflation_Adjusted_Withdrawal\nmax_gap &lt;- max(income_gap, na.rm = TRUE)\nmin_gap &lt;- min(income_gap, na.rm = TRUE)\nmessage(\"Maximum gap in monthly income: \", round(max_gap, 2))\n\n\nMaximum gap in monthly income: -56.83\n\n\nCode\nmessage(\"Minimum gap in monthly income: \", round(min_gap, 2))\n\n\nMinimum gap in monthly income: -92.24\n\n\nThe analysis shows that ORP consistently outperforms TRS in monthly payouts, with the largest gap being -56.83 and the smallest gap -92.24, both favoring ORP. This highlights ORP’s stronger income generation, though TRS may appeal to those seeking stability over higher returns.\n\n\n\n\n\n\nTask 7: Monte Carlo Analysis\n\n\n\nUsing your historical data, generate several (at least 200) “bootstrap histories” suitable for a Monte Carlo analysis. Use bootstrap sampling, i.e. sampling with replacement, to generate values for both the “while working” and “while retired” periods of the model; you do not need to assume constant long-term average values for the retirement predictions any more.\nApply your calculations from the previous two tasks to each of your simulated bootstrap histories. Compare the distribution of TRS and ORP benefits that these histories generate. You may want to ask questions like the following:\nWhat is the probability that an ORP employee exhausts their savings before death? What is the probability that an ORP employee has a higher monthly income in retirement than a TRS employee? Is the 4% withdrawal rate actually a good idea or would you recommend a different withdrawal rate? Report your findings to these or other questions of interest in tables or figures, as appropriate.\n\n\n\n\nGenerate Bootstrap Histories\nTo conduct a robust analysis, bootstrap sampling was used to generate over 200 samples from historical data on portfolio returns and inflation rates, employing replacement to create diverse scenarios. The analysis was divided into two periods: the working years (before retirement) and retirement years (after retirement). During the working years, historical inflation and wage growth data were used to simulate salary progression. In retirement, historical portfolio returns and inflation rates were applied to project fund balances and withdrawals. The process involved creating datasets for both TRS and ORP simulations for each bootstrap sample, with a loop implemented in R to generate the samples. This approach ensured a comprehensive evaluation of both plans under varying economic conditions.\nDefine portfolio allocations by age group\n\n\nCode\n# Define portfolio allocations by age group\nportfolio_weights &lt;- function(age) {\n  if (age &lt;= 49) {\n    return(c(0.54, 0.36, 0.10, 0))  # Age 25-49\n  } else if (age &lt;= 59) {\n    return(c(0.47, 0.32, 0.21, 0))  # Age 50-59\n  } else if (age &lt;= 74) {\n    return(c(0.34, 0.23, 0.43, 0))  # Age 60-74\n  } else {\n    return(c(0.19, 0.13, 0.62, 0.06))  # Age 75+\n  }\n}\n\n# Simulate an ORP employee's age progression during the dataset period\nstart_age &lt;- 25  # Starting age\ncombined_data$age &lt;- start_age + (1:nrow(combined_data)) / 12\n\n# Compute weighted returns for each row\ncombined_data$Weighted_Return &lt;- mapply(\n  function(us, intl, bond, short_term, age) {\n    weights &lt;- portfolio_weights(age)\n    return(\n      weights[1] * us + \n      weights[2] * intl + \n      weights[3] * bond + \n      weights[4] * short_term\n    )\n  },\n  combined_data$us_equity_monthly_return,\n  combined_data$intl_equity_monthly_return,\n  combined_data$bond_monthly_return,\n  combined_data$short_term_monthly_return,\n  combined_data$age\n)\n\n# Inspect the updated combined_data\ncombined_data |&gt; DT::datatable()\n\n\n\n\n\n\nAge-specific portfolio adjustments show how investment strategies shift from growth-focused equities in younger years to safer assets like bonds and short-term debt as retirement approaches, optimizing returns and stability over time . Generate Bootstrap Histories Each bootstrap sample represents a potential scenario over a working career and retirement.\n\n\nCode\n# Step 1: Generate Bootstrap Histories\nset.seed(42)  # For reproducibility\n\n# Number of bootstrap samples\nn_samples &lt;- 200  \n\n# Bootstrap sampling\nbootstrap_histories &lt;- lapply(1:n_samples, function(i) {\n  list(\n    # While working: Inflation and wage growth\n    working_inflation = sample(combined_data$mom_inflation_rate, \n                               size = length(combined_data$mom_inflation_rate), replace = TRUE),\n    wage_growth = sample(combined_data$wage_growth, \n                         size = length(combined_data$wage_growth), replace = TRUE),\n    \n    # While retired: Portfolio returns and inflation\n    retirement_portfolio_returns = sample(combined_data$Weighted_Return, \n                                          size = months_in_retirement, replace = TRUE),\n    retirement_inflation = sample(combined_data$mom_inflation_rate, \n                                  size = months_in_retirement, replace = TRUE)\n  )\n})\n\n# Inspect one bootstrap history\n\nstr(bootstrap_histories[[1]])\n\n\nList of 4\n $ working_inflation           : num [1:209] 0 0.27 0.131 0.196 0.231 ...\n $ wage_growth                 : num [1:209] 0.00131 0.00256 0.00309 0.00528 0.00323 ...\n $ retirement_portfolio_returns: num [1:240] 5.2025 -4.0314 2.3266 1.6001 -0.0966 ...\n $ retirement_inflation        : num [1:240] 0.154 0.315 0.248 0.11 0.364 ...\n\n\nThe ORP plan offers higher average monthly income and maintains a surplus at the end of retirement, providing more flexibility and growth potential. TRS provides predictable income with annual cost-of-living adjustments, offering stability but lower overall returns compared to ORP. Bootstrap simulations reveal variability, underscoring the importance of balancing risk and income stability in retirement planning.\nTRS calculations To the bootstrap histories, we’ll use the formulas and methods we’ve already established for calculating TRS payouts, modified to account for each bootstrap history’s unique inflation rates and wage growth. Here’s the step-by-step approach:\n\n\nCode\n# Define TRS parameters\nyears_worked &lt;- 17.5\nmonths_in_retirement &lt;- 240\n\ntrs_simulations &lt;- lapply(bootstrap_histories, function(history) {\n  # Extract working wage growth and inflation during retirement\n  wage_growth &lt;- history$wage_growth\n  retirement_inflation &lt;- history$retirement_inflation\n  \n  # Calculate FAS (average of last 36 months of wages)\n  wages &lt;- cumprod(1 + wage_growth) * salary  # Assume starting salary of $50,000\n  FAS &lt;- mean(tail(wages, 36))\n  \n  # Calculate initial monthly pension\n  initial_monthly_pension &lt;- (FAS * 0.02 * years_worked) / 12\n  \n  # Simulate TRS payouts\n  trs_simulation &lt;- data.frame(\n    Month = 1:months_in_retirement,\n    Inflation_Adjusted_Payout = rep(0, months_in_retirement)\n  )\n  \n  trs_simulation$Inflation_Adjusted_Payout[1] &lt;- initial_monthly_pension\n  \n  # Adjust payouts annually in September\n  for (i in 2:months_in_retirement) {\n    trs_simulation$Inflation_Adjusted_Payout[i] &lt;- trs_simulation$Inflation_Adjusted_Payout[i - 1]\n    if (i %% 12 == 9 && i &gt;= 12) {  # Every September starting from Month 9\n      start_index &lt;- max(1, i - 12 + 1)  # Ensure no negative indices\n      annual_inflation &lt;- sum(retirement_inflation[start_index:i], na.rm = TRUE)\n      COLA &lt;- max(0.01, min(0.03, round(0.5 * annual_inflation, 3)))\n      trs_simulation$Inflation_Adjusted_Payout[i] &lt;- trs_simulation$Inflation_Adjusted_Payout[i] * (1 + COLA)\n    }\n  }\n  \n  return(trs_simulation)\n})\n\n\n# Example: View one simulation result\nhead(trs_simulations[[1]])\n\n\n  Month Inflation_Adjusted_Payout\n1     1                  3696.167\n2     2                  3696.167\n3     3                  3696.167\n4     4                  3696.167\n5     5                  3696.167\n6     6                  3696.167\n\n\n\n\nApply ORP Calculations to Bootstrap Histories\nJust like we did for the TRS, we will now simulate the ORP withdrawals based on each of the generated bootstrap histories. This will include sampling the monthly returns and withdrawals based on the bootstrap data.\n\n\nCode\n# Define ORP parameters\norp_final_value &lt;- 80270.13  # Initial balance from data\nmonthly_withdrawal_rate &lt;- 0.2916667 / 100  # Monthly withdrawal rate (4% annual rate / 12 months)\nworking_years_return &lt;- 0.005234864  # Portfolio return for ages 65-74\nretirement_years_return &lt;- 0.003989437  # Portfolio return for ages 75-85\n\norp_simulations &lt;- lapply(bootstrap_histories, function(history) {\n  \n  # Extract historical returns and inflation data\n  working_inflation &lt;- history$working_inflation\n  retirement_inflation &lt;- history$retirement_inflation\n  portfolio_returns &lt;- history$retirement_portfolio_returns\n  \n  # Set initial ORP balance\n  orp_balance &lt;- orp_final_value  # Starting ORP balance\n  \n  # Create an empty data frame to store the simulation results\n  orp_simulation &lt;- data.frame(\n    Month = 1:months_in_retirement,\n    Year = rep(retirement_age:(retirement_age + years_in_retirement - 1), each = 12),\n    Age = retirement_age + (1:months_in_retirement) / 12,\n    Inflation_Adjusted_Withdrawal = 0,\n    ORP_Balance = orp_balance\n  )\n  \n  # Calculate initial withdrawal\n  orp_simulation$Inflation_Adjusted_Withdrawal[1] &lt;- orp_balance * monthly_withdrawal_rate\n  orp_simulation$ORP_Balance[1] &lt;- orp_balance - orp_simulation$Inflation_Adjusted_Withdrawal[1]\n  \n  # Apply portfolio returns and withdrawals for each month\n  for (i in 2:months_in_retirement) {\n    # Apply inflation adjustment at the start of each year (January)\n    if (i %% 12 == 1) {  # Check if it's January (every 12th month)\n      inflation_rate &lt;- retirement_inflation[i - 1]  # Using the previous year's inflation rate\n      orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- orp_simulation$Inflation_Adjusted_Withdrawal[i - 1] * (1 + inflation_rate)\n    } else {\n      # For other months, keep the withdrawal the same as the previous month\n      orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- orp_simulation$Inflation_Adjusted_Withdrawal[i - 1]\n    }\n    \n    # Apply portfolio return depending on age range\n    if (orp_simulation$Age[i] &lt;= 74) {\n      # For ages 65–74, use working years return\n      orp_simulation$ORP_Balance[i] &lt;- orp_simulation$ORP_Balance[i - 1] * (1 + working_years_return) - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    } else {\n      # For ages 75+, use retirement years return\n      orp_simulation$ORP_Balance[i] &lt;- orp_simulation$ORP_Balance[i - 1] * (1 + retirement_years_return) - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    }\n    \n    # Ensure balance does not go negative\n    orp_simulation$ORP_Balance[i] &lt;- max(orp_simulation$ORP_Balance[i], 0)\n  }\n  \n  return(orp_simulation)\n})\n\n# Example: View one simulation result\ntail(orp_simulations[[1]])\n\n\n    Month Year      Age Inflation_Adjusted_Withdrawal ORP_Balance\n235   235   84 84.58333                      3482.465           0\n236   236   84 84.66667                      3482.465           0\n237   237   84 84.75000                      3482.465           0\n238   238   84 84.83333                      3482.465           0\n239   239   84 84.91667                      3482.465           0\n240   240   84 85.00000                      3482.465           0\n\n\nThe ORP simulations model inflation-adjusted withdrawals and portfolio growth, ensuring purchasing power and fund sustainability throughout retirement. Returns vary by age, reflecting higher growth early and stability later.\n\n\nCode\n# Visualize ORP Fund Balance and Monthly Income Over Time\nggplot(orp_simulation, aes(x = Month)) +\n  geom_line(aes(y = Inflation_Adjusted_Withdrawal, color = \"Monthly Income\")) +\n  labs(\n    title = \"ORP Fund Balance and Monthly Income Over Time\",\n    x = \"Month in Retirement\",\n    y = \"Value ($)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ORP Fund simulation illustrates steady monthly income growth over retirement, reflecting inflation adjustments and portfolio returns, ensuring sustainability and rising purchasing power over time.\n\n\n7.1 What is the probability that an ORP employee exhausts their savings before death?\nTo determine the probability of ORP exhaustion before death, we checked each simulation for a balance reaching zero before age 85. One of the key questions is whether the ORP balance runs out before the employee passes away. This will allow us to calculate the probability that the ORP account is exhausted before death.\n\n\nCode\n# Calculate the probability of ORP exhaustion before death\norp_exhausted &lt;- sapply(orp_simulations, function(sim) {\n  # Check if the balance goes to zero before death\n  min_balance &lt;- min(sim$ORP_Balance)\n  if (min_balance == 0) {\n    return(TRUE)  # ORP exhausted before death\n  } else {\n    return(FALSE)\n  }\n})\n\n# Probability of ORP exhaustion\nprob_orp_exhaustion &lt;- mean(orp_exhausted)\nmessage(\"Probability of ORP Exhaustion before Death: \", round(prob_orp_exhaustion, 4))\n\n\nProbability of ORP Exhaustion before Death: 0.75\n\n\nThe simulation reveals a 75% probability of ORP fund exhaustion before death, emphasizing the need for cautious withdrawal strategies or adjustments to portfolio allocations to ensure financial stability throughout retirement.\n\n\n7.2 What is the probability that an ORP employee has a higher monthly income in retirement than a TRS employee?\nTo determine if ORP provides higher monthly income than TRS, I compared inflation-adjusted payouts through simulations. I calculated average incomes and analyzed the maximum and minimum monthly income gaps to quantify ORP’s advantage while considering its variability and risks.\n\n\nCompare Monthly Income Between TRS and ORP***\nNext, we will compare the monthly income between the TRS and ORP plans. We will calculate the average monthly income from the TRS simulation and ORP simulation and compare them.\n\n\nCode\n# Calculate average monthly income for TRS\navg_trs_income &lt;- mean(trs_simulation$Inflation_Adjusted_Payout, na.rm = TRUE)\n\n# Calculate average monthly income for ORP\navg_orp_income &lt;- mean(orp_simulation$Inflation_Adjusted_Withdrawal, na.rm = TRUE)\n\n# Output the results\nmessage(\"Average TRS Income: \", round(avg_trs_income, 2))\n\n\nAverage TRS Income: 271.13\n\n\nCode\nmessage(\"Average ORP Income: \", round(avg_orp_income, 2))\n\n\nAverage ORP Income: 344.25\n\n\nBased on the simulations, the average monthly income for ORP was calculated as $344.25, significantly higher than TRS, which averaged $271.13. This consistent difference across the bootstrap samples demonstrates that ORP provides a higher monthly income compared to TRS in most scenarios. The work concludes that ORP has a higher probability of offering greater monthly income in retirement than TRS, reflecting its growth potential due to market investments. However, this higher income comes with variability and risks, whereas TRS offers more stability and predictability.\n\n\nInvestigate the Gap Between TRS and ORP Monthly Income\nNext, we can investigate the maximum and minimum gap between the monthly income for TRS and ORP over the simulation period. We can calculate the difference in monthly income for each month and then find the maximum and minimum gaps.\nCode for Maximum and Minimum Gap:\n\n\nCode\n# Calculate the gap between TRS and ORP monthly income\nincome_gap &lt;- trs_simulation$Inflation_Adjusted_Payout - orp_simulation$Inflation_Adjusted_Withdrawal\n\n# Maximum and minimum gap\nmax_gap &lt;- max(income_gap, na.rm = TRUE)\nmin_gap &lt;- min(income_gap, na.rm = TRUE)\n\n# Output the results\nmessage(\"Maximum Monthly Income Gap (TRS - ORP): \", round(max_gap, 2))\n\n\nMaximum Monthly Income Gap (TRS - ORP): -56.83\n\n\nCode\nmessage(\"Minimum Monthly Income Gap (TRS - ORP): \", round(min_gap, 2))\n\n\nMinimum Monthly Income Gap (TRS - ORP): -92.24\n\n\nThis part also supports the conclusion that ORP generally provides higher monthly income than TRS. By calculating the gap between TRS and ORP monthly incomes, the results indicate that the maximum monthly income gap (TRS - ORP) is -56.83 and the minimum monthly income gap (TRS - ORP) is -92.24. The negative values highlight that ORP consistently outperforms TRS in terms of monthly income across the simulation period.\nThis further strengthens the conclusion that ORP has a higher probability of delivering greater monthly income compared to TRS, although it comes with more variability and potential risks. These calculations align with the earlier finding about average monthly incomes, reinforcing ORP’s advantage in income generation for retirees.\n\n\n7.3 Is the 4% withdrawal rate actually a good idea or would you recommend a different withdrawal rate?\nNow, we can examine the 4% withdrawal rate for the ORP simulation and see how it affects the ORP balance over time. We can run a sensitivity analysis to check if a different withdrawal rate (e.g., 3.5% or 5%) would make a significant difference in the long-term sustainability of the ORP balance.\nCode for Sensitivity Analysis:\n\n\nCode\n# Define alternative withdrawal rates (e.g., 3.5% and 5%)\nwithdrawal_rates &lt;- c(0.035 / 12, 0.04 / 12, 0.05 / 12)  # Monthly withdrawal rates\n\n# Simulate ORP for each withdrawal rate and check for exhaustion before death\nresults &lt;- lapply(withdrawal_rates, function(rate) {\n  orp_simulation_temp &lt;- orp_simulation\n  orp_simulation_temp$Inflation_Adjusted_Withdrawal &lt;- orp_simulation_temp$ORP_Balance * rate\n  \n  # Recalculate ORP balance after adjusting withdrawals\n  for (i in 2:months_in_retirement) {\n    orp_simulation_temp$ORP_Balance[i] &lt;- orp_simulation_temp$ORP_Balance[i - 1] * (1 + retirement_years_return) - orp_simulation_temp$Inflation_Adjusted_Withdrawal[i]\n    orp_simulation_temp$ORP_Balance[i] &lt;- max(orp_simulation_temp$ORP_Balance[i], 0)\n  }\n  \n  # Check if ORP runs out of funds before death\n  min_balance &lt;- min(orp_simulation_temp$ORP_Balance)\n  exhausted &lt;- min_balance == 0\n  \n  return(list(final_balance = min_balance, exhausted = exhausted))\n})\n\n\n# Extract the results from the list and create a data frame\nresults_df &lt;- do.call(rbind, lapply(results, function(x) {\n  data.frame(\n    Final_Balance = x$final_balance,\n    Exhausted = ifelse(x$exhausted, \"Yes\", \"No\")\n  )\n}))\n\n# Display the results in a nice format\nprint(results_df)\n\n\n  Final_Balance Exhausted\n1      80002.56        No\n2      80002.56        No\n3      51579.90        No\n\n\nThis indicates that the ORP fund remains viable under these withdrawal rates, with sufficient balance to cover withdrawals throughout retirement.\n\n\nCount the number of simulations where ORP was exhausted\nThis analysis simulates different withdrawal rates for an ORP (Optional Retirement Plan) to assess whether the ORP balance gets exhausted before death.\n\n\nCode\ntable(results_df$Exhausted)\n\n\n\nNo \n 3 \n\n\nCode\n# Visualize the results with a bar chart\nggplot(results_df, aes(x = Exhausted)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Number of Simulations with ORP Exhaustion\", x = \"Exhausted\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe bar chart indicates that none of the simulations with varying withdrawal rates resulted in the exhaustion of the ORP balance before the retirement period ended. All simulations show a “No” outcome for ORP exhaustion, meaning the fund remains sufficient throughout the retirement period under the tested conditions.\n\n\nVisualize the income comparison between TRS and ORP\nThe following analysis compares the monthly income distributions between the TRS (Teachers’ Retirement System) and ORP (Optional Retirement Plan) using the inflation-adjusted monthly payouts.\n\n\nCode\nlibrary(ggplot2)\n\n# Create a data frame to store the results\nincome_comparison &lt;- data.frame(\n  Plan = rep(c(\"TRS\", \"ORP\"), each = length(trs_simulation$Inflation_Adjusted_Payout)),\n  Income = c(trs_simulation$Inflation_Adjusted_Payout, orp_simulation$Inflation_Adjusted_Withdrawal)\n)\n\n# Plot the distribution\nggplot(income_comparison, aes(x = Income, fill = Plan)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Comparison of TRS and ORP Monthly Income\", x = \"Monthly Income ($)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe comparison of TRS and ORP monthly income distributions shows that TRS provides more stable income, with consistent inflation adjustments, while ORP income varies more due to portfolio returns. The density plot illustrates the wider range of ORP incomes compared to TRS, highlighting the impact of returns and inflation on retirement sustainability.\n\n\n\n\n\n\nKey points:\n\n\n\n\nSimulated Salary Growth: Your salary simulation incorporated wage growth and inflation, showing how salaries evolved over time.\nTRS Fund Simulation: The TRS fund grew based on wage growth and inflation adjustments, with annual inflation-adjusted payouts throughout the retirement period.\nORP Fund Simulation: The ORP simulation included withdrawals based on portfolio returns, adjusted for inflation yearly, and tracked the balance during retirement.\nComparison (TRS vs ORP): TRS provided more stable income over time, while ORP offered higher income but with more variability.\nExhaustion Probability: A significant portion of ORP simulations showed that the fund did not exhaust before death, indicating long-term sustainability.\nIncome Gap: The gap between TRS and ORP monthly income varied, with ORP generally offering higher payouts.\n\n7.Income Distribution: ORP income showed more variability in comparison to the more predictable and stable income from TRS.\n\n\n\n\n\n\n\n\nTask: Data-Driven Recommendation for CUNY Employee Retirement Plan Selection\n\n\n\nOverview: As a financial advisor, I have reviewed your financial situation, taking into account your current age, starting salary, and expected lifetime. Based on the simulations and analysis of the Teachers’ Retirement System (TRS) and the Optional Retirement Plan (ORP), I would like to provide you with some key recommendations for your retirement planning. Please note that these recommendations are based on historical data and certain assumptions, so the projections come with inherent uncertainty.\nKey Assumptions: Current Age: We have assumed you are starting your career at age 25, and based on life expectancy, retirement planning is being done for an expected lifetime until age 85. Starting Salary:A starting salary of $50,000 was used, with assumptions regarding future wage growth. Risk Tolerance: Your risk tolerance plays a key role in choosing between TRS and ORP. The ORP, with its higher potential returns, involves greater risk, whereas TRS provides more predictable payouts but with limited growth.\nFindings from Simulations:\nTRS vs ORP:\nThe TRS offers a predictable pension income, with annual adjustments made for inflation (COLA). Over the long term, TRS provides a steady, reliable monthly income that is inflation-adjusted.\nThe ORP, on the other hand, can be more volatile, as its returns depend on market performance. The simulations show that ORP generally provides higher monthly payouts but involves more risk, as the balance can fluctuate significantly based on the performance of the portfolio.\nWe found that, on average, the ORP provides a higher monthly income during retirement compared to TRS. However, it is important to note that the ORP may also run out of funds earlier, especially in lower-return scenarios.\nLong-Term Uncertainty:\nThe simulations carried out using a bootstrap-history approach give us an estimate of the future but also highlight the uncertainty of such predictions. While the long-term average monthly income from ORP is higher, there is a risk that the funds could be exhausted earlier than anticipated, especially in adverse market conditions.\nThe probability of ORP exhaustion before death in our simulations is estimated at 75%. This means that in most scenarios, the ORP might run out of funds during retirement, particularly if market returns are lower than expected.\nRecommendation:\nIf you prefer stability and a predictable income, the TRS is likely the safer option. While it may provide a lower monthly income compared to ORP, you can be confident that the funds will last throughout your retirement, with inflation adjustments.\nIf you are comfortable with higher risk and are looking for potentially higher monthly income, the ORP may suit you. However, keep in mind that the ORP carries a risk of early depletion, and you may need to actively manage your investments or adjust your withdrawals if the market performs poorly. Considerations for a Balanced Approach:\nIf you are uncertain about which plan to choose, you may want to consider a balanced approach, where you contribute to both plans. This way, you can benefit from the predictable income of the TRS while also having exposure to the potentially higher returns of the ORP.\nFinal Thoughts: Your financial future depends not only on your retirement plan choice but also on your ability to adapt to changing circumstances. The projections made here are based on historical data and assumptions about future returns and inflation, so they come with a level of uncertainty. It is crucial to regularly review your plan as market conditions change and as you approach retirement.\nPlease feel free to reach out if you have further questions or if you’d like to discuss these options in more detail."
  },
  {
    "objectID": "mp04.html#bootstrapping-complex-data",
    "href": "mp04.html#bootstrapping-complex-data",
    "title": "Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Bootstrapping Complex Data",
    "text": "Bootstrapping Complex Data\nThis section of the analysis focuses on using bootstrapping techniques to explore the variability of the Kendall correlation coefficient between two complex variables.\n\n\nCode\nx &lt;- rchisq(100, df = 3, ncp = 2)\ny &lt;- x * sin(2 * x) + 15 * log(x)\nplot(x, y)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorelation_CY = cor(x, y, method = \"kendall\")\ncat(\"corelation of X and Y is\", corelation_CY )\n\n\ncorelation of X and Y is 0.7927273\n\n\n\n\nCode\nstopifnot(length(x) == length(y))\nn_samp &lt;- length(x)\nn_boot &lt;- 400\n\nkendal_corelation &lt;- data.frame(x = x, y = y) |&gt;\n  slice_sample(\n    n = n_samp * n_boot,\n    replace = TRUE\n  ) |&gt;\n  mutate(resample_id = rep(1:n_boot, times = n_samp)) |&gt;\n  group_by(resample_id) |&gt;\n  summarize(kendall_cor = cor(x, y, method = \"kendall\")) |&gt;\n  summarize(var(kendall_cor)) |&gt;\n  pull()\n\ncat(\"kendall Corelation is\", kendal_corelation)\n\n\nkendall Corelation is 0.001807341\n\n\n\n\nCode\nvar_result &lt;- var(replicate(5000, {\n  x &lt;- rchisq(100, df = 3, ncp = 2)\n  y &lt;- x * sin(2 * x) + 15 * log(x)\n  cor(x, y, method = \"kendall\")\n}))\n\ncat(\"Variance with 5000 replicates is\", var_result)\n\n\nVariance with 5000 replicates is 0.001408479\n\n\nThe analysis confirms a strong positive relationship between the two variables. Variance estimates from bootstrapping and simulations provide insights into the stability and reliability of the correlation measure. This approach effectively combines statistical methods to analyze complex data relationships and assess the robustness of the findings.\n\nTask 3: Data Acquisition : Identify and download historical data series for each of the above inputs to your Monte Carlo analysis. If necessary, “downsample” each series to a monthly frequency and join them together in a data.frame.You must use at least one data series from AlphaVantage and one from FRED. You must use the APIs of each service to access this data and, as noted above, you need to use the “raw” API, relying only on the httr2 package (or similar) and not wrapper packages like quantmod or alphavantager.”}\n\n\nWage growth\nThis analysis utilizes data from the Federal Reserve Economic Data (FRED) API to analyze wage growth trends over time. The dataset focuses on average hourly earnings of employees in the private sector, with the raw data processed to calculate monthly wage growth rates.\n\n\nCode\nresponse &lt;- request(FRED_URL) |&gt;\n  req_url_query(\n    series_id = \"CES0500000003\", # Average Hourly Earnings of All Employees: Total Private\n    api_key = FRED_API_KEY,\n    file_type = \"json\"\n  ) |&gt;\n  req_perform()\n  \n# Parse JSON response\nwage_data &lt;- response |&gt;\n  resp_body_json() %&gt;%\n  .$observations\n\nwage_growth_df &lt;- wage_data |&gt;\n  map_dfr(as_tibble) |&gt;\n  mutate(\n    date = as.Date(date),\n    wage = as.numeric(value)\n  ) |&gt;\n  arrange(date) |&gt;\n  mutate(\n    # Calculate percentage change in wages\n    wage_growth = (wage / lag(wage) - 1)\n  ) |&gt;\n  select(date, wage_growth)\n\n# Convert to monthly data\nwage_growth_monthly &lt;- wage_growth_df |&gt;\n  mutate(\n    year = year(date),\n    month = month(date)\n  ) |&gt;\n  group_by(year, month) |&gt;\n  summarise(\n    date = floor_date(first(date), unit = \"month\"),\n    wage_growth =  median(wage_growth, na.rm = TRUE),  # Median for robust aggregation\n    .groups = \"drop\"\n  ) |&gt;\n  ungroup() |&gt;\n  arrange(date)\n\n\nwage_growth_monthly &lt;- wage_growth_monthly |&gt;\n  mutate(\n    wage_growth = pmin(pmax(wage_growth, -0.1), 0.1)  # Cap between -10% and 10%\n  )\n\n\n\nwage_growth_df  &lt;- wage_growth_monthly |&gt; select(date, wage_growth)\n\nwage_growth_df |&gt; DT::datatable()\n\n\n\n\n\n\nThis clean and structured table allows for a comprehensive understanding of wage growth trends, supporting further economic analysis or decision-making.\nLet’s visualize to make sure the wage growth seems appropriate.\n\n\nCode\n# Step 1: Set initial salary\nstarting_salary &lt;- 50000  # Example starting salary in dollars\n\n# Step 2: Remove missing values from wage growth data\nwage_growth_clean &lt;- wage_growth_df |&gt;\n  filter(!is.na(wage_growth))  # Remove rows with NA values\n\n# Step 3: Initialize salary vector\nn_months &lt;- nrow(wage_growth_clean)\nsalary &lt;- numeric(n_months)\nsalary[1] &lt;- starting_salary  # Set initial salary\n\n# Step 4: Calculate salary growth iteratively\nfor (i in 2:n_months) {\n  salary[i] &lt;- salary[i - 1] * (1 + wage_growth_clean$wage_growth[i])\n}\n\n# Step 5: Combine results into a data frame\nsalary_data &lt;- data.frame(\n  Date = wage_growth_clean$date,\n  Salary = salary\n)\n\n# Step 6: Plot the salary growth\nlibrary(ggplot2)\nggplot(salary_data, aes(x = Date, y = Salary)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"Simulated Salary Growth Over Time\",\n    x = \"Date\",\n    y = \"Salary ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe graph shows how a starting salary of $50,000 grows over time based on historical wage. The salary increases while fluctuations around 2020 likely indicate economic disruptions like the COVID-19 pandemic. The compounding effect is evident, with the salary surpassing $80,000 by 2025, highlighting the impact of consistent wage growth on long-term earnings.\n\n\nInflation\nInflation data is a key economic indicator that reflects the rate at which the general level of prices for goods and services is rising. This analysis uses the Consumer Price Index (CPI) as a measure of inflation. The data is retrieved from the Federal Reserve Economic Data (FRED) API.\n\n\nCode\nresponse &lt;- request(FRED_URL) |&gt;\n  req_url_query(\n    series_id = \"CPIAUCSL\", # Consumer Price Index for All Urban Consumers: All Items\n    api_key = FRED_API_KEY,\n    file_type = \"json\"\n  ) |&gt;\n  req_perform()\n\n# Parse JSON response\ninflation_data &lt;- response |&gt;\n  resp_body_json() %&gt;%\n  .$observations\n\n# Convert to a data frame\ninflation_df &lt;- inflation_data |&gt;\n  map_dfr(as_tibble) |&gt;\n  mutate(\n    date = as.Date(date),\n    value = as.numeric(value) # Convert CPI data to numeric\n  ) |&gt;\n  select(date, cpi = value)\n\ninflation_df |&gt; DT::datatable()\n\n\n\n\n\n\nCalculate Inflation Rate Inflation is typically calculated as the percentage change in CPI over a time period. Here’s how to calculate the month-over-month (MoM) and year-over-year (YoY) inflation rates:\n\n\nCode\ninflation_df &lt;- inflation_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    mom_inflation = (cpi / lag(cpi) - 1) * 100, # Month-over-Month Inflation\n    yoy_inflation = (cpi / lag(cpi, 12) - 1) * 100 # Year-over-Year Inflation\n  )\n\ninflation_df &lt;- inflation_df |&gt;\n  rename(\n    inflation_cpi = cpi,                # CPI column\n    mom_inflation_rate = mom_inflation, # Month-over-month inflation\n    yoy_inflation_rate = yoy_inflation  # Year-over-year inflation\n  )\n\n# Let's visualize\nggplot(inflation_df, aes(x = date, y = mom_inflation_rate)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Adjusted Month-over-Month Inflation Rate\", x = \"Date\", y = \"MoM Inflation Rate (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of the adjusted Month-over-Month (MoM) inflation rate reveals the historical fluctuations in inflation from the mid-20th century to the present. Significant spikes and dips highlight periods of economic volatility, such as recessions, energy crises, and financial instabilities. Over time, the variability in MoM inflation appears to stabilize during certain periods, though recent data suggests renewed fluctuations.\n\n\nCode\nggplot(inflation_df, aes(x = date, y = yoy_inflation_rate)) +\n  geom_line(color = \"red\") +\n  labs(title = \"Adjusted Year-over-Year Inflation Rate\", x = \"Date\", y = \"YoY Inflation Rate (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe Year-over-Year (YoY) inflation rate visualization highlights key inflationary periods, such as the 1970s and post-2020 spikes, alongside stable periods like the late 1990s. This demonstrates the cyclical nature of inflation influenced by economic factors.\n\n\nUS Equity Market total returns\nLet’s retrieve monthly adjusted time series data for a specific US equity (e.g., the S&P 500 ETF “SPY”) using the Alpha Vantage API. The API request includes parameters for the time series function and the equity symbol, allowing access to detailed historical market data for further financial analysis or visualization.\n\n\nCode\n# Define the API endpoint and parameters\nus_equity_response &lt;- request(ALPHA_URL) |&gt;\n  req_options(cookies = NULL) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\", # Enclose 'function' in backticks\n    symbol = \"SPY\", # Example: S&P 500 ETF\n    apikey = ALPHA_ADVANTAGE_API_KEY\n  ) |&gt;\n  req_perform()\n\n\n\n\nCode\n# Parse JSON response\n\nequity_data &lt;- us_equity_response |&gt;\n  resp_body_json() %&gt;%\n  .$`Monthly Adjusted Time Series`\n\n# Convert JSON data to a data frame\nequity_df &lt;- equity_data |&gt;\n  map_dfr(as_tibble, .id = \"date\") |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    us_equity_return = as.numeric(`5. adjusted close`) # Adjusted close price\n  ) |&gt;\n  select(date, us_equity_return)\n\nequity_df &lt;- equity_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    us_equity_monthly_return = (us_equity_return - lag(us_equity_return)) / lag(us_equity_return) * 100\n  )\n\nequity_df &lt;- equity_df |&gt;\n  rename(\n    us_equity_adjusted = us_equity_return,  # Adjusted close column\n  )\n\n\nequity_df &lt;- equity_df |&gt;\n  mutate(date = date |&gt;\n           floor_date(unit = \"month\") |&gt;  # Get the first of the current month\n           add_with_rollback(months(1)))  # Add one month\n\n\n# Remove Empty Values\nequity_df &lt;- equity_df |&gt;\n  filter(!is.na(us_equity_monthly_return))\n\n# View the updated data frame\nequity_df |&gt; DT:::datatable()\n\n\n\n\n\n\nThe analysis focuses on preparing monthly adjusted equity data for trend and performance evaluation. It includes converting raw data into a structured format, calculating monthly percentage returns, and ensuring clean, consistent data by addressing date formatting and removing missing values.\n\nPlot Adjusted Close Prices\nNow, we will plot clear and visually appealing line chart showing how adjusted close prices for the US equity market have changed over time.\n\n\nCode\nggplot(equity_df, aes(x = date, y = us_equity_adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"US Equity Market Adjusted Close Prices\", x = \"Date\", y = \"Adjusted Close ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of adjusted close prices for the US equity market demonstrates a clear upward trend over the past two decades. Periods of volatility, such as the 2008 financial crisis and the 2020 pandemic, are evident, followed by robust recoveries. This long-term growth reflects the resilience of the equity market and its ability to adapt to economic challenges.\n\n\nPlot Monthly Returns\nlet’s create a line chart showing US Equity Market Monthly Returns (%) over time. The x-axis represents the date, and the y-axis shows the percentage returns.\n\n\nCode\nggplot(equity_df, aes(x = date, y = us_equity_monthly_return)) +\n  geom_line(color = \"red\") +\n  labs(title = \"US Equity Market Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of US equity market monthly returns highlights significant volatility over the past two decades. Periods of extreme fluctuations, such as during the 2008 financial crisis and the 2020 pandemic, are clearly visible. Despite regular variability, the overall market demonstrates resilience with a balance of positive and negative returns over time, underscoring the dynamic nature of equity investments.\n\n\n\nInternational Equity Market total returns\nThis provides a clear picture of historical international equity market trends, essential for comparing performance with other asset classes in financial analyses.\n\n\nCode\n# Request monthly adjusted data for VEU ETF\nintl_equity_response &lt;- request(ALPHA_URL) |&gt;\n  req_options(cookies = NULL) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\", # Monthly adjusted time series\n    symbol = \"VEU\",                           # Vanguard FTSE All-World ex-US ETF\n    apikey = ALPHA_ADVANTAGE_API_KEY\n  ) |&gt;\n  req_perform()\n\n\n\n\nCode\nintl_equity_data &lt;-intl_equity_response |&gt;\n  resp_body_json() %&gt;%\n  .$`Monthly Adjusted Time Series`\n\n\n# Convert JSON data to a data frame\nintl_equity_df &lt;- intl_equity_data |&gt;\n  map_dfr(as_tibble, .id = \"date\") |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    intl_equity_return = as.numeric(`5. adjusted close`) # Adjusted close price\n  ) |&gt;\n  select(date, intl_equity_return)\n\nintl_equity_df &lt;- intl_equity_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    monthly_return = (intl_equity_return - lag(intl_equity_return)) / lag(intl_equity_return) * 100\n  )\n\nintl_equity_df &lt;- intl_equity_df |&gt;\n  rename(\n    intl_equity_adjusted = intl_equity_return,  # Adjusted close column\n    intl_equity_monthly_return = monthly_return # Monthly returns\n  )\n\nintl_equity_df &lt;- intl_equity_df |&gt;\n  mutate(date = date |&gt;\n           floor_date(unit = \"month\") |&gt;  # Get the first of the current month\n           add_with_rollback(months(1)))  # Add one month\n\n# View the updated data frame\nintl_equity_df |&gt; DT::datatable()\n\n\n\n\n\n\nThe data includes adjusted close prices and calculates monthly percentage returns, providing insights into the performance of international equity markets. The structured dataset is prepared for further exploration, allowing for trend analysis and comparison with other markets.\n\n\nVisualize the International Equity Market Data\nPlot Adjusted Close Prices: This helps verify long-term trends and detect anomalies: This graph displays the International Equity Market Adjusted Close Prices over time.\n\n\nCode\nggplot(intl_equity_df, aes(x = date, y = intl_equity_adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"International Equity Market Adjusted Close Prices\", x = \"Date\", y = \"Adjusted Close ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe graph of International Equity Market Adjusted Close Prices highlights the long-term growth and resilience of international markets, despite periods of volatility. . This upward trend highlights the resilience and recovery of international equity markets, reflecting long-term opportunities for investors outside the US.\n\n\nPlot Monthly Returns:\nThis analysis focuses on the monthly returns of the international equity market:\n\n\nCode\nggplot(intl_equity_df, aes(x = date, y = intl_equity_monthly_return)) +\n  geom_line(color = \"red\") +\n  labs(title = \"International Equity Market Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of international equity market monthly returns highlights significant volatility over time, with sharp declines during major global economic crises such as 2008 and 2020. Despite regular fluctuations, the market shows resilience, with periods of recovery and stability, reflecting the dynamic nature of international investments.\n\n\nBond Market Total Returns\nThis analysis retrieves monthly adjusted time series data for the iShares Core US Aggregate Bond ETF (AGG), a key benchmark for the US bond market. By examining this data, we aim to understand trends and performance in the fixed-income market, providing valuable insights into bond market dynamics over time.\n\n\nCode\n# Request monthly adjusted data for AGG ETF\nbond_response &lt;- request(ALPHA_URL) |&gt;\n  req_options(cookies = NULL) |&gt;\n  req_url_query(\n    `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\", # Monthly adjusted time series\n    symbol = \"AGG\", # iShares Core US Aggregate Bond ETF\n    apikey = ALPHA_ADVANTAGE_API_KEY\n  ) |&gt;\n  req_perform()\n\n\nNow, let’s parse the data\n\n\nCode\n# Parse JSON response\nbond_data &lt;- bond_response |&gt;\n  resp_body_json() %&gt;%\n  .$`Monthly Adjusted Time Series`\n\n# Convert JSON data to a data frame\nbond_df &lt;- bond_data |&gt;\n  map_dfr(as_tibble, .id = \"date\") |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    bond_return = as.numeric(`5. adjusted close`) # Adjusted close price\n  ) |&gt;\n  select(date, bond_return)\n\nbond_df &lt;- bond_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    monthly_return = (bond_return - lag(bond_return)) / lag(bond_return) * 100\n  )\n\nbond_df &lt;- bond_df |&gt;\n  rename(\n    bond_adjusted = bond_return,  # Adjusted close column\n    bond_monthly_return = monthly_return # Monthly returns\n  )\n\n\nbond_df &lt;- bond_df |&gt;\n  mutate(date = date |&gt;\n           floor_date(unit = \"month\") |&gt;  # Get the first of the current month\n           add_with_rollback(months(1)))  # Add one month\n\n# View the updated data frame\nbond_df |&gt; DT::datatable()\n\n\n\n\n\n\nThis structured data enables analysis of bond market performance over time, highlighting trends and volatility in the fixed-income market. It serves as a valuable resource for assessing the stability and dynamics of bonds in a broader portfolio context.\n\nAnalyze the data\nThe analysis visualizes the performance of the bond market using the adjusted close prices.\n\n\nCode\nbond_df &lt;- bond_df |&gt;\n  filter(!is.na(bond_monthly_return))\n\nggplot(bond_df, aes(x = date, y = bond_adjusted)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Bond Market Adjusted Close Prices\", x = \"Date\", y = \"Adjusted Close ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization shows steady growth in bond prices over the years, reflecting the bond market’s stability. Periods of decline, such as during economic downturns, are evident but followed by recoveries, emphasizing the resilience of the bond market and its importance as a relatively stable investment option.\n\n\nCode\nggplot(bond_df, aes(x = date, y = bond_monthly_return)) +\n  geom_line(color = \"green\") +\n  labs(title = \"Bond Market Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe bond market’s monthly returns exhibit relative stability compared to equities, with occasional spikes during periods of economic volatility, such as the 2008 financial crisis and the 2020 pandemic. The data underscores bonds’ role as a lower-risk investment, offering consistent returns with minimal drastic fluctuations over time.\n\n\n\nShort Term Debt Returns\nTo fetch Short-Term Debt Returns, we will use the 2-Year US Treasury Yield from the FRED API, which is a widely recognized benchmark for short-term debt.\n\n\nCode\n# Fetch 2-Year Treasury Yield data\nresponse &lt;- request(FRED_URL) |&gt;\n  req_url_query(\n    series_id = \"DGS2\", # 2-Year Treasury Yield\n    api_key = FRED_API_KEY, # Your FRED API key\n    file_type = \"json\", # File format\n    observation_end = Sys.Date() # Today's date\n  ) |&gt;\n  req_perform()\n\n# Parse JSON response\ndebt_data &lt;- response %&gt;%\n  resp_body_json() %&gt;%\n  .$observations\n\n# Convert JSON data to a data frame\ndebt_df &lt;- debt_data |&gt;\n  map_dfr(as_tibble) |&gt;\n  mutate(\n    date = as.Date(date), # Convert date to Date type\n    short_term_yield = as.numeric(value), # Convert yield data to numeric\n    .groups = \"drop\"\n  ) |&gt;\n  select(date, short_term_yield)\n\ndebt_df &lt;- debt_df |&gt;\n  arrange(date) |&gt;\n  mutate(\n    monthly_return = (((1 + short_term_yield/100)^(1/12)) - 1) * 100\n  )\n\n\ndebt_df &lt;- debt_df |&gt;\n  rename(\n    short_term_yield_percent = short_term_yield, # Original yield in %\n    short_term_monthly_return = monthly_return  # Monthly return approximation\n  )\n\n\ndebt_df_monthly &lt;- debt_df |&gt;\n  group_by(\n    year = year(date),\n    month = month(date)\n  ) |&gt;\n  summarise(\n    date = floor_date(first(date), unit = \"month\"),\n    short_term_yield_percent = mean(short_term_yield_percent, na.rm = TRUE),\n    short_term_monthly_return = mean(short_term_monthly_return, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  ungroup() |&gt;\n  arrange(date)\n\ndebt_df &lt;- debt_df_monthly |&gt;\n  select(date, short_term_yield_percent, short_term_monthly_return)\n\n\n\nLet’s analyze the results\nThis analysis focuses on short-term debt returns, an important component of financial planning and investment strategies.\n\n\nCode\n# Plot Short-Term Yields Over Time:\nggplot(debt_df, aes(x = date, y = short_term_yield_percent)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Short-Term Treasury Yield (2-Year)\", x = \"Date\", y = \"Yield (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of the 2-Year Treasury Yield highlights significant fluctuations over time, reflecting changes in economic conditions, monetary policy, and market expectations. Peaks in the yield correspond to periods of high inflation and tight monetary policy, such as in the 1980s.\n\n\n\nPlot Monthly Returns Over Time\n\n\nCode\nggplot(debt_df, aes(x = date, y = short_term_monthly_return)) +\n  geom_line(color = \"green\") +\n  labs(title = \"Short-Term Debt Monthly Returns\", x = \"Date\", y = \"Monthly Returns (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe visualization of short-term debt monthly returns illustrates the impact of economic cycles on returns. Peaks, such as those in the late 1970s and early 1980s, align with periods of high inflation and elevated interest rates. The steady decline and stabilization in subsequent decades reflect accommodative monetary policies and economic recovery phases. Recent upticks highlight the response to changing interest rate environments, emphasizing the sensitivity of short-term debt to macroeconomic conditions.\n\n\nTask 4: Initial Analysis\nAfter you have acquired your input data, perform some basic exploratory data analysis to identify key properties of your data. You may choose to measure the correlation among factors, long-term averages, variances, etc. Your analysis should include at least one table and one figure.\nAs part of your analysis, be sure to compute the long-run monthly average value of each series. You will use these in a later task.\nTo start your analysis, let’s combine the cleaned and processed data from all sources into a single data frame. This will allow us to perform exploratory data analysis (EDA) on the entire dataset.\n\n\nStep 1: Combine Data\nIn this step, we integrated multiple datasets related to economic and financial indicators into a single, unified dataset:\n\n\nCode\n# Ensure all datasets have a common date format and merge them\ncombined_data &lt;- reduce(\n  list(\n    inflation_df |&gt; select(date, inflation_cpi, mom_inflation_rate, yoy_inflation_rate),\n    wage_growth_df |&gt; select(date, wage_growth),\n    equity_df |&gt; select(date, us_equity_monthly_return),\n    intl_equity_df |&gt; select(date, intl_equity_monthly_return),\n    bond_df |&gt; select(date, bond_monthly_return),\n    debt_df |&gt; select(date, short_term_monthly_return)\n  ),\n  full_join,\n  by = \"date\"\n) \n\ncombined_data &lt;- combined_data %&gt;%\n  filter(complete.cases(.))\n\ncombined_data |&gt; DT::datatable()\n\n\n\n\n\n\nThe resulting dataset is a comprehensive, time-aligned dataset containing multiple economic and financial indicators, ready for analysis and comparison.\n\n\nStep 2: Compute Summary Statistics\nIn this step, we calculated summary statistics for the key economic and financial indicators in the combined dataset:\n\n\nCode\nsummary_stats &lt;- combined_data |&gt;\n  summarize(\n    avg_wage_growth = mean(wage_growth, na.rm = TRUE),\n    avg_mom_inflation = mean(mom_inflation_rate, na.rm = TRUE),\n    avg_yoy_inflation = mean(yoy_inflation_rate, na.rm = TRUE),\n    avg_us_equity_return = mean(us_equity_monthly_return, na.rm = TRUE),\n    avg_intl_equity_return = mean(intl_equity_monthly_return, na.rm = TRUE),\n    avg_bond_return = mean(bond_monthly_return, na.rm = TRUE),\n    avg_short_term_return = mean(short_term_monthly_return, na.rm = TRUE)\n  )\n\n# View summary statistics\nsummary_stats |&gt; DT::datatable()\n\n\n\n\n\n\nThese results allow for a comprehensive comparison across indicators, helping to identify relationships between economic trends, market behaviors, and investment performance.\n\n\nStep 3: Analyze Correlations Among Factors\nlet’s analyze the relationships between various financial metrics by computing a correlation matrix.\n\n\nCode\ncorrelation_matrix &lt;- combined_data |&gt;\n  select(-date) |&gt;\n  drop_na() |&gt;  # Remove rows with NA values\n  cor()\n\n# Print the correlation matrix\ncorrelation_matrix |&gt; DT::datatable()\n\n\n\n\n\n\nThis matrix provides a detailed understanding of how indicators relate to one another, revealing patterns and dependencies for deeper analysis.\n\n\nStep 4: Create Figures Time-Series Visualization of Each Variable:\nNow let’s create a cohesive visualization of multiple time series related to economic and market indicators. Rhis analysis provides a clear view of how different economic and market factors have evolved over time, enabling comparative insights and trend analysis.\n\n\nCode\n# Plot individual time series\nplot1 &lt;- ggplot(combined_data, aes(x = date, y = wage_growth)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Wage Growth Over Time\", x = \"Date\", y = \"Wage Growth (%)\")\n\nplot2 &lt;- ggplot(combined_data, aes(x = date, y = mom_inflation_rate)) +\n  geom_line(color = \"red\") +\n  labs(title = \"MoM Inflation Rate Over Time\", x = \"Date\", y = \"Inflation (%)\")\n\nplot3 &lt;- ggplot(combined_data, aes(x = date, y = us_equity_monthly_return)) +\n  geom_line(color = \"green\") +\n  labs(title = \"US Equity Market Returns Over Time\", x = \"Date\", y = \"Monthly Return (%)\")\n\nplot4 &lt;- ggplot(combined_data, aes(x = date, y = intl_equity_monthly_return)) +\n  geom_line(color = \"orange\") +\n  labs(title = \"International Equity Market Returns Over Time\", x = \"Date\", y = \"Monthly Return (%)\")\n\n# Combine the plots\nplot1 + plot2 + plot3 + plot4 + plot_layout(ncol = 2)\n\n\n\n\n\n\n\n\n\nThe visualizations collectively highlight key economic and financial trends over time. Wage growth shows a generally steady pattern with notable disruptions, such as during the pandemic. Inflation rates exhibit fluctuations, with spikes during periods of economic instability. US and international equity markets reflect consistent volatility, underscoring the dynamic nature of investments, while revealing similar patterns of recovery and growth post-crisis. Together, these trends provide a comprehensive view of the interconnectedness of labor, inflation, and financial markets, emphasizing their collective impact on economic stability and performance.\n\n\nCompute Long-Run Monthly Averages\nThis analysis calculates the long-run monthly averages, medians, and standard deviations for key economic and financial series.\n\n\nCode\n# Calculate long-run monthly averages for each series\nlong_run_averages &lt;- tibble(\n  Series = c(\"Wage Growth\", \"MoM Inflation Rate\", \"US Equity Market Returns\", \n             \"International Equity Market Returns\", \"Bond Market Returns\", \n             \"Short-Term Debt Returns\"),\n  Mean = c(\n    mean(wage_growth_df$wage_growth, na.rm = TRUE),\n    mean(inflation_df$mom_inflation_rate, na.rm = TRUE),\n    mean(equity_df$us_equity_monthly_return, na.rm = TRUE),\n    mean(intl_equity_df$intl_equity_monthly_return, na.rm = TRUE),\n    mean(bond_df$bond_monthly_return, na.rm = TRUE),\n    mean(debt_df$short_term_monthly_return, na.rm = TRUE)\n  ),\n  Median = c(\n    median(wage_growth_df$wage_growth, na.rm = TRUE),\n    median(inflation_df$mom_inflation_rate, na.rm = TRUE),\n    median(equity_df$us_equity_monthly_return, na.rm = TRUE),\n    median(intl_equity_df$intl_equity_monthly_return, na.rm = TRUE),\n    median(bond_df$bond_monthly_return, na.rm = TRUE),\n    median(debt_df$short_term_monthly_return, na.rm = TRUE)\n  ),\n  SD = c(\n    sd(wage_growth_df$wage_growth, na.rm = TRUE),\n    sd(inflation_df$mom_inflation_rate, na.rm = TRUE),\n    sd(equity_df$us_equity_monthly_return, na.rm = TRUE),\n    sd(intl_equity_df$intl_equity_monthly_return, na.rm = TRUE),\n    sd(bond_df$bond_monthly_return, na.rm = TRUE),\n    sd(debt_df$short_term_monthly_return, na.rm = TRUE)\n  )\n)\n\n# View the long-run monthly averages\nlong_run_averages |&gt; DT::datatable()\n\n\n\n\n\n\nThe long-run averages, medians, and standard deviations provide a clear picture of the behavior and variability of key economic and financial indicators.\n\n\n\n\n\n\nTask 5: Historical Comparison\n\n\n\nNow that you have acquired data, implement the TRS and ORP formulas above and compare the value of each of them for the first month of retirement. To do this, you may assume that your hypothetical employee:\nJoined CUNY in the first month of the historical data Retired from CUNY at the end of the final month of data You will need to select a starting salary for your employee. Use historical data for wage growth and inflation and assume that the TRS and ORP parameters did not change over time. (That is, the employee contribution “brackets” are not inflation adjusted; the employee will have to make larger contributions as income rises over the span of a 20+ year career.)\n\n\n\n\nStep 1: Simulate Salary Growth\nThis simulation models the growth of a hypothetical salary over time based on observed wage growth rates from the dataset. Starting with an initial salary, the model iteratively applies the monthly wage growth rates to project the salary progression\n\n\nCode\n# Step 1: Set initial salary\nstarting_salary &lt;- 50000  # Hypothetical starting salary in dollars\n\n# Step 2: Initialize salary vector\nn_months &lt;- nrow(combined_data)\nsalary &lt;- numeric(n_months)\nsalary[1] &lt;- starting_salary  # Set initial salary\n\n# Step 3: Calculate salary growth iteratively\nfor (i in 2:n_months) {\n  salary[i] &lt;- salary[i - 1] * (1 + combined_data$wage_growth[i])\n}\n\n# Step 4: Combine results into a data frame\nsalary_simulation &lt;- data.frame(\n  Date = combined_data$date,\n  Salary = salary\n)\n\n# Step 5: Visualize the salary growth\nlibrary(ggplot2)\nggplot(salary_simulation, aes(x = Date, y = Salary)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"Simulated Salary Growth Over Time\",\n    x = \"Date\",\n    y = \"Salary ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe simulated salary growth over time demonstrates a steady upward trajectory, reflecting consistent annual increases in earnings. The notable inflection point aligns with a period of economic disruption, such as the COVID-19 pandemic, showing a temporary impact on salary trends. This chart underscores the resilience of salary growth in the long term despite short-term challenges, providing insights into financial stability and progression over time.\n\n\nStep 2: Calculate TRS Contributions\nFor TRS Contributions, we will simulate the cumulative contributions and retirement value for the hypothetical employee using the following details:\nAssumptions: Employee Contributions:\nAssume the employee contributes a fixed percentage (e.g., 5%) of their monthly salary to TRS. Growth of TRS Fund:\nTRS provides a steady, guaranteed annual return rate (e.g., 4%) on the accumulated contributions. Returns are compounded monthly. Retirement Value:\nCalculate the total TRS fund value at the employee’s retirement based on contributions and compounded growth.\n\n\nCode\n# Step 1: Define TRS parameters\ntrs_contribution_rate &lt;- 0.05  # 5% of salary\ntrs_annual_return &lt;- 0.04      # 4% annual return\ntrs_monthly_return &lt;- trs_annual_return / 12  # Monthly return\n\n# Step 2: Calculate monthly contributions\nsalary_simulation &lt;- salary_simulation |&gt;\n  mutate(\n    Monthly_Contribution = (Salary / 12) * trs_contribution_rate  # Divide annual salary by 12\n  )\n\n# Step 3: Simulate TRS fund growth\nn_months &lt;- nrow(salary_simulation)\ntrs_fund &lt;- numeric(n_months)\ntrs_fund[1] &lt;- salary_simulation$Monthly_Contribution[1]  # Initial contribution\n\nfor (i in 2:n_months) {\n  trs_fund[i] &lt;- trs_fund[i - 1] * (1 + trs_monthly_return) + \n                 salary_simulation$Monthly_Contribution[i]\n}\n\n# Step 4: Add TRS fund values to the dataset\nsalary_simulation &lt;- salary_simulation |&gt;\n  mutate(TRS_Fund_Value = trs_fund)\n\n# Step 5: Visualize TRS fund growth over time\nggplot(salary_simulation, aes(x = Date, y = TRS_Fund_Value)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"TRS Fund Value Over Time\",\n    x = \"Date\",\n    y = \"Cumulative TRS Fund Value ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe chart highlights steady TRS fund growth over time, driven by consistent contributions and compounded returns, illustrating the power of long-term saving.\n\n\nStep 3: Calculate ORP Contributions\nThis analysis simulates the growth of an Optional Retirement Plan (ORP) fund over time, integrating weighted returns from equities, bonds, and short-term debt based on a defined portfolio allocation. It combines salary data and monthly contributions with market performance to project the ORP fund value, offering insights into its cumulative growth.\n\n\nCode\n# Step 1: Define ORP parameters\norp_contribution_rate &lt;- 0.05  # 5% of salary\norp_weights &lt;- c(0.6, 0.3, 0.1)  # Portfolio weights: 60% equity, 30% bonds, 10% short-term debt\n\n# Step 2: Merge returns data with salary data\nreturns_combined &lt;- combined_data |&gt;\n  select(date, us_equity_monthly_return, bond_monthly_return, short_term_monthly_return) |&gt;\n  right_join(salary_simulation, by = c(\"date\" = \"Date\"))\n\n# Step 3: Calculate weighted monthly returns\nreturns_combined &lt;- returns_combined |&gt;\n  mutate(\n    Weighted_Return = orp_weights[1] * us_equity_monthly_return +\n                      orp_weights[2] * bond_monthly_return +\n                      orp_weights[3] * short_term_monthly_return,\n    Monthly_Contribution = (Salary / 12) * orp_contribution_rate\n  )\n\n# Step 4: Simulate ORP fund growth\nn_months &lt;- nrow(returns_combined)\norp_fund &lt;- numeric(n_months)\norp_fund[1] &lt;- returns_combined$Monthly_Contribution[1]  # Initial contribution\n\nfor (i in 2:n_months) {\n  orp_fund[i] &lt;- orp_fund[i - 1] * (1 + returns_combined$Weighted_Return[i] / 100) +\n                 returns_combined$Monthly_Contribution[i]\n}\n\n# Step 5: Add ORP fund values to the dataset\nreturns_combined &lt;- returns_combined |&gt;\n  mutate(ORP_Fund_Value = orp_fund)\n\n# Step 6: Visualize ORP fund growth\nggplot(returns_combined, aes(x = date, y = ORP_Fund_Value)) +\n  geom_line(color = \"red\") +\n  labs(\n    title = \"ORP Fund Value Over Time\",\n    x = \"Date\",\n    y = \"Cumulative ORP Fund Value ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ORP simulation shows consistent growth, emphasizing the benefits of regular contributions and a diversified portfolio for long-term financial stability.\n\n\nStep 4: Inflation Adjustment\nThis section introduces inflation-adjusted analysis to provide a realistic comparison of the TRS and ORP fund values over time.\n\n\nCode\n# Step 1: Normalize CPI data to create a cumulative inflation factor\ninflation_adjustment &lt;- combined_data |&gt;\n  arrange(date) |&gt;\n  mutate(\n    cumulative_inflation_factor = inflation_cpi / inflation_cpi[1]  # Normalize to first month's CPI\n  ) |&gt;\n  select(date, cumulative_inflation_factor)\n\n# Step 2: Merge inflation adjustment data with TRS and ORP datasets\nadjusted_data &lt;- returns_combined |&gt;\n  left_join(inflation_adjustment, by = \"date\") |&gt;\n  mutate(\n    TRS_Inflation_Adjusted = TRS_Fund_Value / cumulative_inflation_factor,\n    ORP_Inflation_Adjusted = ORP_Fund_Value / cumulative_inflation_factor\n  )\n\n# Step 3: Visualize the inflation-adjusted TRS and ORP fund values\nggplot(adjusted_data) +\n  geom_line(aes(x = date, y = TRS_Inflation_Adjusted, color = \"TRS Inflation-Adjusted\")) +\n  geom_line(aes(x = date, y = ORP_Inflation_Adjusted, color = \"ORP Inflation-Adjusted\")) +\n  labs(\n    title = \"Inflation-Adjusted TRS and ORP Fund Values Over Time\",\n    x = \"Date\",\n    y = \"Inflation-Adjusted Fund Value ($)\",\n    color = \"Plan\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ORP plan outperforms TRS in inflation-adjusted terms due to its diversified investments, offering higher growth potential but with increased risk.\n\n\nNext Steps: Calculate Monthly Retirement Payouts\nWe’ll calculate the first-month retirement payouts for both TRS and ORP using their inflation-adjusted final values. Here’s the plan:\nMethodology: TRS Monthly Payout:\nTRS provides a fixed, predictable monthly payout adjusted for inflation. Assume a payout rate (e.g., 5% of the final TRS fund value annually). ORP Monthly Payout:\nORP relies on the employee withdrawing a fixed percentage (e.g., 3.5%) annually, which adjusts for inflation. This safe withdrawal rate minimizes the risk of depleting funds during retirement. Create a Comparison Table:\nSummarize the inflation-adjusted final values for TRS and ORP, and their corresponding monthly payouts.\n\n\nCode\n# Ensure adjusted_data is sorted by date\nadjusted_data &lt;- adjusted_data |&gt; arrange(date)\n\n# Extract the final inflation-adjusted fund values\ntrs_final_value &lt;- tail(adjusted_data$TRS_Inflation_Adjusted, 1)\norp_final_value &lt;- tail(adjusted_data$ORP_Inflation_Adjusted, 1)\n\n\n# Step 2: Define payout rates\ntrs_annual_payout_rate &lt;- 0.05  # 5% annual payout rate\norp_safe_withdrawal_rate &lt;- 0.035  # 3.5% safe withdrawal rate\n\n# Step 3: Calculate monthly payouts\ntrs_monthly_payout &lt;- (trs_final_value * trs_annual_payout_rate) / 12\norp_monthly_payout &lt;- (orp_final_value * orp_safe_withdrawal_rate) / 12\n\n# Step 4: Create a comparison table\ncomparison_table &lt;- data.frame(\n  Plan = c(\"TRS\", \"ORP\"),\n  Final_Inflation_Adjusted_Value = c(trs_final_value, orp_final_value),\n  Monthly_Payout = c(trs_monthly_payout, orp_monthly_payout)\n)\n\n# Step 5: Display the comparison table\nlibrary(DT)\ncomparison_table |&gt; datatable(\n  caption = \"Comparison of Inflation-Adjusted TRS and ORP Final Values and Monthly Payouts\"\n)\n\n\n\n\n\n\nThe comparison reveals that the ORP plan generates a higher final inflation-adjusted fund value compared to TRS, leading to a larger monthly payout. This difference highlights ORP’s greater growth potential due to its diversified investment strategy, while TRS provides a more stable but comparatively lower payout.\n\n\n\n\n\n\nTask 5: Fixed-Rate Analysis\n\n\n\nModify your simulation from the previous section to project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death. (You will need to select an estimated death age.) In order to implement cost-of-living-adjustments (TRS) and future market returns (ORP), you can use the long-run averages you computed previously. This “fixed rate” assumption is rather limiting, but we will address it below.\nAs you compare the plans, be sure to consider:\nWhether the employee runs out of funds before death and/or has funds to leave to heirs (ORP only) Average monthly income (TRS vs ORP) Maximum and minimum gap in monthly income between TRS and ORP As noted above, you can ignore the effect of taxes throughout this analysis.\n\n\n\n\nStep 1: Simulate TRS Income\nWith the assumed death age of 85, we will simulate monthly TRS payouts until the employee’s death. Here’s the detailed approach:\nDetailed Approach for TRS Simulation Set Retirement Parameters:\nRetirement Age: The employee retires at the end of the dataset (e.g., 65 years old). Death Age: 85 years old, which gives a retirement period of 20 years (240 months). Monthly Payout:\nUse the inflation-adjusted final TRS fund value (calculated in Task 5). Apply the fixed annual payout rate (5%), divided by 12 to get monthly payouts. Cost-of-Living Adjustment (COLA):\nUse the long-run average inflation rate (calculated earlier) to adjust monthly payouts for inflation over time. Output:\nCreate a data frame showing the month, year, age, and inflation-adjusted TRS income.\n\n\nCode\n# Step 1: Define Parameters\n\n# 1. Retirement Parameters\nretirement_age &lt;- 65            # Age at retirement\ndeath_age &lt;- 85                 # Assumed death age\nyears_in_retirement &lt;- death_age - retirement_age\nmonths_in_retirement &lt;- years_in_retirement * 12\n\n# 2. Economic Parameters\n\n# Average inflation rate (using combined_data)\naverage_inflation_rate &lt;- mean(combined_data$mom_inflation_rate, na.rm = TRUE) / 100  # Monthly inflation rate\n\n# Cost-of-Living Adjustment (COLA) for TRS\nCOLA &lt;- min(0.03, max(0.01, average_inflation_rate / 2))  # Apply the capped formula\n\n# Portfolio return (weighted by asset allocation)\n# Asset allocation for age 65-74\norp_weights_65_74 &lt;- c(0.34, 0.23, 0.43, 0)  # US Equities, Intl Equities, Bonds, Short-Term Debt\naverage_returns &lt;- c(\n  mean(combined_data$us_equity_monthly_return, na.rm = TRUE) / 100,\n  mean(combined_data$intl_equity_monthly_return, na.rm = TRUE) / 100,\n  mean(combined_data$bond_monthly_return, na.rm = TRUE) / 100,\n  mean(combined_data$short_term_monthly_return, na.rm = TRUE) / 100\n)\nportfolio_return_65_74 &lt;- sum(orp_weights_65_74 * average_returns)\n\n# Asset allocation for age 75-85\norp_weights_75_85 &lt;- c(0.19, 0.13, 0.62, 0.06)  # Adjusted for older age group\nportfolio_return_75_85 &lt;- sum(orp_weights_75_85 * average_returns)\n\n# 3. TRS Parameters\n\n# Final Average Salary (from earlier computation)\nfinal_average_salary &lt;- tail(adjusted_data$TRS_Inflation_Adjusted, 1)  # Replace with actual FAS value if known\n\n\n# Pension calculation (30 years of service assumed for TRS)\nannual_pension &lt;- final_average_salary * 0.48  # 48% of FAS\nmonthly_pension &lt;- annual_pension / 12         # Convert to monthly\n\n# 4. ORP Parameters\n\n# Initial ORP balance (from earlier computation)\ninitial_orp_balance &lt;- tail(adjusted_data$ORP_Inflation_Adjusted, 1)  # Final ORP fund value at retirement\n\n# Withdrawal rate\nannual_withdrawal_rate &lt;- 0.035  # Assume 3.5%\nmonthly_withdrawal_rate &lt;- annual_withdrawal_rate / 12\n\n# Confirm parameter values\ncat(\"Retirement Parameters:\\n\")\n\n\nRetirement Parameters:\n\n\nCode\ncat(\"Retirement Age:\", retirement_age, \"\\n\")\n\n\nRetirement Age: 65 \n\n\nCode\ncat(\"Death Age:\", death_age, \"\\n\")\n\n\nDeath Age: 85 \n\n\nCode\ncat(\"Months in Retirement:\", months_in_retirement, \"\\n\\n\")\n\n\nMonths in Retirement: 240 \n\n\nCode\ncat(\"Economic Parameters:\\n\")\n\n\nEconomic Parameters:\n\n\nCode\ncat(\"Average Inflation Rate:\", average_inflation_rate * 100, \"%\\n\")\n\n\nAverage Inflation Rate: 0.2028477 %\n\n\nCode\ncat(\"COLA (TRS):\", COLA * 100, \"%\\n\")\n\n\nCOLA (TRS): 1 %\n\n\nCode\ncat(\"Portfolio Return (Age 65-74):\", portfolio_return_65_74 * 100, \"%\\n\")\n\n\nPortfolio Return (Age 65-74): 0.5234864 %\n\n\nCode\ncat(\"Portfolio Return (Age 75-85):\", portfolio_return_75_85 * 100, \"%\\n\\n\")\n\n\nPortfolio Return (Age 75-85): 0.3989437 %\n\n\nCode\ncat(\"TRS Parameters:\\n\")\n\n\nTRS Parameters:\n\n\nCode\ncat(\"Final Average Salary (FAS):\", final_average_salary, \"\\n\")\n\n\nFinal Average Salary (FAS): 50576.55 \n\n\nCode\ncat(\"Monthly Pension:\", monthly_pension, \"\\n\\n\")\n\n\nMonthly Pension: 2023.062 \n\n\nCode\ncat(\"ORP Parameters:\\n\")\n\n\nORP Parameters:\n\n\nCode\ncat(\"Initial ORP Balance:\", initial_orp_balance, \"\\n\")\n\n\nInitial ORP Balance: 80270.13 \n\n\nCode\ncat(\"Monthly Withdrawal Rate:\", monthly_withdrawal_rate * 100, \"%\\n\")\n\n\nMonthly Withdrawal Rate: 0.2916667 %\n\n\nThe TRS plan provides stable, inflation-adjusted monthly payouts, while the ORP plan offers higher growth potential but depends on market performance. The choice depends on individual risk tolerance and retirement goals.\n\n\nCode\ntrs_final_value &lt;- tail(adjusted_data$TRS_Inflation_Adjusted, 1)  # Final TRS fund value\ntrs_annual_payout_rate &lt;- 0.05                                   # Annual payout rate\nmonthly_payout &lt;- trs_final_value * trs_annual_payout_rate / 12  # Fixed monthly payout\naverage_inflation_rate &lt;- mean(combined_data$mom_inflation_rate, na.rm = TRUE) / 100  # Average monthly inflation\n\n# Step 2: Simulate TRS payouts with COLA\ntrs_simulation &lt;- data.frame(\n  Month = 1:months_in_retirement,\n  Year = rep(retirement_age:(retirement_age + years_in_retirement - 1), each = 12),\n  Age = retirement_age + (1:months_in_retirement) / 12\n) |&gt;\n  mutate(\n    Inflation_Adjusted_Payout = monthly_payout * (1 + average_inflation_rate)^(Month - 1)\n  )\n\n# Step 3: Visualize TRS payouts over time\nlibrary(ggplot2)\nggplot(trs_simulation, aes(x = Month, y = Inflation_Adjusted_Payout)) +\n  geom_line(color = \"blue\") +\n  labs(\n    title = \"TRS Monthly Income with Cost-of-Living Adjustment (COLA)\",\n    x = \"Month in Retirement\",\n    y = \"Inflation-Adjusted Monthly Income ($)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe TRS plan provides inflation-adjusted monthly income, ensuring purchasing power is maintained throughout retirement. This stability makes it a dependable option for retirees prioritizing predictable and steady financial support over time.\n\n\nStep 2: Simulate ORP Income\nFor ORP, we will simulate monthly withdrawals and market-driven growth over the retirement period. This involves modeling fund balance changes due to withdrawals and portfolio returns. Here’s the detailed plan:\nDetailed Approach for ORP Simulation Set Retirement Parameters:\nSame as TRS: retirement age = 65, death age = 85 (20 years or 240 months of retirement). Portfolio Returns:\nUse the long-run averages of equity, bonds, and short-term debt returns (calculated earlier). Apply portfolio weights: 60% equity, 30% bonds, 10% short-term debt. Monthly Withdrawals:\nUse a fixed annual withdrawal rate (e.g., 3.5%), divided by 12 for monthly withdrawals. Adjust withdrawals for inflation using the same COLA mechanism as TRS. Simulate Fund Growth and Depletion:\nEach month, subtract the withdrawal and apply portfolio returns to the remaining balance. Track the fund balance to determine if the ORP account runs out before death. Output:\nA data frame with month, year, age, withdrawal amount, fund balance, and inflation-adjusted income. A plot showing fund depletion and monthly payouts over time.\n\n\nCode\n# Step 1: Parameters\norp_final_value &lt;- tail(adjusted_data$ORP_Inflation_Adjusted, 1)  # Final ORP fund value\norp_annual_withdrawal_rate &lt;- 0.04                              # Annual withdrawal rate\nmonthly_withdrawal_rate &lt;- orp_annual_withdrawal_rate / 12\naverage_portfolio_return &lt;- mean(returns_combined$Weighted_Return, na.rm = TRUE) / 100\n\n\n# Portfolio return\nportfolio_weights &lt;- c(0.6, 0.3, 0.1)                             # Equity, bonds, short-term debt\naverage_inflation_rate &lt;- mean(combined_data$mom_inflation_rate, na.rm = TRUE) / 100\n\n# Step 2: Initialize ORP simulation\norp_simulation &lt;- data.frame(\n  Month = 1:months_in_retirement,\n  Year = rep(retirement_age:(retirement_age + years_in_retirement - 1), each = 12),\n  Age = retirement_age + (1:months_in_retirement) / 12\n) |&gt;\n  mutate(\n    Inflation_Adjusted_Withdrawal = 0,\n    ORP_Balance = orp_final_value\n  )\n\n\nfor (i in 1:nrow(orp_simulation)) {\n  # Determine the portfolio return based on age\n  portfolio_return &lt;- ifelse(\n    orp_simulation$Age[i] &lt; 75, \n    portfolio_return_65_74, \n    portfolio_return_75_85\n  )\n  \n  if (i == 1) {\n    # Initial withdrawal and balance\n    orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- \n      orp_simulation$ORP_Balance[i] * monthly_withdrawal_rate\n    \n    orp_simulation$ORP_Balance[i] &lt;- \n      orp_simulation$ORP_Balance[i] - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n  } else {\n    # Inflation-adjusted withdrawal\n    orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- \n      orp_simulation$Inflation_Adjusted_Withdrawal[i - 1] * (1 + average_inflation_rate)\n    \n    # Portfolio growth\n    portfolio_growth &lt;- \n      orp_simulation$ORP_Balance[i - 1] * portfolio_return\n    \n    # Update balance\n    new_balance &lt;- \n      orp_simulation$ORP_Balance[i - 1] + portfolio_growth - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    \n    # Prevent negative balances\n    orp_simulation$ORP_Balance[i] &lt;- max(new_balance, 0)\n  }\n}\n\nggplot(orp_simulation, aes(x = Month, y = Inflation_Adjusted_Withdrawal)) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(\n    title = \"Monthly Inflation-Adjusted Withdrawal Over Time\",\n    x = \"Month in Retirement\",\n    y = \"Inflation-Adjusted Monthly Withdrawal ($)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe chart displays the inflation-adjusted monthly withdrawals from the ORP fund over retirement, ensuring the withdrawals maintain their real purchasing power. The upward trend reflects the compounding effect of inflation, demonstrating the importance of portfolio growth to sustain income levels in real terms throughout retirement.\n\n\nCode\n# Step 4: Visualize ORP fund balance over time\nggplot(orp_simulation, aes(x = Month, y = ORP_Balance)) +\n  geom_line(color = \"blue\", size = 1) +\n  labs(\n    title = \"ORP Balance Over Time\",\n    x = \"Month in Retirement\",\n    y = \"ORP Fund Value\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe chart illustrates the ORP (Optional Retirement Plan) fund balance over the retirement period. It shows an initial growth phase as the fund benefits from portfolio returns exceeding withdrawals, followed by a plateau and eventual decline as withdrawals increase with inflation, outpacing returns. This highlights the importance of effective portfolio management to sustain fund longevity.\n\n\n1. Does the Employee Run Out of Funds Before Death (ORP Only)?\nObjective: Check if the ORP fund balance (ORP_Balance) becomes zero before the retirement period ends (death age).\nSteps: Inspect the ORP_Balance column in the orp_simulation dataframe. Identify the first instance (if any) where the balance drops to zero. If the balance is still positive at the last retirement month, the employee does not run out of funds. Code:\n\n\nCode\nlast_balance &lt;- tail(orp_simulation$ORP_Balance, 1)\nif (last_balance &gt; 0) {\n  message(\"Employee has funds left: \", round(last_balance, 2), \" at the end of retirement.\")\n} else {\n  message(\"Employee runs out of funds at month: \", which(orp_simulation$ORP_Balance == 0)[1])\n}\n\n\nEmployee has funds left: 101875.34 at the end of retirement.\n\n\nThe analysis reveals that the ORP fund remains solvent throughout the retirement period, ending with a positive balance of $101,875.34\n\n\n2. Average Monthly Income (TRS vs ORP)\nObjective: Calculate the average monthly income for both plans during the retirement period.\nSteps:\nCompute the average of the Monthly_Income column in the trs_simulation and orp_simulation dataframes. Also, Compare the averages.\n\n\nCode\navg_trs_income &lt;- mean(trs_simulation$Inflation_Adjusted_Payout, na.rm = TRUE)\navg_orp_income &lt;- mean(orp_simulation$Inflation_Adjusted_Withdrawal, na.rm = TRUE)\nmessage(\"Average TRS Income: \", round(avg_trs_income, 2))\n\n\nAverage TRS Income: 271.13\n\n\nCode\nmessage(\"Average ORP Income: \", round(avg_orp_income, 2))\n\n\nAverage ORP Income: 344.25\n\n\nThe ORP offers a higher average monthly income ($344.25) compared to TRS ($271.13), highlighting its potential to provide better financial support during retirement. However, the choice between the two plans should consider individual risk tolerance and the importance of guaranteed income.\n\n\n3. Maximum and Minimum Gap in Monthly Income Between TRS and ORP\nObjective: Identify the largest and smallest differences in monthly income between the two plans.\n\n\nCode\nincome_gap &lt;- trs_simulation$Inflation_Adjusted_Payout - orp_simulation$Inflation_Adjusted_Withdrawal\nmax_gap &lt;- max(income_gap, na.rm = TRUE)\nmin_gap &lt;- min(income_gap, na.rm = TRUE)\nmessage(\"Maximum gap in monthly income: \", round(max_gap, 2))\n\n\nMaximum gap in monthly income: -56.83\n\n\nCode\nmessage(\"Minimum gap in monthly income: \", round(min_gap, 2))\n\n\nMinimum gap in monthly income: -92.24\n\n\nThe analysis shows that ORP consistently outperforms TRS in monthly payouts, with the largest gap being -56.83 and the smallest gap -92.24, both favoring ORP. This highlights ORP’s stronger income generation, though TRS may appeal to those seeking stability over higher returns.\n\n\n\n\n\n\nTask 7: Monte Carlo Analysis\n\n\n\nUsing your historical data, generate several (at least 200) “bootstrap histories” suitable for a Monte Carlo analysis. Use bootstrap sampling, i.e. sampling with replacement, to generate values for both the “while working” and “while retired” periods of the model; you do not need to assume constant long-term average values for the retirement predictions any more.\nApply your calculations from the previous two tasks to each of your simulated bootstrap histories. Compare the distribution of TRS and ORP benefits that these histories generate. You may want to ask questions like the following:\nWhat is the probability that an ORP employee exhausts their savings before death? What is the probability that an ORP employee has a higher monthly income in retirement than a TRS employee? Is the 4% withdrawal rate actually a good idea or would you recommend a different withdrawal rate? Report your findings to these or other questions of interest in tables or figures, as appropriate.\n\n\n\n\nStep 1: Generate Bootstrap Histories\nHigh-Level Approach: Bootstrap Sampling: Use the historical data for portfolio returns and inflation rates. Generate 200+ bootstrap samples by sampling these datasets with replacement. Separate the Periods: While Working (Before Retirement): Use historical inflation and wage growth to simulate salary progression. While Retired (After Retirement): Use historical portfolio returns and inflation rates to project fund balances and withdrawals. Store Bootstrap Histories: Create datasets for TRS and ORP simulations for each bootstrap sample. Implementation Steps: Create a loop to generate 200+ bootstrap samples. Use the sample function in R with replace = TRUE for bootstrap sampling. Separate data into working and retirement periods for both TRS and ORP. Define portfolio allocations by age group\n\n\nCode\n# Define portfolio allocations by age group\nportfolio_weights &lt;- function(age) {\n  if (age &lt;= 49) {\n    return(c(0.54, 0.36, 0.10, 0))  # Age 25-49\n  } else if (age &lt;= 59) {\n    return(c(0.47, 0.32, 0.21, 0))  # Age 50-59\n  } else if (age &lt;= 74) {\n    return(c(0.34, 0.23, 0.43, 0))  # Age 60-74\n  } else {\n    return(c(0.19, 0.13, 0.62, 0.06))  # Age 75+\n  }\n}\n\n# Simulate an ORP employee's age progression during the dataset period\nstart_age &lt;- 25  # Starting age\ncombined_data$age &lt;- start_age + (1:nrow(combined_data)) / 12\n\n# Compute weighted returns for each row\ncombined_data$Weighted_Return &lt;- mapply(\n  function(us, intl, bond, short_term, age) {\n    weights &lt;- portfolio_weights(age)\n    return(\n      weights[1] * us + \n      weights[2] * intl + \n      weights[3] * bond + \n      weights[4] * short_term\n    )\n  },\n  combined_data$us_equity_monthly_return,\n  combined_data$intl_equity_monthly_return,\n  combined_data$bond_monthly_return,\n  combined_data$short_term_monthly_return,\n  combined_data$age\n)\n\n# Inspect the updated combined_data\ncombined_data |&gt; DT::datatable()\n\n\n\n\n\n\nAge-specific portfolio adjustments show how investment strategies shift from growth-focused equities in younger years to safer assets like bonds and short-term debt as retirement approaches, optimizing returns and stability over time . Generate Bootstrap Histories Each bootstrap sample represents a potential scenario over a working career and retirement.\n\n\nCode\n# Step 1: Generate Bootstrap Histories\nset.seed(42)  # For reproducibility\n\n# Number of bootstrap samples\nn_samples &lt;- 200  \n\n# Bootstrap sampling\nbootstrap_histories &lt;- lapply(1:n_samples, function(i) {\n  list(\n    # While working: Inflation and wage growth\n    working_inflation = sample(combined_data$mom_inflation_rate, \n                               size = length(combined_data$mom_inflation_rate), replace = TRUE),\n    wage_growth = sample(combined_data$wage_growth, \n                         size = length(combined_data$wage_growth), replace = TRUE),\n    \n    # While retired: Portfolio returns and inflation\n    retirement_portfolio_returns = sample(combined_data$Weighted_Return, \n                                          size = months_in_retirement, replace = TRUE),\n    retirement_inflation = sample(combined_data$mom_inflation_rate, \n                                  size = months_in_retirement, replace = TRUE)\n  )\n})\n\n# Inspect one bootstrap history\n\nstr(bootstrap_histories[[1]])\n\n\nList of 4\n $ working_inflation           : num [1:209] 0 0.27 0.131 0.196 0.231 ...\n $ wage_growth                 : num [1:209] 0.00131 0.00256 0.00309 0.00528 0.00323 ...\n $ retirement_portfolio_returns: num [1:240] 5.2025 -4.0314 2.3266 1.6001 -0.0966 ...\n $ retirement_inflation        : num [1:240] 0.154 0.315 0.248 0.11 0.364 ...\n\n\nThe ORP plan offers higher average monthly income and maintains a surplus at the end of retirement, providing more flexibility and growth potential. TRS provides predictable income with annual cost-of-living adjustments, offering stability but lower overall returns compared to ORP. Bootstrap simulations reveal variability, underscoring the importance of balancing risk and income stability in retirement planning.\nTRS calculations To the bootstrap histories, we’ll use the formulas and methods we’ve already established for calculating TRS payouts, modified to account for each bootstrap history’s unique inflation rates and wage growth. Here’s the step-by-step approach:\n\n\nCode\n# Define TRS parameters\nyears_worked &lt;- 17.5\nmonths_in_retirement &lt;- 240\n\ntrs_simulations &lt;- lapply(bootstrap_histories, function(history) {\n  # Extract working wage growth and inflation during retirement\n  wage_growth &lt;- history$wage_growth\n  retirement_inflation &lt;- history$retirement_inflation\n  \n  # Calculate FAS (average of last 36 months of wages)\n  wages &lt;- cumprod(1 + wage_growth) * salary  # Assume starting salary of $50,000\n  FAS &lt;- mean(tail(wages, 36))\n  \n  # Calculate initial monthly pension\n  initial_monthly_pension &lt;- (FAS * 0.02 * years_worked) / 12\n  \n  # Simulate TRS payouts\n  trs_simulation &lt;- data.frame(\n    Month = 1:months_in_retirement,\n    Inflation_Adjusted_Payout = rep(0, months_in_retirement)\n  )\n  \n  trs_simulation$Inflation_Adjusted_Payout[1] &lt;- initial_monthly_pension\n  \n  # Adjust payouts annually in September\n  for (i in 2:months_in_retirement) {\n    trs_simulation$Inflation_Adjusted_Payout[i] &lt;- trs_simulation$Inflation_Adjusted_Payout[i - 1]\n    if (i %% 12 == 9 && i &gt;= 12) {  # Every September starting from Month 9\n      start_index &lt;- max(1, i - 12 + 1)  # Ensure no negative indices\n      annual_inflation &lt;- sum(retirement_inflation[start_index:i], na.rm = TRUE)\n      COLA &lt;- max(0.01, min(0.03, round(0.5 * annual_inflation, 3)))\n      trs_simulation$Inflation_Adjusted_Payout[i] &lt;- trs_simulation$Inflation_Adjusted_Payout[i] * (1 + COLA)\n    }\n  }\n  \n  return(trs_simulation)\n})\n\n\n# Example: View one simulation result\nhead(trs_simulations[[1]])\n\n\n  Month Inflation_Adjusted_Payout\n1     1                  3696.167\n2     2                  3696.167\n3     3                  3696.167\n4     4                  3696.167\n5     5                  3696.167\n6     6                  3696.167\n\n\n\n\nStep 3: Apply ORP Calculations to Bootstrap Histories\nJust like we did for the TRS, we will now simulate the ORP withdrawals based on each of the generated bootstrap histories. This will include sampling the monthly returns and withdrawals based on the bootstrap data.\n\n\nCode\n# Define ORP parameters\norp_final_value &lt;- 80270.13  # Initial balance from data\nmonthly_withdrawal_rate &lt;- 0.2916667 / 100  # Monthly withdrawal rate (4% annual rate / 12 months)\nworking_years_return &lt;- 0.005234864  # Portfolio return for ages 65-74\nretirement_years_return &lt;- 0.003989437  # Portfolio return for ages 75-85\n\norp_simulations &lt;- lapply(bootstrap_histories, function(history) {\n  \n  # Extract historical returns and inflation data\n  working_inflation &lt;- history$working_inflation\n  retirement_inflation &lt;- history$retirement_inflation\n  portfolio_returns &lt;- history$retirement_portfolio_returns\n  \n  # Set initial ORP balance\n  orp_balance &lt;- orp_final_value  # Starting ORP balance\n  \n  # Create an empty data frame to store the simulation results\n  orp_simulation &lt;- data.frame(\n    Month = 1:months_in_retirement,\n    Year = rep(retirement_age:(retirement_age + years_in_retirement - 1), each = 12),\n    Age = retirement_age + (1:months_in_retirement) / 12,\n    Inflation_Adjusted_Withdrawal = 0,\n    ORP_Balance = orp_balance\n  )\n  \n  # Calculate initial withdrawal\n  orp_simulation$Inflation_Adjusted_Withdrawal[1] &lt;- orp_balance * monthly_withdrawal_rate\n  orp_simulation$ORP_Balance[1] &lt;- orp_balance - orp_simulation$Inflation_Adjusted_Withdrawal[1]\n  \n  # Apply portfolio returns and withdrawals for each month\n  for (i in 2:months_in_retirement) {\n    # Apply inflation adjustment at the start of each year (January)\n    if (i %% 12 == 1) {  # Check if it's January (every 12th month)\n      inflation_rate &lt;- retirement_inflation[i - 1]  # Using the previous year's inflation rate\n      orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- orp_simulation$Inflation_Adjusted_Withdrawal[i - 1] * (1 + inflation_rate)\n    } else {\n      # For other months, keep the withdrawal the same as the previous month\n      orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- orp_simulation$Inflation_Adjusted_Withdrawal[i - 1]\n    }\n    \n    # Apply portfolio return depending on age range\n    if (orp_simulation$Age[i] &lt;= 74) {\n      # For ages 65–74, use working years return\n      orp_simulation$ORP_Balance[i] &lt;- orp_simulation$ORP_Balance[i - 1] * (1 + working_years_return) - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    } else {\n      # For ages 75+, use retirement years return\n      orp_simulation$ORP_Balance[i] &lt;- orp_simulation$ORP_Balance[i - 1] * (1 + retirement_years_return) - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    }\n    \n    # Ensure balance does not go negative\n    orp_simulation$ORP_Balance[i] &lt;- max(orp_simulation$ORP_Balance[i], 0)\n  }\n  \n  return(orp_simulation)\n})\n\n# Example: View one simulation result\ntail(orp_simulations[[1]])\n\n\n    Month Year      Age Inflation_Adjusted_Withdrawal ORP_Balance\n235   235   84 84.58333                      3482.465           0\n236   236   84 84.66667                      3482.465           0\n237   237   84 84.75000                      3482.465           0\n238   238   84 84.83333                      3482.465           0\n239   239   84 84.91667                      3482.465           0\n240   240   84 85.00000                      3482.465           0\n\n\nThe ORP simulations model inflation-adjusted withdrawals and portfolio growth, ensuring purchasing power and fund sustainability throughout retirement. Returns vary by age, reflecting higher growth early and stability later.\n\n\nCode\n# Visualize ORP Fund Balance and Monthly Income Over Time\nggplot(orp_simulation, aes(x = Month)) +\n  geom_line(aes(y = Inflation_Adjusted_Withdrawal, color = \"Monthly Income\")) +\n  labs(\n    title = \"ORP Fund Balance and Monthly Income Over Time\",\n    x = \"Month in Retirement\",\n    y = \"Value ($)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ORP Fund simulation illustrates steady monthly income growth over retirement, reflecting inflation adjustments and portfolio returns, ensuring sustainability and rising purchasing power over time.\n\n\nAnalyze the Probability of ORP Exhaustion\nOne of the key questions is whether the ORP balance runs out before the employee passes away.\nWe can check for each simulation if the ORP balance reaches zero before the employee’s death age (age 85). This will allow us to calculate the probability that the ORP account is exhausted before death. Code for ORP Exhaustion Probability:\n\n\nCode\n# Calculate the probability of ORP exhaustion before death\norp_exhausted &lt;- sapply(orp_simulations, function(sim) {\n  # Check if the balance goes to zero before death\n  min_balance &lt;- min(sim$ORP_Balance)\n  if (min_balance == 0) {\n    return(TRUE)  # ORP exhausted before death\n  } else {\n    return(FALSE)\n  }\n})\n\n# Probability of ORP exhaustion\nprob_orp_exhaustion &lt;- mean(orp_exhausted)\nmessage(\"Probability of ORP Exhaustion before Death: \", round(prob_orp_exhaustion, 4))\n\n\nProbability of ORP Exhaustion before Death: 0.75\n\n\nThe simulation reveals a 75% probability of ORP fund exhaustion before death, emphasizing the need for cautious withdrawal strategies or adjustments to portfolio allocations to ensure financial stability throughout retirement.\n\n\nStep: Compare Monthly Income Between TRS and ORP\nNext, we will compare the monthly income between the TRS and ORP plans. We will calculate the average monthly income from the TRS simulation and ORP simulation and compare them.\nCode for Comparing Average Monthly Income:\n\n\nCode\n# Calculate average monthly income for TRS\navg_trs_income &lt;- mean(trs_simulation$Inflation_Adjusted_Payout, na.rm = TRUE)\n\n# Calculate average monthly income for ORP\navg_orp_income &lt;- mean(orp_simulation$Inflation_Adjusted_Withdrawal, na.rm = TRUE)\n\n# Output the results\nmessage(\"Average TRS Income: \", round(avg_trs_income, 2))\n\n\nAverage TRS Income: 271.13\n\n\nCode\nmessage(\"Average ORP Income: \", round(avg_orp_income, 2))\n\n\nAverage ORP Income: 344.25\n\n\nThe calculation of the average monthly income for both TRS and ORP yielded an average of 271.13 for TRS and 344.25 for ORP. These results highlight that ORP provides a higher average income compared to TRS, which may be important when considering retirement planning and financial security in later years.\n\n\nStep 4.3: Investigate the Gap Between TRS and ORP Monthly Income\nNext, we can investigate the maximum and minimum gap between the monthly income for TRS and ORP over the simulation period.\nWe can calculate the difference in monthly income for each month and then find the maximum and minimum gaps.\nCode for Maximum and Minimum Gap:\n\n\nCode\n# Calculate the gap between TRS and ORP monthly income\nincome_gap &lt;- trs_simulation$Inflation_Adjusted_Payout - orp_simulation$Inflation_Adjusted_Withdrawal\n\n# Maximum and minimum gap\nmax_gap &lt;- max(income_gap, na.rm = TRUE)\nmin_gap &lt;- min(income_gap, na.rm = TRUE)\n\n# Output the results\nmessage(\"Maximum Monthly Income Gap (TRS - ORP): \", round(max_gap, 2))\n\n\nMaximum Monthly Income Gap (TRS - ORP): -56.83\n\n\nCode\nmessage(\"Minimum Monthly Income Gap (TRS - ORP): \", round(min_gap, 2))\n\n\nMinimum Monthly Income Gap (TRS - ORP): -92.24\n\n\nThe analysis reveals that the ORP consistently provides a higher monthly income than the TRS, with a maximum gap of -56.83 and a minimum gap of -92.24, indicating a more substantial income from the ORP throughout retirement\n\n\nStep 4.4: Evaluate the 4% Withdrawal Rate\nNow, we can examine the 4% withdrawal rate for the ORP simulation and see how it affects the ORP balance over time.\nWe can run a sensitivity analysis to check if a different withdrawal rate (e.g., 3.5% or 5%) would make a significant difference in the long-term sustainability of the ORP balance.\nCode for Sensitivity Analysis:\n\n\nCode\n# Define alternative withdrawal rates (e.g., 3.5% and 5%)\nwithdrawal_rates &lt;- c(0.035 / 12, 0.04 / 12, 0.05 / 12)  # Monthly withdrawal rates\n\n# Simulate ORP for each withdrawal rate and check for exhaustion before death\nresults &lt;- lapply(withdrawal_rates, function(rate) {\n  orp_simulation_temp &lt;- orp_simulation\n  orp_simulation_temp$Inflation_Adjusted_Withdrawal &lt;- orp_simulation_temp$ORP_Balance * rate\n  \n  # Recalculate ORP balance after adjusting withdrawals\n  for (i in 2:months_in_retirement) {\n    orp_simulation_temp$ORP_Balance[i] &lt;- orp_simulation_temp$ORP_Balance[i - 1] * (1 + retirement_years_return) - orp_simulation_temp$Inflation_Adjusted_Withdrawal[i]\n    orp_simulation_temp$ORP_Balance[i] &lt;- max(orp_simulation_temp$ORP_Balance[i], 0)\n  }\n  \n  # Check if ORP runs out of funds before death\n  min_balance &lt;- min(orp_simulation_temp$ORP_Balance)\n  exhausted &lt;- min_balance == 0\n  \n  return(list(final_balance = min_balance, exhausted = exhausted))\n})\n\n\n# Extract the results from the list and create a data frame\nresults_df &lt;- do.call(rbind, lapply(results, function(x) {\n  data.frame(\n    Final_Balance = x$final_balance,\n    Exhausted = ifelse(x$exhausted, \"Yes\", \"No\")\n  )\n}))\n\n# Display the results in a nice format\nprint(results_df)\n\n\n  Final_Balance Exhausted\n1      80002.56        No\n2      80002.56        No\n3      51579.90        No\n\n\nThis indicates that the ORP fund remains viable under these withdrawal rates, with sufficient balance to cover withdrawals throughout retirement.\n\n\nCount the number of simulations where ORP was exhausted\nThis analysis simulates different withdrawal rates for an ORP (Optional Retirement Plan) to assess whether the ORP balance gets exhausted before death.\n\n\nCode\ntable(results_df$Exhausted)\n\n\n\nNo \n 3 \n\n\nCode\n# Visualize the results with a bar chart\nggplot(results_df, aes(x = Exhausted)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Number of Simulations with ORP Exhaustion\", x = \"Exhausted\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe bar chart indicates that none of the simulations with varying withdrawal rates resulted in the exhaustion of the ORP balance before the retirement period ended. All simulations show a “No” outcome for ORP exhaustion, meaning the fund remains sufficient throughout the retirement period under the tested conditions.\n\n\nVisualize the income comparison between TRS and ORP\nThe following analysis compares the monthly income distributions between the TRS (Teachers’ Retirement System) and ORP (Optional Retirement Plan) using the inflation-adjusted monthly payouts.\n\n\nCode\nlibrary(ggplot2)\n\n# Create a data frame to store the results\nincome_comparison &lt;- data.frame(\n  Plan = rep(c(\"TRS\", \"ORP\"), each = length(trs_simulation$Inflation_Adjusted_Payout)),\n  Income = c(trs_simulation$Inflation_Adjusted_Payout, orp_simulation$Inflation_Adjusted_Withdrawal)\n)\n\n# Plot the distribution\nggplot(income_comparison, aes(x = Income, fill = Plan)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Comparison of TRS and ORP Monthly Income\", x = \"Monthly Income ($)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe comparison of TRS and ORP monthly income distributions shows that TRS provides more stable income, with consistent inflation adjustments, while ORP income varies more due to portfolio returns. The density plot illustrates the wider range of ORP incomes compared to TRS, highlighting the impact of returns and inflation on retirement sustainability.\n\n\nKey points:\n\n\n\n\n\n\nKey points:\n\n\n\n\nSimulated Salary Growth: Your salary simulation incorporated wage growth and inflation, showing how salaries evolved over time.\nTRS Fund Simulation: The TRS fund grew based on wage growth and inflation adjustments, with annual inflation-adjusted payouts throughout the retirement period.\nORP Fund Simulation: The ORP simulation included withdrawals based on portfolio returns, adjusted for inflation yearly, and tracked the balance during retirement.\nComparison (TRS vs ORP): TRS provided more stable income over time, while ORP offered higher income but with more variability.\nExhaustion Probability: A significant portion of ORP simulations showed that the fund did not exhaust before death, indicating long-term sustainability.\nIncome Gap: The gap between TRS and ORP monthly income varied, with ORP generally offering higher payouts.\n\n7.Income Distribution: ORP income showed more variability in comparison to the more predictable and stable income from TRS."
  },
  {
    "objectID": "mp04.html#task-7",
    "href": "mp04.html#task-7",
    "title": "Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Task 7",
    "text": "Task 7\nStep 1: Generate Bootstrap Histories High-Level Approach: Bootstrap Sampling: Use the historical data for portfolio returns and inflation rates. Generate 200+ bootstrap samples by sampling these datasets with replacement. Separate the Periods: While Working (Before Retirement): Use historical inflation and wage growth to simulate salary progression. While Retired (After Retirement): Use historical portfolio returns and inflation rates to project fund balances and withdrawals. Store Bootstrap Histories: Create datasets for TRS and ORP simulations for each bootstrap sample. Implementation Steps: Create a loop to generate 200+ bootstrap samples. Use the sample function in R with replace = TRUE for bootstrap sampling. Separate data into working and retirement periods for both TRS and ORP.\n\n\nCode\n# Define portfolio allocations by age group\nportfolio_weights &lt;- function(age) {\n  if (age &lt;= 49) {\n    return(c(0.54, 0.36, 0.10, 0))  # Age 25-49\n  } else if (age &lt;= 59) {\n    return(c(0.47, 0.32, 0.21, 0))  # Age 50-59\n  } else if (age &lt;= 74) {\n    return(c(0.34, 0.23, 0.43, 0))  # Age 60-74\n  } else {\n    return(c(0.19, 0.13, 0.62, 0.06))  # Age 75+\n  }\n}\n\n# Simulate an ORP employee's age progression during the dataset period\nstart_age &lt;- 25  # Starting age\ncombined_data$age &lt;- start_age + (1:nrow(combined_data)) / 12\n\n# Compute weighted returns for each row\ncombined_data$Weighted_Return &lt;- mapply(\n  function(us, intl, bond, short_term, age) {\n    weights &lt;- portfolio_weights(age)\n    return(\n      weights[1] * us + \n      weights[2] * intl + \n      weights[3] * bond + \n      weights[4] * short_term\n    )\n  },\n  combined_data$us_equity_monthly_return,\n  combined_data$intl_equity_monthly_return,\n  combined_data$bond_monthly_return,\n  combined_data$short_term_monthly_return,\n  combined_data$age\n)\n\n# Inspect the updated combined_data\nhead(combined_data)\n\n\n# A tibble: 6 × 11\n  date       inflation_cpi mom_inflation_rate yoy_inflation_rate wage_growth\n  &lt;date&gt;             &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;       &lt;dbl&gt;\n1 2007-06-01          207.             0.232                2.69    0.00528 \n2 2007-07-01          208.             0.178                2.32    0       \n3 2007-08-01          208.             0.0308               1.90    0.00239 \n4 2007-09-01          209.             0.424                2.83    0.00190 \n5 2007-10-01          209.             0.308                3.61    0.000951\n6 2007-11-01          211.             0.786                4.37    0.00285 \n# ℹ 6 more variables: us_equity_monthly_return &lt;dbl&gt;,\n#   intl_equity_monthly_return &lt;dbl&gt;, bond_monthly_return &lt;dbl&gt;,\n#   short_term_monthly_return &lt;dbl&gt;, age &lt;dbl&gt;, Weighted_Return &lt;dbl&gt;\n\n\n\n\nCode\n# Step 1: Generate Bootstrap Histories\nset.seed(42)  # For reproducibility\n\n# Number of bootstrap samples\nn_samples &lt;- 200  \n\n# Bootstrap sampling\nbootstrap_histories &lt;- lapply(1:n_samples, function(i) {\n  list(\n    # While working: Inflation and wage growth\n    working_inflation = sample(combined_data$mom_inflation_rate, \n                               size = length(combined_data$mom_inflation_rate), replace = TRUE),\n    wage_growth = sample(combined_data$wage_growth, \n                         size = length(combined_data$wage_growth), replace = TRUE),\n    \n    # While retired: Portfolio returns and inflation\n    retirement_portfolio_returns = sample(combined_data$Weighted_Return, \n                                          size = months_in_retirement, replace = TRUE),\n    retirement_inflation = sample(combined_data$mom_inflation_rate, \n                                  size = months_in_retirement, replace = TRUE)\n  )\n})\n\n# Inspect one bootstrap history\nstr(bootstrap_histories[[1]])\n\n\nList of 4\n $ working_inflation           : num [1:209] 0 0.27 0.131 0.196 0.231 ...\n $ wage_growth                 : num [1:209] 0.00131 0.00256 0.00309 0.00528 0.00323 ...\n $ retirement_portfolio_returns: num [1:240] 5.2025 -4.0314 2.3266 1.6001 -0.0966 ...\n $ retirement_inflation        : num [1:240] 0.154 0.315 0.248 0.11 0.364 ...\n\n\nTo apply the TRS calculations to the bootstrap histories, we’ll use the formulas and methods we’ve already established for calculating TRS payouts, modified to account for each bootstrap history’s unique inflation rates and wage growth. Here’s the step-by-step approach:\n\n\nCode\n# Define TRS parameters\nyears_worked &lt;- 17.5\nmonths_in_retirement &lt;- 240\n\ntrs_simulations &lt;- lapply(bootstrap_histories, function(history) {\n  # Extract working wage growth and inflation during retirement\n  wage_growth &lt;- history$wage_growth\n  retirement_inflation &lt;- history$retirement_inflation\n  \n  # Calculate FAS (average of last 36 months of wages)\n  wages &lt;- cumprod(1 + wage_growth) * salary  # Assume starting salary of $50,000\n  FAS &lt;- mean(tail(wages, 36))\n  \n  # Calculate initial monthly pension\n  initial_monthly_pension &lt;- (FAS * 0.02 * years_worked) / 12\n  \n  # Simulate TRS payouts\n  trs_simulation &lt;- data.frame(\n    Month = 1:months_in_retirement,\n    Inflation_Adjusted_Payout = rep(0, months_in_retirement)\n  )\n  \n  trs_simulation$Inflation_Adjusted_Payout[1] &lt;- initial_monthly_pension\n  \n  # Adjust payouts annually in September\n  for (i in 2:months_in_retirement) {\n    trs_simulation$Inflation_Adjusted_Payout[i] &lt;- trs_simulation$Inflation_Adjusted_Payout[i - 1]\n    if (i %% 12 == 9 && i &gt;= 12) {  # Every September starting from Month 9\n      start_index &lt;- max(1, i - 12 + 1)  # Ensure no negative indices\n      annual_inflation &lt;- sum(retirement_inflation[start_index:i], na.rm = TRUE)\n      COLA &lt;- max(0.01, min(0.03, round(0.5 * annual_inflation, 3)))\n      trs_simulation$Inflation_Adjusted_Payout[i] &lt;- trs_simulation$Inflation_Adjusted_Payout[i] * (1 + COLA)\n    }\n  }\n  \n  return(trs_simulation)\n})\n\n\n# Example: View one simulation result\nhead(trs_simulations[[1]])\n\n\n  Month Inflation_Adjusted_Payout\n1     1                  3696.167\n2     2                  3696.167\n3     3                  3696.167\n4     4                  3696.167\n5     5                  3696.167\n6     6                  3696.167\n\n\nStep 3: Apply ORP Calculations to Bootstrap Histories Just like we did for the TRS, we will now simulate the ORP withdrawals based on each of the generated bootstrap histories. This will include sampling the monthly returns and withdrawals based on the bootstrap data.\n\n\nCode\n# Define ORP parameters\norp_final_value &lt;- 80270.13  # Initial balance from data\nmonthly_withdrawal_rate &lt;- 0.2916667 / 100  # Monthly withdrawal rate (4% annual rate / 12 months)\nworking_years_return &lt;- 0.005234864  # Portfolio return for ages 65-74\nretirement_years_return &lt;- 0.003989437  # Portfolio return for ages 75-85\n\norp_simulations &lt;- lapply(bootstrap_histories, function(history) {\n  \n  # Extract historical returns and inflation data\n  working_inflation &lt;- history$working_inflation\n  retirement_inflation &lt;- history$retirement_inflation\n  portfolio_returns &lt;- history$retirement_portfolio_returns\n  \n  # Set initial ORP balance\n  orp_balance &lt;- orp_final_value  # Starting ORP balance\n  \n  # Create an empty data frame to store the simulation results\n  orp_simulation &lt;- data.frame(\n    Month = 1:months_in_retirement,\n    Year = rep(retirement_age:(retirement_age + years_in_retirement - 1), each = 12),\n    Age = retirement_age + (1:months_in_retirement) / 12,\n    Inflation_Adjusted_Withdrawal = 0,\n    ORP_Balance = orp_balance\n  )\n  \n  # Calculate initial withdrawal\n  orp_simulation$Inflation_Adjusted_Withdrawal[1] &lt;- orp_balance * monthly_withdrawal_rate\n  orp_simulation$ORP_Balance[1] &lt;- orp_balance - orp_simulation$Inflation_Adjusted_Withdrawal[1]\n  \n  # Apply portfolio returns and withdrawals for each month\n  for (i in 2:months_in_retirement) {\n    # Apply inflation adjustment at the start of each year (January)\n    if (i %% 12 == 1) {  # Check if it's January (every 12th month)\n      inflation_rate &lt;- retirement_inflation[i - 1]  # Using the previous year's inflation rate\n      orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- orp_simulation$Inflation_Adjusted_Withdrawal[i - 1] * (1 + inflation_rate)\n    } else {\n      # For other months, keep the withdrawal the same as the previous month\n      orp_simulation$Inflation_Adjusted_Withdrawal[i] &lt;- orp_simulation$Inflation_Adjusted_Withdrawal[i - 1]\n    }\n    \n    # Apply portfolio return depending on age range\n    if (orp_simulation$Age[i] &lt;= 74) {\n      # For ages 65–74, use working years return\n      orp_simulation$ORP_Balance[i] &lt;- orp_simulation$ORP_Balance[i - 1] * (1 + working_years_return) - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    } else {\n      # For ages 75+, use retirement years return\n      orp_simulation$ORP_Balance[i] &lt;- orp_simulation$ORP_Balance[i - 1] * (1 + retirement_years_return) - orp_simulation$Inflation_Adjusted_Withdrawal[i]\n    }\n    \n    # Ensure balance does not go negative\n    orp_simulation$ORP_Balance[i] &lt;- max(orp_simulation$ORP_Balance[i], 0)\n  }\n  \n  return(orp_simulation)\n})\n\n# Example: View one simulation result\ntail(orp_simulations[[1]])\n\n\n    Month Year      Age Inflation_Adjusted_Withdrawal ORP_Balance\n235   235   84 84.58333                      3482.465           0\n236   236   84 84.66667                      3482.465           0\n237   237   84 84.75000                      3482.465           0\n238   238   84 84.83333                      3482.465           0\n239   239   84 84.91667                      3482.465           0\n240   240   84 85.00000                      3482.465           0\n\n\n\n\nCode\n# Visualize ORP Fund Balance and Monthly Income Over Time\nggplot(orp_simulation, aes(x = Month)) +\n  geom_line(aes(y = Inflation_Adjusted_Withdrawal, color = \"Monthly Income\")) +\n  labs(\n    title = \"ORP Fund Balance and Monthly Income Over Time\",\n    x = \"Month in Retirement\",\n    y = \"Value ($)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAnalyze the Probability of ORP Exhaustion One of the key questions is whether the ORP balance runs out before the employee passes away.\nWe can check for each simulation if the ORP balance reaches zero before the employee’s death age (age 85). This will allow us to calculate the probability that the ORP account is exhausted before death. Code for ORP Exhaustion Probability:\n\n\nCode\n# Calculate the probability of ORP exhaustion before death\norp_exhausted &lt;- sapply(orp_simulations, function(sim) {\n  # Check if the balance goes to zero before death\n  min_balance &lt;- min(sim$ORP_Balance)\n  if (min_balance == 0) {\n    return(TRUE)  # ORP exhausted before death\n  } else {\n    return(FALSE)\n  }\n})\n\n# Probability of ORP exhaustion\nprob_orp_exhaustion &lt;- mean(orp_exhausted)\nmessage(\"Probability of ORP Exhaustion before Death: \", round(prob_orp_exhaustion, 4))\n\n\nProbability of ORP Exhaustion before Death: 0.75\n\n\nStep 4.2: Compare Monthly Income Between TRS and ORP Next, we will compare the monthly income between the TRS and ORP plans. We will calculate the average monthly income from the TRS simulation and ORP simulation and compare them.\nCode for Comparing Average Monthly Income:\n\n\nCode\n# Calculate average monthly income for TRS\navg_trs_income &lt;- mean(trs_simulation$Inflation_Adjusted_Payout, na.rm = TRUE)\n\n# Calculate average monthly income for ORP\navg_orp_income &lt;- mean(orp_simulation$Inflation_Adjusted_Withdrawal, na.rm = TRUE)\n\n# Output the results\nmessage(\"Average TRS Income: \", round(avg_trs_income, 2))\n\n\nAverage TRS Income: 271.13\n\n\nCode\nmessage(\"Average ORP Income: \", round(avg_orp_income, 2))\n\n\nAverage ORP Income: 344.25\n\n\nStep 4.3: Investigate the Gap Between TRS and ORP Monthly Income Next, we can investigate the maximum and minimum gap between the monthly income for TRS and ORP over the simulation period.\nWe can calculate the difference in monthly income for each month and then find the maximum and minimum gaps.\nCode for Maximum and Minimum Gap:\n\n\nCode\n# Calculate the gap between TRS and ORP monthly income\nincome_gap &lt;- trs_simulation$Inflation_Adjusted_Payout - orp_simulation$Inflation_Adjusted_Withdrawal\n\n# Maximum and minimum gap\nmax_gap &lt;- max(income_gap, na.rm = TRUE)\nmin_gap &lt;- min(income_gap, na.rm = TRUE)\n\n# Output the results\nmessage(\"Maximum Monthly Income Gap (TRS - ORP): \", round(max_gap, 2))\n\n\nMaximum Monthly Income Gap (TRS - ORP): -56.83\n\n\nCode\nmessage(\"Minimum Monthly Income Gap (TRS - ORP): \", round(min_gap, 2))\n\n\nMinimum Monthly Income Gap (TRS - ORP): -92.24\n\n\nStep 4.4: Evaluate the 4% Withdrawal Rate Now, we can examine the 4% withdrawal rate for the ORP simulation and see how it affects the ORP balance over time.\nWe can run a sensitivity analysis to check if a different withdrawal rate (e.g., 3.5% or 5%) would make a significant difference in the long-term sustainability of the ORP balance.\nCode for Sensitivity Analysis:\n\n\nCode\n# Define alternative withdrawal rates (e.g., 3.5% and 5%)\nwithdrawal_rates &lt;- c(0.035 / 12, 0.04 / 12, 0.05 / 12)  # Monthly withdrawal rates\n\n# Simulate ORP for each withdrawal rate and check for exhaustion before death\nresults &lt;- lapply(withdrawal_rates, function(rate) {\n  orp_simulation_temp &lt;- orp_simulation\n  orp_simulation_temp$Inflation_Adjusted_Withdrawal &lt;- orp_simulation_temp$ORP_Balance * rate\n  \n  # Recalculate ORP balance after adjusting withdrawals\n  for (i in 2:months_in_retirement) {\n    orp_simulation_temp$ORP_Balance[i] &lt;- orp_simulation_temp$ORP_Balance[i - 1] * (1 + retirement_years_return) - orp_simulation_temp$Inflation_Adjusted_Withdrawal[i]\n    orp_simulation_temp$ORP_Balance[i] &lt;- max(orp_simulation_temp$ORP_Balance[i], 0)\n  }\n  \n  # Check if ORP runs out of funds before death\n  min_balance &lt;- min(orp_simulation_temp$ORP_Balance)\n  exhausted &lt;- min_balance == 0\n  \n  return(list(final_balance = min_balance, exhausted = exhausted))\n})\n\n\n# Extract the results from the list and create a data frame\nresults_df &lt;- do.call(rbind, lapply(results, function(x) {\n  data.frame(\n    Final_Balance = x$final_balance,\n    Exhausted = ifelse(x$exhausted, \"Yes\", \"No\")\n  )\n}))\n\n# Display the results in a nice format\nprint(results_df)\n\n\n  Final_Balance Exhausted\n1      80002.56        No\n2      80002.56        No\n3      51579.90        No"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Class_practice",
    "section": "",
    "text": "{r} # cuny &lt;- read_html(\"https://en.wikipedia.org/wiki/List_of_City_University_of_New_York_institutions\") # cuny_campuses &lt;- cuny |&gt;  #     html_elements(\"tbody\") #            html_elements(\"tr td:nth-child(2)\") # cuny_campuses #\n\n\n{r} # URL &lt;- \"https://cocktails.hadley.nz/\" # URLS &lt;- read_html(URL) |&gt; #     html_element(\"nav\") |&gt; #     html_elements(\"a\") |&gt; #     html_attr(\"href\") |&gt; #     paste0(URL, ...=_) #  # pull_cocktails &lt;- function(url){ #     cocktails &lt;- read_html(url) |&gt; html_elements(\".cocktail\") #     name &lt;- cocktails |&gt; html_elements(\"h2\") |&gt; html_text() #     ingredients &lt;- cocktails |&gt; html_elements(\"ul\") |&gt; html_text()  #     data.frame(name, ingredients) # } # #  # recipies &lt;- map(URLS, pull_cocktails, .progress=TRUE) |&gt; list_rbind() # } #\nhttps://michael-weylandt.com/STA9750/labs/lab11.html"
  },
  {
    "objectID": "TransitDATA.html#setup-libraries-needed-for-analysis",
    "href": "TransitDATA.html#setup-libraries-needed-for-analysis",
    "title": "Personal MTA performance Analysis Report",
    "section": "setup libraries needed for analysis",
    "text": "setup libraries needed for analysis\nI leveraged several powerful libraries to streamline the process.It allowed me to efficiently manipulate and clean the dataset, calculate trends, and generate high-quality visualizations to uncover insights.\n\n\nCode\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(httr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n\nLoad data\nI am loading the data which is sourced from the MTA Open Data platform. I am assigning the variable url to link to dataset. Following that I will be viewing the first few rows of the dataset to get a quick overview of its structure and contents for a better understanding. Also, I am using the glimpse() function from the dplyr package in R. This function provides a quick overview of the structure of the mta_data dataset.\n\n\nCode\nurl &lt;- \"https://data.ny.gov/api/views/vtvh-gimj/rows.csv?accessType=DOWNLOAD\"\n\n\nmta_data &lt;- read.csv(url)\n\n# View the first few rows\nhead(mta_data)\n\n\n       month   division line day_type num_on_time_trips num_sched_trips\n1 2020-01-01 A DIVISION    1        1              9035           10554\n2 2020-01-01 A DIVISION    1        2              2866            3050\n3 2020-01-01 A DIVISION    2        1              6058            7330\n4 2020-01-01 A DIVISION    2        2              1674            2340\n5 2020-01-01 A DIVISION    3        1              5732            6603\n6 2020-01-01 A DIVISION    3        2              1527            1875\n  terminal_on_time_performance\n1                    0.8560735\n2                    0.9396721\n3                    0.8264666\n4                    0.7153846\n5                    0.8680903\n6                    0.8144000\n\n\nCode\ndim(mta_data) \n\n\n[1] 2641    7\n\n\n\n\nCode\nglimpse(mta_data)\n\n\nRows: 2,641\nColumns: 7\n$ month                        &lt;chr&gt; \"2020-01-01\", \"2020-01-01\", \"2020-01-01\",…\n$ division                     &lt;chr&gt; \"A DIVISION\", \"A DIVISION\", \"A DIVISION\",…\n$ line                         &lt;chr&gt; \"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"…\n$ day_type                     &lt;int&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2,…\n$ num_on_time_trips            &lt;int&gt; 9035, 2866, 6058, 1674, 5732, 1527, 6552,…\n$ num_sched_trips              &lt;int&gt; 10554, 3050, 7330, 2340, 6603, 1875, 8124…\n$ terminal_on_time_performance &lt;dbl&gt; 0.8560735, 0.9396721, 0.8264666, 0.715384…\n\n\n\n\nData Cleaning:\nI will be dropping rows with missing values eliminates the risk of errors in calculations, statistical models, or visualizations later in the analysis annd verify the cleaning process.\n\n\nCode\n# Drop rows with missing values in all columns\nmta_data &lt;- mta_data |&gt; \n  drop_na()\n\n\n\n\nCode\ncolSums(is.na(mta_data))\n\n\n                       month                     division \n                           0                            0 \n                        line                     day_type \n                           0                            0 \n           num_on_time_trips              num_sched_trips \n                           0                            0 \nterminal_on_time_performance \n                           0 \n\n\n::: {.callout-tip title=“Data Analysis: What is the trend in terminal on-time performance from 2020–2024?”} ### 1.Which subway lines maintain the highest schedule? To identify the subway lines that consistently perform well in maintaining their schedules, I calculated the average terminal on-time performance for each line using the terminal_on_time_performance metric.\n\n\nCode\n# Calculate the average terminal on-time performance for each line\ntop_lines &lt;- mta_data |&gt;\n  group_by(line)|&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# View the top lines with the highest average on-time performance\nhead(top_lines)\n\n\n# A tibble: 6 × 2\n  line   avg_on_time_performance\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 GS                       0.995\n2 S 42nd                   0.994\n3 S Fkln                   0.994\n4 FS                       0.993\n5 S Rock                   0.968\n6 H                        0.965\n\n\n\n\nMonthly Trends in Terminal On-Time Performance (2020–2024): Impact of Delays on System Efficiency:\nThe analysis examines monthly trends in on-time performance from 2020 to 2024, focusing on peak and low periods to identify the impact of delays, safety incidents, and operational challenges.\n\n\nCode\n# Clean the data\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  drop_na(terminal_on_time_performance) |&gt;\n  filter(year(month) &gt;= 2020 & year(month) &lt;= 2024)\n\n# Calculate monthly average on-time performance\nmonthly_performance &lt;- mta_data |&gt;\n  group_by(month) |&gt;\n  summarize(avg_performance = mean(terminal_on_time_performance, na.rm = TRUE))\n\n# Identify the most and least on-time months\nmost_on_time &lt;- monthly_performance[which.max(monthly_performance$avg_performance), ]\nleast_on_time &lt;- monthly_performance[which.min(monthly_performance$avg_performance), ]\n\n# Plot the monthly performance with highlights\nggplot(monthly_performance, aes(x = month, y = avg_performance)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(data = most_on_time, aes(x = month, y = avg_performance), color = \"green\", size = 2) +\n  geom_point(data = least_on_time, aes(x = month, y = avg_performance), color = \"red\", size = 3) +\n  geom_text(aes(x = most_on_time$month, y = most_on_time$avg_performance, \n                label = paste0(\"Most On-Time: \", round(most_on_time$avg_performance, 2), \"%\")), \n            vjust = 1, color = \"green\") +\n  geom_text(aes(x = least_on_time$month, y = least_on_time$avg_performance, \n                label = paste0(\"Least On-Time: \", round(least_on_time$avg_performance, 2), \"%\")), \n            vjust = 1, color = \"red\") +\n  labs(\n    title = \"Monthly Terminal On-Time Performance (2020-2024)\",\n    x = \"Month\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_text(aes(x = most_on_time$month, y = most_on_time$avg_performance, : All aesthetics have length 1, but the data has 59 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = least_on_time$month, y = least_on_time$avg_performance, : All aesthetics have length 1, but the data has 59 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThe data highlights significant disruptions post-pandemic, with delays and operational inefficiencies driving variability, emphasizing the need for targeted improvements to stabilize performance.\n\n\nComparison\nThis analysis compares terminal on-time performance during the COVID (2020–2021) and post-COVID (2022–2024) periods to examine how delays and operational challenges influenced system efficiency.\n\n\nCode\n# Data Cleaning and Preparation\nmta_data &lt;- mta_data |&gt;\n  mutate(\n    month = as.Date(month, format = \"%Y-%m-%d\"),  # Convert 'month' to Date format\n    period = case_when(                          # Classify as COVID or Post-COVID\n      year(month) %in% c(2020, 2021) ~ \"COVID (2020-2021)\",\n      year(month) %in% c(2022, 2023, 2024) ~ \"Post-COVID (2022-2024)\"\n    )\n  ) |&gt;\n  filter(!is.na(terminal_on_time_performance))   # Remove missing performance data\n\n# Group by Period and Month\nperformance_by_period &lt;- mta_data |&gt;\n  group_by(period, month) |&gt;\n  summarize(\n    avg_performance = mean(terminal_on_time_performance, na.rm = TRUE)\n  )\n\n\n`summarise()` has grouped output by 'period'. You can override using the\n`.groups` argument.\n\n\nCode\n# Plot the Comparison Graph\nggplot(performance_by_period, aes(x = month, y = avg_performance, color = period, group = period)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Comparison of Terminal On-Time Performance (COVID vs Post-COVID)\",\n    x = \"Month\",\n    y = \"Average On-Time Performance (%)\",\n    color = \"Period\"\n  ) +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"6 months\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nPercentage Comparison\nThis analysis compares the average terminal on-time performance between the COVID (2020–2021) and post-COVID (2022–2024) periods to evaluate changes in system efficiency and the impact of operational challenges\n\n\nCode\n# Ensure 'month' column is in Date format and filter relevant years\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month)) |&gt;\n  filter(year(month) &gt;= 2020 & year(month) &lt;= 2024)\n\n# Add a period column to classify data into COVID and post-COVID\nmta_data &lt;- mta_data |&gt;\n  mutate(period = case_when(\n    year(month) %in% c(2020, 2021) ~ \"COVID (2020-2021)\",\n    year(month) %in% c(2022, 2023, 2024) ~ \"Post-COVID (2022-2024)\"\n  ))\n\n# Calculate average terminal on-time performance by period\nperformance_by_period &lt;- mta_data |&gt;\n  group_by(period) |&gt;\n  summarize(\n    avg_performance = mean(terminal_on_time_performance, na.rm = TRUE),\n    min_performance = min(terminal_on_time_performance, na.rm = TRUE),\n    max_performance = max(terminal_on_time_performance, na.rm = TRUE)\n  )\n\n# Print performance summary by period\nprint(performance_by_period)\n\n\n# A tibble: 2 × 4\n  period                 avg_performance min_performance max_performance\n  &lt;chr&gt;                            &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n1 COVID (2020-2021)                0.864           0.493               1\n2 Post-COVID (2022-2024)           0.819           0.310               1\n\n\nCode\n# Bar chart for COVID vs Post-COVID comparison\nggplot(performance_by_period, aes(x = period, y = avg_performance, fill = period)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Terminal On-Time Performance: COVID vs Post-COVID\",\n    x = \"Period\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe average on-time performance during the COVID period was 78%, while in the post-COVID period, it declined to 75%, reflecting an approximate 3% decrease. This decline highlights the impact of increased ridership, operational strain, and infrastructure challenges post-pandemic. Addressing these issues through targeted maintenance and resource optimization is crucial to restoring performance levels.\n\n\nTop 10 Subway Lines Post-COVID (2020–2022): Assessing On-Time Performance\nThis analysis identifies the top 10 subway lines with the highest on-time performance post-COVID (2020–2022) and highlights factors like shorter routes and efficient operations that contribute to their reliability. These insights aim to guide improvements across the system\n\n\nCode\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Step 2: Filter data for the period 2020 to 2022\nmta_data_filtered &lt;- mta_data |&gt;\n  filter(month &gt;= as.Date(\"2020-01-01\") & month &lt;= as.Date(\"2022-12-31\"))\n\n# Step 3: Calculate the average terminal on-time performance for each line\ntop_lines &lt;- mta_data_filtered |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# Step 4: Select the top 10 lines with the highest average on-time performance\ntop_10_lines &lt;- top_lines |&gt; \n  slice_head(n = 10)\n\n# Step 5: Plot the top 10 lines using a bar chart in descending order\nggplot(top_10_lines, aes(x = reorder(line, -avg_on_time_performance), y = avg_on_time_performance, fill = line)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Top 10 Subway Lines by Average On-Time Performance post-COVID  (2020–2022)\",\n    x = \"Subway Line\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\" # Remove legend for simplicity\n  )\n\n\n\n\n\n\n\n\n\nThe graph highlights the top 10 subway lines by average on-time performance post-COVID (2020–2022). Lines GS, FS, and H lead with near-perfect performance, showcasing high operational efficiency. Other lines, including L, 7, M, G, J, and 5, also maintain consistent reliability, indicating strong system performance despite post-COVID challenges.\n\n\nTop Performing Subway Lines Post-COVID (2023–2024): Analyzing On-Time Efficiency Trends\nThis analysis highlights the top 10 subway lines by average on-time performance post-COVID (2023–2024) to identify the most efficient lines and assess trends in reliability across the system.\n\n\nCode\n# Step 1: Convert 'month' column to Date format\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Step 2: Filter data for the period 2020 to 2022\nmta_data_filtered &lt;- mta_data |&gt;\n  filter(month &gt;= as.Date(\"2023-01-01\") & month &lt;= as.Date(\"2024-12-31\"))\n\n# Step 3: Calculate the average terminal on-time performance for each line\ntop_lines &lt;- mta_data_filtered |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# Step 4: Select the top 10 lines with the highest average on-time performance\ntop_10_lines &lt;- top_lines |&gt; \n  slice_head(n = 10)\n\n# Step 5: Plot the top 10 lines using a bar chart in descending order\nggplot(top_10_lines, aes(x = reorder(line, -avg_on_time_performance), y = avg_on_time_performance, fill = line)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Top 10 Subway Lines by Average On-Time Performance post-COVID  (2023–2024)\",\n    x = \"Subway Line\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\" # Remove legend for simplicity\n  )\n\n\n\n\n\n\n\n\n\nThe GS, S 42nd, S Fkln, and FS lines recorded the highest average on-time performance, nearing 100%, reflecting strong operational efficiency. Other lines like H, L, 1, M, and G also maintained high performance, though slightly lower than the top lines.\n\n\nHow has the overall terminal on-time performance trended each month from 2020 to 2024?\nThe analysis examines monthly on-time performance trends during the COVID-19 pandemic (2020–2021) and the subsequent recovery period (2022–2024).\n\n\nCode\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Calculate the average on-time performance for each month across all lines\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Calculate the average on-time performance for each month across all lines\nmonthly_performance &lt;- mta_data |&gt;\n  group_by(month = format(month, \"%Y-%m\")) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  mutate(month = as.Date(paste0(month, \"-01\"), format = \"%Y-%m-%d\"))\n\n# Add a column to indicate the period (COVID vs Post-COVID)\nmonthly_performance &lt;- monthly_performance |&gt;\n  mutate(period = ifelse(format(month, \"%Y\") %in% c(\"2020\", \"2021\"), \"COVID Period (2020-2021)\", \"Post-COVID Recovery (2022-2024)\"))\n\n# Plot the monthly trend with different colors for each period\nggplot(monthly_performance, aes(x = month, y = avg_terminal_on_time_performance, color = period)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Monthly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Period\") +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"3 months\") +\n  scale_color_manual(values = c(\"COVID Period (2020-2021)\" = \"pink\", \"Post-COVID Recovery (2022-2024)\" = \"purple\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe graph shows a clear decline in on-time performance during the COVID period (2020–2021), peaking above 90% initially but dropping to around 80% by late 2021. Post-COVID (2022–2024), performance began at a lower baseline of approximately 78% but demonstrated gradual recovery, reaching 83–85% by late 2024. Despite improvements, fluctuations indicate lingering operational challenges. Continued efforts are needed to stabilize performance and return to pre-COVID levels.\n\n\nSeasonal Impact\nLet’s identify seasonal and monthly trends. The goal is to highlight how external factors, such as weather and operational challenges, impact system reliability.\n\n\nCode\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\"))\n\n# Extract month name, month number, and assign a season based on month\nmta_data &lt;- mta_data |&gt;\n  mutate(month_name = format(month, \"%B\"),\n         month_num = as.numeric(format(month, \"%m\")),\n         season = case_when(\n           month_num %in% c(12, 1, 2) ~ \"Winter\",\n           month_num %in% c(3, 4, 5) ~ \"Spring\",\n           month_num %in% c(6, 7, 8) ~ \"Summer\",\n           month_num %in% c(9, 10, 11) ~ \"Fall\"\n         ))\n\n# Calculate the average on-time performance for each month across all years\nseasonal_performance &lt;- mta_data |&gt;\n  group_by(month_name, month_num, season) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(month_num)  # Arrange by month number for chronological order\n\n\n`summarise()` has grouped output by 'month_name', 'month_num'. You can override\nusing the `.groups` argument.\n\n\nCode\n# Plot the seasonal/monthly trend with colors by season\nggplot(seasonal_performance, aes(x = reorder(month_name, month_num), y = avg_on_time_performance, color = season, group = 1)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Seasonal/Monthly Terminal On-Time Performance Trend Across Years\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Season\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Winter\" = \"skyblue\", \"Spring\" = \"green\", \"Summer\" = \"orange\", \"Fall\" = \"brown\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe graph shows winter as the least reliable season, likely due to weather-related disruptions, while spring has the highest performance with fewer interruptions, highlighting the need for targeted strategies.\n\n\ncreating a dataset named borough_data that maps subway lines to their corresponding boroughs in New York City. Each row represents a subway line and the borough(s) it serves.\n\n\nCode\n# borough_data &lt;- data.frame(\n#   line = c(\"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"4\", \"5\", \n#            \"5\", \"6\", \"6\", \"7\", \"7\", \"A\", \"A\", \"B\", \"B\", \"B\", \n#            \"C\", \"C\", \"D\", \"D\", \"D\", \"E\", \"E\", \"F\", \"F\", \"F\", \n#            \"G\", \"G\", \"J\", \"J\", \"Z\", \"Z\", \"L\", \"L\", \"N\", \"N\", \n#            \"N\", \"Q\", \"Q\", \"R\", \"R\", \"R\", \"S Rock\", \"S Fkln\", \"S 42nd\"),\n#   borough = c(\"Manhattan\", \"Bronx\", \"Brooklyn\", \"Manhattan\", \n#               \"Brooklyn\", \"Manhattan\", \"Manhattan\", \"Brooklyn\", \n#               \"Bronx\", \"Brooklyn\", \"Manhattan\", \"Manhattan\", \n#               \"Bronx\", \"Manhattan\", \"Queens\", \"Manhattan\", \n#               \"Brooklyn\", \"Brooklyn\", \"Bronx\", \"Manhattan\", \n#               \"Manhattan\", \"Brooklyn\", \"Brooklyn\", \"Manhattan\", \n#               \"Bronx\", \"Queens\", \"Manhattan\", \"Manhattan\",\"Queens\", \"Brooklyn\", \n#               \"Queens\", \"Brooklyn\", \"Manhattan\", \"Queens\", \n#               \"Manhattan\", \"Queens\", \"Manhattan\", \"Brooklyn\", \n#               \"Queens\", \"Manhattan\", \"Brooklyn\", \"Manhattan\", \n#               \"Brooklyn\", \"Queens\", \"Manhattan\", \"Brooklyn\", \n#               \"Manhattan\", \"Brooklyn\", \"Manhattan\")\n# )\n# \n# mta_data_joined &lt;- borough_data |&gt;\n#   inner_join(borough_mapping, by = \"line\") |&gt;\n#   filter(!is.na(borough), !is.na(terminal_on_time_performance)) # Remove NAs\n# \n# \n# avg_performance_by_borough_period &lt;- mta_data_joined %&gt;%\n#   group_by(borough, period) %&gt;%\n#   summarize(avg_performance = mean(terminal_on_time_performance, na.rm = TRUE))\n# \n# # Create a bar chart to compare performance by borough and period\n# ggplot(avg_performance_by_borough_period, aes(x = borough, y = avg_performance, fill = period)) +\n#   geom_bar(stat = \"identity\", position = \"dodge\") +\n#   labs(\n#     title = \"Average Terminal On-Time Performance by Borough: COVID vs. Post-COVID\",\n#     x = \"Borough\",\n#     y = \"Average On-Time Performance (%)\"\n#   ) +\n#   theme_minimal() +\n#   theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nKey Takeaways:\n1.Lines such as GS, S 42nd, and FS emerged as the most reliable across all periods. 2.The system demonstrated resilience, with significant recovery post-COVID despite ongoing challenges. 3.Seasonal and monthly trends offer actionable insights into performance variations that can guide future operational strategies.\nThese findings align with the broader project objective of understanding how delays, on-time performance, and safety incidents influenced NYC subway efficiency. This analysis provides a foundation for exploring further borough-specific or external factors impacting performance.\n```"
  },
  {
    "objectID": "transit_data_final.html#setup-libraries-needed-for-analysis",
    "href": "transit_data_final.html#setup-libraries-needed-for-analysis",
    "title": "Personal MTA performance Analysis Report",
    "section": "setup libraries needed for analysis",
    "text": "setup libraries needed for analysis\nI leveraged several powerful libraries to streamline the process.It allowed me to efficiently manipulate and clean the dataset, calculate trends, and generate high-quality visualizations to uncover insights.\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(httr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(DT)\n\n\n\nLoad data\nI am loading the data which is sourced from the MTA Open Data platform. I am assigning the variable url to link to dataset. Following that I will be viewing the first few rows of the dataset to get a quick overview of its structure and contents for a better understanding. Also, I am using the glimpse() function from the dplyr package in R. This function provides a quick overview of the structure of the mta_data dataset.\n\n\nCode\nurl &lt;- \"https://data.ny.gov/api/views/vtvh-gimj/rows.csv?accessType=DOWNLOAD\"\n\n\nmta_data &lt;- read.csv(url)\n\n# View the first few rows\nDT::datatable(mta_data)\n\n\n\n\n\n\n\n\nData Cleaning:\nI will be dropping rows with missing values eliminates the risk of errors in calculations, statistical models, or visualizations later in the analysis annd verify the cleaning process.\n\n\nCode\n# Drop rows with missing values in all columns\nmta_data &lt;- mta_data |&gt; \n  drop_na()\n\n\n# Clean the data\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  drop_na(terminal_on_time_performance) |&gt;\n  filter(year(month) &gt;= 2020 & year(month) &lt;= 2024)\n\n\n::: {.callout-tip title=“Data Analysis: What is the trend in terminal on-time performance from 2020–2024?”} ### 1.Which subway lines maintain the highest schedule? To identify the subway lines that consistently perform well in maintaining their schedules, I calculated the average terminal on-time performance for each line using the terminal_on_time_performance metric.\n\n\nCode\n# Calculate the average terminal on-time performance for each line\ntop_lines &lt;- mta_data |&gt;\n  group_by(line)|&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# View the top lines with the highest average on-time performance\n DT::datatable(top_lines)\n\n\n\n\n\n\nLines such as GS, S 42nd, and FS serve as benchmarks for operational efficiency and customer satisfaction.\n\n\n\n\n\nCode\n# Calculate monthly average on-time performance\nmonthly_performance &lt;- mta_data |&gt;\n  group_by(month) |&gt;\n  summarize(avg_performance = mean(terminal_on_time_performance, na.rm = TRUE))\n\n# Identify the most and least on-time months\nmost_on_time &lt;- monthly_performance[which.max(monthly_performance$avg_performance), ]\nleast_on_time &lt;- monthly_performance[which.min(monthly_performance$avg_performance), ]\n\n# Plot the monthly performance with highlights\nggplot(monthly_performance, aes(x = month, y = avg_performance)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(data = most_on_time, aes(x = month, y = avg_performance), color = \"green\", size = 2) +\n  geom_point(data = least_on_time, aes(x = month, y = avg_performance), color = \"red\", size = 3) +\n  geom_text(aes(x = most_on_time$month, y = most_on_time$avg_performance, \n                label = paste0(\"Most On-Time: \", round(most_on_time$avg_performance, 2), \"%\")), \n            vjust = 1, color = \"green\") +\n  geom_text(aes(x = least_on_time$month, y = least_on_time$avg_performance, \n                label = paste0(\"Least On-Time: \", round(least_on_time$avg_performance, 2), \"%\")), \n            vjust = 1, color = \"red\") +\n  labs(\n    title = \"Monthly Terminal On-Time Performance (2020-2024)\",\n    x = \"Month\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCategorize data for COVID and Post-COVID and visualize\n\n\nCode\n# Clean and process the data\nmta_data &lt;- mta_data |&gt;\n  mutate(\n    month = as.Date(month, format = \"%Y-%m-%d\"),  # Convert 'month' to Date format\n    day_type = as.factor(day_type),              # Convert 'day_type' to a factor (1=Weekdays, 2=Weekends)\n    period = case_when(                          # Create a 'period' column\n      year(month) %in% c(2020, 2021) ~ \"COVID (2020-2021)\",\n      year(month) %in% c(2022, 2023, 2024) ~ \"Post-COVID (2022-2024)\"\n    )\n  ) |&gt;\n  filter(!is.na(terminal_on_time_performance))   # Remove rows with missing performance data\n\n# Add Peak vs. Off-Peak labeling\nmta_data &lt;- mta_data |&gt;\n  mutate(peak_offpeak = case_when(\n    day_type == 1 ~ \"Peak (Weekdays)\",\n    day_type == 2 ~ \"Off-Peak (Weekends)\"\n  ))\n\n# Group by COVID/Post-COVID and Peak/Off-Peak\nperformance_covid_peak &lt;- mta_data |&gt;\n  group_by(period, peak_offpeak) |&gt;\n  summarize(\n    avg_performance = mean(terminal_on_time_performance, na.rm = TRUE),\n    min_performance = min(terminal_on_time_performance, na.rm = TRUE),\n    max_performance = max(terminal_on_time_performance, na.rm = TRUE)\n  )\n\n\n# Plot Peak vs Off-Peak comparison for COVID and Post-COVID periods\nggplot(performance_covid_peak, aes(x = peak_offpeak, y = avg_performance, fill = period)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(\n    title = \"Terminal On-Time Performance: Peak vs Off-Peak (COVID vs Post-COVID)\",\n    x = \"Time Period\",\n    y = \"Average On-Time Performance (%)\",\n    fill = \"Period\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nVisualize Performance by Period\n\n\nCode\n# Group by Period and Month\nperformance_by_period &lt;- mta_data |&gt;\n  group_by(period, month) |&gt;\n  summarize(\n    avg_performance = mean(terminal_on_time_performance, na.rm = TRUE)\n  )\n\n# Plot the Comparison Graph\nggplot(performance_by_period, aes(x = month, y = avg_performance, color = period, group = period)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Comparison of Terminal On-Time Performance (COVID vs Post-COVID)\",\n    x = \"Month\",\n    y = \"Average On-Time Performance (%)\",\n    color = \"Period\"\n  ) +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"6 months\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nComparison of Terminal On-Time Performance (COVID vs Post-COVID)\n\n\nCode\n# Separate plots using facet_wrap\nggplot(performance_by_period, aes(x = month, y = avg_performance)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(size = 2, color = \"red\") +\n  facet_wrap(~ period, ncol = 1, scales = \"free_y\") +  # Create separate plots for each period\n  labs(\n    title = \"Comparison of Terminal On-Time Performance (COVID vs Post-COVID)\",\n    x = \"Month\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"6 months\") +\n  theme(\n    strip.text = element_text(size = 12, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\nTerminal On-Time Performance During COVID and post-COVID\n\n\nCode\n# Set up the plotting area to create two plots side by side\npar(mfrow = c(2, 1))  # Create a 2-row, 1-column layout\n\n# Plot 1: COVID Period\nwith(performance_by_period %&gt;% filter(period == \"COVID (2020-2021)\"), {\n  plot(\n    month, avg_performance, type = \"b\", col = \"blue\", pch = 16, lwd = 2,\n    xlab = \"Month\", ylab = \"On-Time Performance (%)\",\n    main = \"Terminal On-Time Performance During COVID (2020-2021)\",\n    xaxt = \"n\"\n  )\n  axis(1, at = month, labels = format(month, \"%b %Y\"), las = 2)\n})\n\n# Plot 2: Post-COVID Period\nwith(performance_by_period %&gt;% filter(period == \"Post-COVID (2022-2024)\"), {\n  plot(\n    month, avg_performance, type = \"b\", col = \"green\", pch = 16, lwd = 2,\n    xlab = \"Month\", ylab = \"On-Time Performance (%)\",\n    main = \"Terminal On-Time Performance Post-COVID (2022-2024)\",\n    xaxt = \"n\"\n  )\n  axis(1, at = month, labels = format(month, \"%b %Y\"), las = 2)\n})\n\n\n\n\n\n\n\n\n\nCode\n# Reset plotting area\npar(mfrow = c(1, 1))\n\n\n\n\nTerminal On-Time Performance: COVID vs Post-COVID\n\n\nCode\n# Calculate average, min, and max performance, and identify specific months\nperformance_summary &lt;- mta_data |&gt;\n  group_by(period) |&gt;\n  summarize(\n    avg_performance = mean(terminal_on_time_performance, na.rm = TRUE),\n    avg_month = month[which.min(abs(terminal_on_time_performance - mean(terminal_on_time_performance, na.rm = TRUE)))],  # Closest to avg\n    min_performance = min(terminal_on_time_performance, na.rm = TRUE),\n    min_month = month[which.min(terminal_on_time_performance)],  # Month for min\n    max_performance = max(terminal_on_time_performance, na.rm = TRUE),\n    max_month = month[which.max(terminal_on_time_performance)]   # Month for max\n  )\n\n# Reshape the data for plotting\nperformance_summary_long &lt;- performance_summary |&gt;\n  pivot_longer(cols = c(avg_performance, min_performance, max_performance),\n               names_to = \"Metric\",\n               values_to = \"Performance\")\n\n# Add time labels and format them\nperformance_summary_long &lt;- performance_summary_long |&gt;\n  mutate(\n    Time = case_when(\n      Metric == \"min_performance\" ~ format(min_month, \"%b %Y\"),\n      Metric == \"max_performance\" ~ format(max_month, \"%b %Y\"),\n      Metric == \"avg_performance\" ~ format(avg_month, \"%b %Y\")\n    )\n  )\n\n# Plot with adjustments for cropped labels\nggplot(performance_summary_long, aes(x = Performance, y = Metric, fill = period)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.4) +  # Reduce bar width\n  geom_text(aes(label = Time), position = position_dodge(width = 0.4), hjust = -0.3, size = 4) +  # Adjust hjust for better alignment\n  labs(\n    title = \"Terminal On-Time Performance: COVID vs Post-COVID\",\n    x = \"Percentage (%)\",\n    y = \"Performance Metric\",\n    fill = \"Period\"\n  ) +\n  theme_minimal(base_size = 12) +  # Reduce the base font size\n  theme(\n    axis.text.x = element_text(size = 8),  # Smaller x-axis text\n    axis.text.y = element_text(size = 8),  # Smaller y-axis text\n    plot.title = element_text(hjust = 0.5, size = 10, face = \"bold\"),  # Smaller title\n    legend.position = c(0.85, 0.85),  # Keep the legend in the top-right corner\n    legend.title = element_text(size = 10),  # Keep legend title size small\n    legend.text = element_text(size = 8),  # Smaller legend text\n    plot.margin = margin(8, 18, 8, 8)  # Increase right margin for text\n  ) +\n  scale_fill_manual(values = c(\"COVID (2020-2021)\" = \"blue\", \"Post-COVID (2022-2024)\" = \"red\")) +\n  coord_cartesian(clip = \"off\", xlim = c(0, 1.10))  # Extend x-axis to prevent cropping\n\n\n\n\n\n\n\n\n\n\n\nYearly Trends in Terminal On-Time Performance (2020-2024)\n\n\nCode\n# Bar chart for COVID vs Post-COVID comparison\nggplot(performance_by_period, aes(x = period, y = avg_performance, fill = period)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Terminal On-Time Performance: COVID vs Post-COVID\",\n    x = \"Period\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nHow does the terminal on-time performance vary across different subway lines from 2020 to 2024?\nTo identify the subway lines that consistently maintain the highest terminal on-time performance from 2020 to 2024, This approach provides a clear, data-driven way to identify the subway lines that performed the best during the COVID period (2020–2022) and early post-COVID recovery (2023-2024).\n\n\nCode\n# Step 1: Convert 'month' column to Date format\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Step 2: Filter data for the period 2020 to 2022\nmta_data_filtered &lt;- mta_data |&gt;\n  filter(month &gt;= as.Date(\"2020-01-01\") & month &lt;= as.Date(\"2022-12-31\"))\n\n# Step 3: Calculate the average terminal on-time performance for each line\ntop_lines &lt;- mta_data_filtered |&gt;\n  group_by(line) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# Step 4: Select the top 10 lines with the highest average on-time performance\ntop_10_lines &lt;- top_lines |&gt; \n  slice_head(n = 10)\n\n# Step 5: Plot the top 10 lines using a bar chart in descending order\nggplot(top_10_lines, aes(x = reorder(line, -avg_on_time_performance), y = avg_on_time_performance, fill = line)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Top 10 Lines by Average On-Time Performance COVID period (2020–2022)\",\n    x = \"Subway Line\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 42, hjust = 1),\n    legend.position = \"none\" # Remove legend for simplicity\n  )\n\n\n\n\n\n\n\n\n\n\n\nLine with the maximum and minimum number of on-time trips\n\n\nCode\n# Exclude rows where line is \"Systemwide\" becuase that is overall all\nfiltered_data &lt;- subset(mta_data, line != \"Systemwide\")\n\n# Calculate the total number of on-time trips for each line\nline_on_time_trips &lt;- aggregate(num_on_time_trips ~ line, data = filtered_data, sum)\n\n# Find the line with the maximum number of on-time trips\nmax_on_time_line &lt;- line_on_time_trips[which.max(line_on_time_trips$num_on_time_trips), ]\n\n# Find the line with the minimum number of on-time trips\nmin_on_time_line &lt;- line_on_time_trips[which.min(line_on_time_trips$num_on_time_trips), ]\n\n# Display the results\nprint(paste(\"The line with the maximum on-time trips is Line\", max_on_time_line$line,\n            \"with\", max_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n\n[1] \"The line with the maximum on-time trips is Line 7 with 893206 on-time trips.\"\n\n\nCode\nprint(paste(\"The line with the minimum on-time trips is Line\", min_on_time_line$line,\n            \"with\", min_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n\n[1] \"The line with the minimum on-time trips is Line S Rock with 12216 on-time trips.\"\n\n\n\n\nwhich Month has most and least on-time performance?\n\n\nCode\n# Convert the 'month' column to Date type if it's not already\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Calculate the average terminal on-time performance for each month\nmonthly_performance &lt;- aggregate(terminal_on_time_performance ~ format(month, \"%Y-%m\"), data = mta_data, mean)\n\n# Rename columns for clarity\ncolnames(monthly_performance) &lt;- c(\"month\", \"avg_terminal_on_time_performance\")\n\n# Find the month with the highest terminal on-time performance\nmax_performance_month &lt;- monthly_performance[which.max(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Find the month with the lowest terminal on-time performance\nmin_performance_month &lt;- monthly_performance[which.min(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Display the results\nprint(paste(\"The month with the highest terminal on-time performance is\", max_performance_month$month,\n            \"with an average on-time performance of\", round(max_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n\n[1] \"The month with the highest terminal on-time performance is 2020-05 with an average on-time performance of 0.94 %.\"\n\n\nCode\nprint(paste(\"The month with the lowest terminal on-time performance is\", min_performance_month$month,\n            \"with an average on-time performance of\", round(min_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n\n[1] \"The month with the lowest terminal on-time performance is 2024-02 with an average on-time performance of 0.78 %.\"\n\n\n\n\nYearly on-time performance trend from 2020 to 2024\nTo analyze the yearly terminal on-time performance trend from 2020 to 2024, I examined the average on-time performance for each year, highlighting how the subway system adapted during and after the COVID-19 pandemic.\n\n\nCode\n# Analyze the yearly on-time performance trend from 2020 to 2024\nmta_data |&gt;\n  # Convert 'month' to Date format if not already in Date format\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = format(month, \"%Y\")) |&gt;\n  \n  # Calculate the average on-time performance for each year\n  group_by(year) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  \n  # Plot the trend over years\n  ggplot(aes(x = as.numeric(year), y = avg_terminal_on_time_performance)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"Yearly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Year\",\n       y = \"Average On-Time Performance (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTop 5 Reliable Subway Lines for 2020–2022\n\n\nCode\n# Convert 'month' column to Date format if it's not already\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  \n  # Extract the year from the 'month' column\n  mutate(year = as.numeric(format(month, \"%Y\")))\n\n# Function to calculate top 5 lines for a specific year\nget_top_5_lines &lt;- function(data, year) {\n  data |&gt;\n    filter(year == !!year & !is.na(terminal_on_time_performance)) |&gt;\n    group_by(line) |&gt;\n    summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\") |&gt;\n    arrange(desc(avg_on_time_performance)) |&gt;\n    slice_head(n = 5) |&gt;\n    mutate(year = as.character(year))\n}\n\n# Calculate top 5 lines for 2020, 2021, and 2022\ntop_5_2020 &lt;- get_top_5_lines(mta_data, 2020)\ntop_5_2021 &lt;- get_top_5_lines(mta_data, 2021)\ntop_5_2022 &lt;- get_top_5_lines(mta_data, 2022)\n\n# Combine the results into one data frame\ntop_lines_combined &lt;- bind_rows(top_5_2020, top_5_2021, top_5_2022)\n\n# Plotting the top 5 lines for each year\nggplot(top_lines_combined, aes(x = reorder(line, avg_on_time_performance), \n                               y = avg_on_time_performance, fill = year)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Top 5 Reliable Subway Lines for 2020–2022\",\n       x = \"Subway Line\",\n       y = \"Average On-Time Performance (%)\",\n       fill = \"Year\") +\n  scale_fill_manual(values = c(\"2020\" = \"blue\", \"2021\" = \"orange\", \"2022\" = \"green\")) +\n  coord_flip() +\n  theme_minimal() +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nHow has the overall terminal on-time performance trended each month from 2020 to 2024?\n\n\nCode\n# Calculate the average on-time performance for each month across all lines\nmonthly_performance &lt;- mta_data |&gt;\n  group_by(month = format(month, \"%Y-%m\")) |&gt;\n  summarize(avg_terminal_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  mutate(month = as.Date(paste0(month, \"-01\"), format = \"%Y-%m-%d\"))\n\n# Add a column to indicate the period (COVID vs Post-COVID)\nmonthly_performance &lt;- monthly_performance |&gt;\n  mutate(period = ifelse(format(month, \"%Y\") %in% c(\"2020\", \"2021\"), \"COVID Period (2020-2021)\", \"Post-COVID Recovery (2022-2024)\"))\n\n# Plot the monthly trend with different colors for each period\nggplot(monthly_performance, aes(x = month, y = avg_terminal_on_time_performance, color = period)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Monthly Terminal On-Time Performance Trend (2020-2024)\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Period\") +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"3 months\") +\n  scale_color_manual(values = c(\"COVID Period (2020-2021)\" = \"pink\", \"Post-COVID Recovery (2022-2024)\" = \"purple\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Extract month name, month number, and assign a season based on month\nmta_data &lt;- mta_data |&gt;\n  mutate(month_name = format(month, \"%B\"),\n         month_num = as.numeric(format(month, \"%m\")),\n         season = case_when(\n           month_num %in% c(12, 1, 2) ~ \"Winter\",\n           month_num %in% c(3, 4, 5) ~ \"Spring\",\n           month_num %in% c(6, 7, 8) ~ \"Summer\",\n           month_num %in% c(9, 10, 11) ~ \"Fall\"\n         ))\n\n# Calculate the average on-time performance for each month across all years\nseasonal_performance &lt;- mta_data |&gt;\n  group_by(month_name, month_num, season) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)) |&gt;\n  arrange(month_num)  # Arrange by month number for chronological order\n\n# Plot the seasonal/monthly trend with colors by season\nggplot(seasonal_performance, aes(x = reorder(month_name, month_num), y = avg_on_time_performance, color = season, group = 1)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Seasonal/Monthly Terminal On-Time Performance Trend Across Years\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Season\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Winter\" = \"skyblue\", \"Spring\" = \"green\", \"Summer\" = \"orange\", \"Fall\" = \"brown\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nHow did the on-time performance of the top 5 subway lines evolve over time from 2020 to 2024?\n\n\nCode\nlibrary(gganimate)\nlibrary(gifski)\n\nborough_mapping &lt;- data.frame(\n  line = c(\"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"4\", \"5\", \n           \"5\", \"6\", \"6\", \"7\", \"7\", \"A\", \"A\", \"B\", \"B\", \"B\", \n           \"C\", \"C\", \"D\", \"D\", \"D\", \"E\", \"E\", \"F\", \"F\", \"F\", \n           \"G\", \"G\", \"J\", \"J\", \"Z\", \"Z\", \"L\", \"L\", \"N\", \"N\", \n           \"N\", \"Q\", \"Q\", \"R\", \"R\", \"R\", \"S Rock\", \"S Fkln\", \"S 42nd\"),\n  borough = c(\"Manhattan\", \"Bronx\", \"Brooklyn\", \"Manhattan\", \n              \"Brooklyn\", \"Manhattan\", \"Manhattan\", \"Brooklyn\", \n              \"Bronx\", \"Brooklyn\", \"Manhattan\", \"Manhattan\", \n              \"Bronx\", \"Manhattan\", \"Queens\", \"Manhattan\", \n              \"Brooklyn\", \"Brooklyn\", \"Bronx\", \"Manhattan\", \n              \"Manhattan\", \"Brooklyn\", \"Brooklyn\", \"Manhattan\", \n              \"Bronx\", \"Queens\", \"Manhattan\", \"Manhattan\",\"Queens\", \"Brooklyn\", \n              \"Queens\", \"Brooklyn\", \"Manhattan\", \"Queens\", \n              \"Manhattan\", \"Queens\", \"Manhattan\", \"Brooklyn\", \n              \"Queens\", \"Manhattan\", \"Brooklyn\", \"Manhattan\", \n              \"Brooklyn\", \"Queens\", \"Manhattan\", \"Brooklyn\", \n              \"Manhattan\", \"Brooklyn\", \"Manhattan\")\n)\n\n# Merge borough information with the MTA data\nmta_data_joined &lt;- mta_data |&gt;\n  inner_join(borough_mapping, by = \"line\") |&gt;\n  filter(!is.na(borough), !is.na(terminal_on_time_performance)) # Remove NAs\n\n\n# Calculate average on-time performance by borough and year\navg_performance_by_borough_year &lt;- mta_data_joined |&gt;\n  group_by(borough, year) |&gt;\n  summarize(avg_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\")\n\n# Create a line chart to visualize the trends\nggplot(avg_performance_by_borough_year, aes(x = year, y = avg_performance, color = borough)) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Average Terminal On-Time Performance by Borough (2020-2024)\",\n    x = \"Year\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load required libraries\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gganimate)\n\n# Calculate average on-time performance by borough and year\navg_performance_by_borough_year &lt;- mta_data_joined |&gt;\n  group_by(borough, year) |&gt;\n  summarize(avg_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\")\n\n# Create a line chart with animation\nanimated_plot &lt;-ggplot(avg_performance_by_borough_year, aes(x = year, y = avg_performance, color = borough)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Average Terminal On-Time Performance by Borough (2020-2024)\",\n    x = \"Year\",\n    y = \"Average On-Time Performance (%)\",\n    color = \"Borough\"\n  ) +\n  theme_minimal() +\n  transition_reveal(year) # Animation based on year\n\n# Render the animation\n# animated_gif &lt;- animate(animated_plot, nframes = 40, fps = 5, width = 800, height = 600, renderer = gifski_renderer())\n\nanim_save(\"avg_performance_trend.gif\", animated_plot)\n\n\n\n\n\nAverage Terminal On-Time Performance by Borough (2020-2024)\n\n\n\n\nKey Takeaways:\n1.Lines such as GS, S 42nd, and FS emerged as the most reliable across all periods. 2.The system demonstrated resilience, with significant recovery post-COVID despite ongoing challenges. 3.Seasonal and monthly trends offer actionable insights into performance variations that can guide future operational strategies.\nThese findings align with the broader project objective of understanding how delays, on-time performance, and safety incidents influenced NYC subway efficiency. This analysis provides a foundation for exploring further borough-specific or external factors impacting performance."
  },
  {
    "objectID": "individual_group_project.html",
    "href": "individual_group_project.html",
    "title": "Individual_part- Nikita Gautam",
    "section": "",
    "text": "What is the trend in terminal on-time performance from 2020 to 2024? How does performance differ between the COVID period (2020-2021) and post-COVID recovery (2022-2024)?\n\nMotivations and Importance of Analysis\nWe wanted to analyze borough-specific data to identify key subway challenges by combining ridership, delays, and safety metrics. Higher ridership often leads to increased delays, particularly in congested areas like Manhattan. Exploring the connection between delays and safety incidents revealed that recurring issues, such as track problems, impact both reliability and safety. Seasonal factors, like winter weather, further highlighted how external conditions affect system performance.\n\n\n\nQuantitative Findings and Their Qualitative Insights\nHigh ridership correlates with longer delays and increased safety risks, particularly during peak hours. Track-related issues emerged as a common cause of disruptions, emphasizing the need for preventive maintenance. Seasonal trends showed winter reducing system reliability, while other months performed better. Post-COVID patterns revealed shifting passenger habits and service gaps, particularly on busy lines.\n\n\nImportance of This Analysis\nBy connecting ridership, delays, and safety, we uncovered critical issues like infrastructure strain, and seasonal impacts. These insights support targeted improvements, better maintenance, resource allocation, and weather-specific strategies to enhance reliability, safety, and rider experience. This analysis demonstrates how combined metrics drive meaningful, system-wide improvements.\n\n\nAbout Our Data\nFor our analysis, we use multiple data sets from the New York government’s official website. They focus on different metrics. First, we use transit performance data, which evaluates train wait times against scheduled intervals and delay rates during peak and off-peak hours. Secondly, for safety data, we focus on different types of incidents, including safety incidents, track-related incidents, and train-related incidents. Lastly, for ridership data, we examine metrics such as wait times and ridership. These datasets provide us a comprehensive overview of the subway performance. They are strong for operation and performance analysis and up-to-date; however, the datasets may be biased because they do not include some external factors, such as policy, economics, etc., and do not explore differences between metro lines, such as infrastructure or trains and track quality. \n\n\nTrend of On-terminal by Borough(2024-2024).\nThe graph provides a comparative analysis of average terminal on-time performance by borough from 2020 to 2024. It highlights variations in performance across the Bronx, Brooklyn, Manhattan, and Queens. We aim to uncover how delays and safety incidents may have influenced overall efficiency. This will help pinpoint areas of operational strain and provide insights for improving performance and reliability across the subway system.\n\n\nCode\nknitr::include_graphics(\"animated_borough_performance.gif\")\n\n\n\n\n\n\n\n\n\nThe graph highlights borough-specific trends in on-time performance from 2020 to 2024, with a sharp decline in 2022 due to post-pandemic challenges. Queens consistently performed better, indicating greater resilience, while Brooklyn and the Bronx showed slower recovery and lower efficiency.The findings emphasize the need for targeted efforts to address delays, safety concerns, and operational inefficiencies.\n\n\nSeasonal Impact between (2024-2024).\nLet’s identify seasonal and monthly trends. The goal is to highlight how external factors, such as weather and operational challenges, impact system reliability.\n\n\nCode\nknitr::include_graphics(\"Weather_Impact.png\")\n\n\n\n\n\n\n\n\n\nThe graph shows winter as the least reliable season, likely due to weather-related disruptions, while spring has the highest performance with fewer interruptions, highlighting the need for targeted strategies.\nThis would be the individual analysis for On-Terminal Performance(2020-2024)"
  },
  {
    "objectID": "individual_report.html#setup-libraries-needed-for-analysis",
    "href": "individual_report.html#setup-libraries-needed-for-analysis",
    "title": "MTA On-time Performance Analysis",
    "section": "Setup libraries needed for analysis:",
    "text": "Setup libraries needed for analysis:\nI leveraged several powerful libraries to streamline the process.It allowed me to efficiently manipulate and clean the dataset, calculate trends, and generate high-quality visualizations to uncover insights.\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE)\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"lubridate\")) install.packages(\"lubridate\")\nif (!require(\"DT\")) install.packages(\"DT\")\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(httr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(gganimate)\n\n\n\nLoad data:\nI am loading the dataset from the MTA Open Data platform and assigning it to the variable url. To understand its structure and contents, I will view the first few rows and use the glimpse() function from the dplyr package for a quick structural overview.\n\n\nCode\nurl &lt;- \"https://data.ny.gov/api/views/vtvh-gimj/rows.csv?accessType=DOWNLOAD\"\n\nmta_data &lt;- read.csv(url)\n\n# View the first few rows\nDT::datatable(mta_data)\n\n\n\n\n\n\n\n\nData Cleaning:\nI will drop rows with missing values to ensure accurate analysis and verify the cleaning process. Additionally, I will categorize 2020–2021 as COVID and 2022–2024 as Post-COVID for analysis.\n\n\nCode\n# Drop rows with missing values in all columns\nmta_data &lt;- mta_data |&gt; \n  drop_na()\n\n\n# Clean the data\nmta_data &lt;- mta_data |&gt;\n  mutate(month = as.Date(month, format = \"%Y-%m-%d\")) |&gt;\n  filter(year(month) &gt;= 2020 & year(month) &lt;= 2024) |&gt;\n  drop_na(terminal_on_time_performance)\n\n# Data Cleaning and Preparation\nmta_data &lt;- mta_data |&gt;\n  mutate(\n    month = as.Date(month, format = \"%Y-%m-%d\"),  # Convert 'month' to Date format\n    period = case_when(                          # Classify as COVID or Post-COVID\n      year(month) %in% c(2020, 2021) ~ \"COVID (2020-2021)\",\n      year(month) %in% c(2022, 2023, 2024) ~ \"Post-COVID (2022-2024)\"\n    )\n  ) |&gt;\n  filter(!is.na(terminal_on_time_performance))   # Remove missing performance data\n\nmta_data$day_type &lt;- as.factor(mta_data$day_type)\n\n# Replace day_type levels with \"Weekdays\" and \"Weekends\"\nlevels(mta_data$day_type) &lt;- c(\"Weekdays\", \"Weekends\")\n\n\nData preparation on the MTA dataset by removing rows with missing values and filtering for the years 2020 to 2024.\n\n\n\n\n\n\nSubway Analysis\n\n\n\nWhich subway lines maintain the highest schedule?\nI will calculate and display the average terminal on-time performance for each subway line in the mta_data dataset to identify the most reliable lines and set benchmarks for improving underperforming ones.\n\n\nCode\n# Calculate the average terminal on-time performance for each line\ntop_lines &lt;- mta_data |&gt;\n  group_by(line)|&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(avg_on_time_performance))\n\n# View the top lines with the highest average on-time performance\n DT::datatable(top_lines)\n\n\n\n\n\n\nThe results show that the GS and S 42nd lines have the highest average on-time performance, exceeding 99%. Their shorter routes and lower complexity contribute to exceptional reliability, setting a benchmark for improving other underperforming lines.\n\n\n\n\n\n\n\n\nTrend Analysis\n\n\n\n\nComparison of Terminal On-Time Performance: COVID vs Post-COVID\nThe goal is to compare monthly trends in average terminal on-time performance between COVID and Post-COVID periods by grouping data by period and month, calculating averages, and visualizing the differences with a line graph.\n\n\nCode\n# Group by Period and Month\nperformance_by_period &lt;- mta_data |&gt;\n  group_by(period, month) |&gt;\n  summarize(\n    avg_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\")\n\n# Plot the Comparison Graph\nggplot(performance_by_period, aes(x = month, y = avg_performance, color = period, group = period)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Comparison of Terminal On-Time Performance (COVID vs Post-COVID)\",\n    x = \"Month\",\n    y = \"Average On-Time Performance (%)\",\n    color = \"Period\"\n  ) +\n  theme_minimal() +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"6 months\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe graph shows that average on-time performance peaked in 2020 during the early COVID period, likely due to reduced ridership and simplified operations. However, performance declined through 2021 and began recovering steadily from 2022 onward, showing gradual improvement through 2024.\n\n\nTrends in Terminal On-Time Performance: Peak and Low Periods (2020–2024)\nThis analysis evaluates monthly trends in terminal on-time performance (2020–2024) to identify the best and worst-performing months based on average on-time performance.\n\n\nCode\n# Calculate monthly average on-time performance\nmonthly_performance &lt;- mta_data |&gt;\n  group_by(month) |&gt;\n  summarize(avg_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\")\n\n# Identify the most and least on-time months\nmost_on_time &lt;- monthly_performance[which.max(monthly_performance$avg_performance), ]\nleast_on_time &lt;- monthly_performance[which.min(monthly_performance$avg_performance), ]\n\n# Plot the monthly performance with highlights\nggplot(monthly_performance, aes(x = month, y = avg_performance)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(data = most_on_time, aes(x = month, y = avg_performance), color = \"green\", size = 2) +\n  geom_point(data = least_on_time, aes(x = month, y = avg_performance), color = \"red\", size = 3) +\n  geom_text(aes(x = most_on_time$month, y = most_on_time$avg_performance, \n                label = paste0(\"Most On-Time: \", round(most_on_time$avg_performance, 2), \"%\")), \n            vjust = 1, color = \"green\") +\n  geom_text(aes(x = least_on_time$month, y = least_on_time$avg_performance, \n                label = paste0(\"Least On-Time: \", round(least_on_time$avg_performance, 2), \"%\")), \n            vjust = 1, color = \"red\") +\n  labs(\n    title = \"Terminal On-Time Performance (2020-2024)\",\n    x = \"Year\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe graph shows that the highest average on-time performance (0.94%) occurred in 2020 during the early COVID period. In contrast, the lowest on-time performance (0.78%) was recorded in 2024 during the post-COVID recovery phase, reflecting ongoing operational challenges and increased system demand.\n\n\n\n\n\n\n\n\n\nComparison Analysis\n\n\n\n\nWeekday vs Weekend Trends\nThis analysis compares total on-time trips on weekdays and weekends to reveal differences in operational efficiency and service demand.\n\n\nCode\n# Summarize data by day_type (Weekdays vs Weekends)\nweekend_weekday_summary &lt;- mta_data |&gt;\n  group_by(day_type) |&gt;\n  summarise(Total_On_Time_Trips = sum(num_on_time_trips, na.rm = TRUE)) |&gt;\n  arrange(desc(Total_On_Time_Trips))\n\n\n# Plot the data\nggplot(weekend_weekday_summary, aes(x = day_type, y = Total_On_Time_Trips, fill = day_type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Comparison of On-Time Trips: Weekdays vs Weekends\",\n       x = \"Day Type\",\n       y = \"Total On-Time Trips\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Weekdays\" = \"skyblue\", \"Weekends\" = \"orange\")) +\n  scale_x_discrete(labels = c(\"Weekdays\", \"Weekends\"))\n\n\n\n\n\n\n\n\n\nThe chart shows significantly higher total on-time trips on weekdays compared to weekends, reflecting greater service demand and operational efficiency during weekdays.\n\n\nComparing Terminal On-Time Performance: COVID vs Post-COVID Analysis\nThis analysis compares terminal on-time performance during the COVID and Post-COVID periods by evaluating key metrics: average, minimum, and maximum performance, providing a clear view of trends and variability across both periods.\n\n\nCode\n# Calculate average, min, and max performance, and identify specific months\nperformance_summary &lt;- mta_data |&gt;\n  group_by(period) |&gt;\n  summarize(\n    avg_performance = mean(terminal_on_time_performance, na.rm = TRUE),\n    avg_month = month[which.min(abs(terminal_on_time_performance - mean(terminal_on_time_performance, na.rm = TRUE)))],  # Closest to avg\n    min_performance = min(terminal_on_time_performance, na.rm = TRUE),\n    min_month = month[which.min(terminal_on_time_performance)],  # Month for min\n    max_performance = max(terminal_on_time_performance, na.rm = TRUE),\n    max_month = month[which.max(terminal_on_time_performance)]   # Month for max\n  )\n\n# Reshape the data for plotting\nperformance_summary_long &lt;- performance_summary |&gt;\n  pivot_longer(cols = c(avg_performance, min_performance, max_performance),\n               names_to = \"Metric\",\n               values_to = \"Performance\")\n\n# Add time labels and format them\nperformance_summary_long &lt;- performance_summary_long |&gt;\n  mutate(\n    Time = case_when(\n      Metric == \"min_performance\" ~ format(min_month, \"%b %Y\"),\n      Metric == \"max_performance\" ~ format(max_month, \"%b %Y\"),\n      Metric == \"avg_performance\" ~ format(avg_month, \"%b %Y\")\n    )\n  )\n\n# Plot with adjustments for cropped labels\nggplot(performance_summary_long, aes(x = Performance, y = Metric, fill = period)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.4) +  # Reduce bar width\n  geom_text(aes(label = Time), position = position_dodge(width = 0.4), hjust = -0.3, size = 4) +  # Adjust hjust for better alignment\n  labs(\n    title = \"Terminal On-Time Performance: COVID vs Post-COVID\",\n    x = \"Percentage (%)\",\n    y = \"Performance Metric\",\n    fill = \"Period\"\n  ) +\n  theme_minimal(base_size = 12) +  # Reduce the base font size\n  theme(\n    axis.text.x = element_text(size = 8),  # Smaller x-axis text\n    axis.text.y = element_text(size = 8),  # Smaller y-axis text\n    plot.title = element_text(hjust = 0.5, size = 10, face = \"bold\"),  # Smaller title\n    legend.position = c(0.85, 0.85),  # Keep the legend in the top-right corner\n    legend.title = element_text(size = 10),  # Keep legend title size small\n    legend.text = element_text(size = 8),  # Smaller legend text\n    plot.margin = margin(8, 18, 8, 8)  # Increase right margin for text\n  ) +\n  scale_fill_manual(values = c(\"COVID (2020-2021)\" = \"blue\", \"Post-COVID (2022-2024)\" = \"red\")) +\n  coord_cartesian(clip = \"off\", xlim = c(0, 1.10))  # Extend x-axis to prevent cropping\n\n\n\n\n\n\n\n\n\nThe graph shows similar maximum on-time performance during COVID and post-COVID (0.94%), but the minimum dropped from 0.83% to 0.78%, and the average declined from 0.89% to 0.87%, highlighting increased variability and operational challenges post-COVID.\n\n\n\n\n\n\n\n\n\nBorough Analysis\n\n\n\nHow did the on-time performance of the top 5 subway lines evolve over time from 2020 to 2024?\nThis analysis visualizes NYC boroughs average terminal on-time performance (2020–2024), highlighting yearly trends and variations.\n\n\nCode\nborough_mapping &lt;- data.frame(\n  line = c(\"1\", \"1\", \"2\", \"2\", \"3\", \"3\", \"4\", \"4\", \"4\", \"5\", \n           \"5\", \"6\", \"6\", \"7\", \"7\", \"A\", \"A\", \"B\", \"B\", \"B\", \n           \"C\", \"C\", \"D\", \"D\", \"D\", \"E\", \"E\", \"F\", \"F\", \"F\", \n           \"G\", \"G\", \"J\", \"J\", \"Z\", \"Z\", \"L\", \"L\", \"N\", \"N\", \n           \"N\", \"Q\", \"Q\", \"R\", \"R\", \"R\", \"S Rock\", \"S Fkln\", \"S 42nd\"),\n  borough = c(\"Manhattan\", \"Bronx\", \"Brooklyn\", \"Manhattan\", \n              \"Brooklyn\", \"Manhattan\", \"Manhattan\", \"Brooklyn\", \n              \"Bronx\", \"Brooklyn\", \"Manhattan\", \"Manhattan\", \n              \"Bronx\", \"Manhattan\", \"Queens\", \"Manhattan\", \n              \"Brooklyn\", \"Brooklyn\", \"Bronx\", \"Manhattan\", \n              \"Manhattan\", \"Brooklyn\", \"Brooklyn\", \"Manhattan\", \n              \"Bronx\", \"Queens\", \"Manhattan\", \"Manhattan\",\"Queens\", \"Brooklyn\", \n              \"Queens\", \"Brooklyn\", \"Manhattan\", \"Queens\", \n              \"Manhattan\", \"Queens\", \"Manhattan\", \"Brooklyn\", \n              \"Queens\", \"Manhattan\", \"Brooklyn\", \"Manhattan\", \n              \"Brooklyn\", \"Queens\", \"Manhattan\", \"Brooklyn\", \n              \"Manhattan\", \"Brooklyn\", \"Manhattan\")\n)\n# Ensure borough_mapping has unique mappings\nborough_mapping &lt;- borough_mapping |&gt; distinct(line, borough)\n\n# Join borough information with the MTA data\nmta_data_joined &lt;- mta_data |&gt;\n  inner_join(borough_mapping, by = \"line\", relationship = \"many-to-many\") |&gt;\n  filter(!is.na(borough), !is.na(terminal_on_time_performance)) |&gt; # Remove missing data\n  ungroup()\n\n\n# Extract year from the 'month' column and ensure it's numeric\nmta_data_joined &lt;- mta_data_joined |&gt;\n  mutate(\n    month = as.Date(month, format = \"%Y-%m-%d\"),  # Convert 'month' to Date\n    year = as.numeric(format(month, \"%Y\"))       # Extract and ensure 'year' is numeric\n  )\n\n\n# Calculate average on-time performance by borough and year\navg_performance_by_borough_year &lt;- mta_data_joined |&gt;\n  group_by(borough, year) |&gt;\n  summarize(avg_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\")\n\n# Calculate average on-time performance by borough and year\navg_performance_by_borough_year &lt;- avg_performance_by_borough_year |&gt;\n  group_by(borough) |&gt;\n  filter(n() &gt; 1) |&gt;  # Remove groups with a single observation\n  ungroup()\n\n# Create the animated plot\nsuppressWarnings({\nanimated_plot &lt;- ggplot(avg_performance_by_borough_year, aes(x = year, y = avg_performance, color = borough, group = borough)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Average Terminal On-Time Performance by Borough (2020–2024)\",\n    x = \"Year\",\n    y = \"Average On-Time Performance (%)\",\n    color = \"Borough\"\n  ) +\n  theme_minimal() +\n  transition_reveal(year)})\n\nanim_save(\"borough_performance.gif\", animated_plot)\n\nknitr::include_graphics(\"borough_performance.gif\")\n\n\n\n\n\n\n\n\n\nThe analysis shows a post-COVID decline in on-time performance (2020–2022), with Queens recovering strongest by 2024. Brooklyn experiences the steepest decline, while the Bronx and Manhattan show moderate recovery.\n\n\n\n\n\n\n\n\nAnalyzing lines and Terminal\n\n\n\n\nIdentifying Lines with Maximum and Minimum On-Time Trips\nIdentifying the subway lines with the highest and lowest total on-time trips helps assess operational efficiency and pinpoint performance disparities. This provides insight into how delays and on-time performance vary across lines, contributing to an understanding of their impact on system-wide efficiency and borough-specific trends from 2020 to 2024.\n\n\nCode\n# Exclude rows where line is \"Systemwide\" becuase that is overall all\nfiltered_data &lt;- subset(mta_data, line != \"Systemwide\")\n\n# Calculate the total number of on-time trips for each line\nline_on_time_trips &lt;- aggregate(num_on_time_trips ~ line, data = filtered_data, sum)\n\n# Find the line with the maximum number of on-time trips\nmax_on_time_line &lt;- line_on_time_trips[which.max(line_on_time_trips$num_on_time_trips), ]\n\n# Find the line with the minimum number of on-time trips\nmin_on_time_line &lt;- line_on_time_trips[which.min(line_on_time_trips$num_on_time_trips), ]\n\n# Display the results\nprint(paste(\"The line with the maximum on-time trips is Line\", max_on_time_line$line,\n            \"with\", max_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n\n[1] \"The line with the maximum on-time trips is Line 7 with 893206 on-time trips.\"\n\n\nCode\nprint(paste(\"The line with the minimum on-time trips is Line\", min_on_time_line$line,\n            \"with\", min_on_time_line$num_on_time_trips, \"on-time trips.\"))\n\n\n[1] \"The line with the minimum on-time trips is Line S Rock with 12216 on-time trips.\"\n\n\nLine 7, with the highest on-time trips (893,206), demonstrates exceptional efficiency and demand, while Line S Rock, with the lowest (12,216), highlights limited usage or operational challenges, emphasizing disparities in subway line performance.\n\n\nBest and Worst Months for Terminal On-Time Performance\nThis calculates average monthly on-time performance to identify the best and worst months, revealing trends and operational efficiency.\n\n\nCode\n# Convert the 'month' column to Date type if it's not already\nmta_data$month &lt;- as.Date(mta_data$month, format = \"%Y-%m-%d\")\n\n# Calculate the average terminal on-time performance for each month\nmonthly_performance &lt;- aggregate(terminal_on_time_performance ~ format(month, \"%Y-%m\"), data = mta_data, mean)\n\n# Rename columns for clarity\ncolnames(monthly_performance) &lt;- c(\"month\", \"avg_terminal_on_time_performance\")\n\n# Find the month with the highest terminal on-time performance\nmax_performance_month &lt;- monthly_performance[which.max(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Find the month with the lowest terminal on-time performance\nmin_performance_month &lt;- monthly_performance[which.min(monthly_performance$avg_terminal_on_time_performance), ]\n\n# Display the results\nprint(paste(\"The month with the highest terminal on-time performance is\", max_performance_month$month,\n            \"with an average on-time performance of\", round(max_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n\n[1] \"The month with the highest terminal on-time performance is 2020-05 with an average on-time performance of 0.94 %.\"\n\n\nCode\nprint(paste(\"The month with the lowest terminal on-time performance is\", min_performance_month$month,\n            \"with an average on-time performance of\", round(min_performance_month$avg_terminal_on_time_performance, 2), \"%.\"))\n\n\n[1] \"The month with the lowest terminal on-time performance is 2024-02 with an average on-time performance of 0.78 %.\"\n\n\nMay 2020 had the highest on-time performance (0.94%), possibly due to reduced ridership during COVID, while February 2024 had the lowest (0.78%), which might be influenced by seasonal factors and post-COVID recovery challenges.\n\n\n\n\n\n\n\n\n\nSeasonal Analysis\n\n\n\nSeasonal and Monthly Trends in Terminal On-Time Performance\nNow, let’s analyze how terminal on-time performance varies by season and month to uncover seasonal impacts, monthly trends, and identify periods needing targeted improvements.\n\n\nCode\n# Extract month name, month number, and assign a season based on month\nmta_data &lt;- mta_data |&gt;\n  mutate(month_name = format(month, \"%B\"),\n         month_num = as.numeric(format(month, \"%m\")),\n         season = case_when(\n           month_num %in% c(12, 1, 2) ~ \"Winter\",\n           month_num %in% c(3, 4, 5) ~ \"Spring\",\n           month_num %in% c(6, 7, 8) ~ \"Summer\",\n           month_num %in% c(9, 10, 11) ~ \"Fall\"\n         ))\n\n# Calculate the average on-time performance for each month across all years\nseasonal_performance &lt;- mta_data |&gt;\n  group_by(month_name, month_num, season) |&gt;\n  summarize(avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(month_num)  # Arrange by month number for chronological order\n\n# Plot the seasonal/monthly trend with colors by season\nggplot(seasonal_performance, aes(x = reorder(month_name, month_num), y = avg_on_time_performance, color = season, group = 1)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  labs(title = \"Seasonal/Monthly Terminal On-Time Performance Trend Across Years\",\n       x = \"Month\",\n       y = \"Average On-Time Performance (%)\",\n       color = \"Season\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Winter\" = \"skyblue\", \"Spring\" = \"green\", \"Summer\" = \"orange\", \"Fall\" = \"brown\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe graph shows higher on-time performance in spring, particularly in May, and lower performance in winter, especially February, likely due to weather-related delays. This highlights seasonal impacts on subway efficiency and identifies periods like winter requiring targeted improvements.\n\n\n\n\n\n\n\n\nCorrelation Analysis\n\n\n\n\nAnalyzing the Impact of Total Scheduled Trips on On-Time Performance\nThis analysis aims to explore how the total number of scheduled trips affects the average on-time performance of NYC subway lines, assessing whether higher operational loads impact efficiency and reliability.\n\n\nCode\n# Summarize data: calculate total trips and average on-time performance\ntrip_performance_summary &lt;- mta_data |&gt;\n  group_by(month) |&gt;\n  summarize(\n    total_scheduled_trips = sum(num_sched_trips, na.rm = TRUE),\n    avg_on_time_performance = mean(terminal_on_time_performance, na.rm = TRUE)\n  )\n\n# Plot the relationship between total trips and on-time performance\nggplot(trip_performance_summary, aes(x = total_scheduled_trips, y = avg_on_time_performance)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", formula =  'y ~ x', color = \"red\", se = TRUE) +\n  labs(\n    title = \"Impact of Total Scheduled Trips on On-Time Performance\",\n    x = \"Total Scheduled Trips\",\n    y = \"Average On-Time Performance (%)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe graph shows that as total scheduled trips increase, average on-time performance decreases, indicating that higher operational loads may lead to delays and reduced efficiency across subway lines.\n\n\nCorrelation between total scheduled trips and on-time trips\nThis analysis calculates and visualizes the correlation between total scheduled trips and on-time trips for NYC subway divisions, highlighting their relationship over time with a scatter plot and regression line.\n\n\nCode\n# Extract year from the 'month' column (assuming it's in YYYY-MM-DD format)\nmta_data$Year &lt;- year(as.Date(mta_data$month, format = \"%Y-%m-%d\"))\n\ncorrelation_data &lt;- mta_data |&gt;\n  group_by(division, Year) |&gt;\n  summarise(\n    Total_Scheduled_Trips = sum(num_sched_trips, na.rm = TRUE),\n    Total_On_Time_Trips = sum(num_on_time_trips, na.rm = TRUE), .groups = \"drop\"\n  )\n\n# Compute correlation coefficient\ncorrelation_value &lt;- cor(\n  correlation_data$Total_Scheduled_Trips,\n  correlation_data$Total_On_Time_Trips,\n  use = \"complete.obs\"\n)\n\n\n# Scatter Plot with Regression Line\nggplot(correlation_data, aes(x = Total_Scheduled_Trips, y = Total_On_Time_Trips)) +\n  geom_point(aes(color = division), size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', color = \"blue\", se = FALSE) +\n  labs(\n    title = \"Correlation Between Scheduled and On-Time Trips\",\n    subtitle = paste(\"Correlation Coefficient:\", round(correlation_value, 2)),\n    x = \"Total Scheduled Trips\",\n    y = \"Total On-Time Trips\",\n    color = \"Division\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\nThe strong positive correlation (0.98) indicates when more trips are scheduled, subway lines generally perform better in terms of on-time performance.\n\n\n\n\n\n\n\n\n\nKey Takeaways:\n\n\n\n• On-time performance peaked in 2020 and gradually improved from 2022, showing disruptions during COVID and recovery after.\n• Spring (especially May) showed the highest performance, while winter (especially February) had the lowest, likely due to weather-related challenges.\n• Queens recovered fastest, while Brooklyn showed the steepest decline and slowest recovery, with the Bronx and Manhattan showing moderate recovery.\n• Higher scheduled trips were linked to lower on-time performance, suggesting the need for better capacity management.\n• A strong positive correlation (0.98) between scheduled and on-time trips indicates that more scheduled trips lead to better efficiency.\n• More scheduled trips correlated with lower on-time performance, suggesting the need for better capacity management.\n• Reliable lines like GS and S 42nd maintained over 99% on-time performance, serving as benchmarks, while underperforming lines like S Rock revealed areas needing improvement.\n\n\n\n\nFuture Scope:\nFuture analyses should include pre-pandemic years for long-term trend insights and incorporate external factors like economics, infrastructure, and policies. Comparing with other transit systems could also offer valuable perspectives for improving NYC subway performance.\n\n\nLinks to my teammates reports that address other aspects of the overarching question:\nAnalysis of MTA Incident DistributionExamines incident categories during COVID and post periods.\nSubway Line Performance: Peak vs. Off-Peak DelaysCompares subway delays during COVID and post-COVID, focusing on peak and off-peak periods.\nSafety Incidents on Subway LinesAnalyzes safety incidents and trends across subway lines.\nRidership and Wait Time Analysis Investigates the relationship between ridership and wait times."
  },
  {
    "objectID": "individual_report.html",
    "href": "individual_report.html",
    "title": "MTA On-time Performance Analysis",
    "section": "",
    "text": "Overarching Question\n\n\n\n\nThis Report analyze how delays, on-time performance, and safety incidents impacted the efficiency of NYC subway lines across boroughs from 2020 to 2024, focusing on trends during the COVID and post-COVID periods and offering insights to enhance system reliability."
  },
  {
    "objectID": "group_project.html",
    "href": "group_project.html",
    "title": "Group 2: Subway Surfers",
    "section": "",
    "text": "Authors\nNikita Gautam\nHaolin Mo\nYanting Zhao\nChloe Yu\nTimbila Nikiema\n\n\n\n\n\n\nOverarching Question\n\n\n\nHow have delays, on-time performance, and safety incidents influenced the efficiency of subway lines across NYC boroughs from 2020 to 2024?\n\n\n\nMotivations\nThe motivation for this analysis is to uncover how delays, on-time performance, and safety incidents impacted the efficiency of NYC subway lines and boroughs during the COVID and the post-COVID recovery period. By exploring these factors, we aim to identify patterns, address operational inefficiencies, and provide insights to improve reliability and safety for future subway operations.\n\n\nQuantitative Findings and Qualitative Insights\nHigh ridership leads to longer delays and higher safety risks, especially during peak hours. Track-related disruptions highlight the need for better maintenance. Winter months reduce reliability, while other seasons perform better. Post-COVID trends show changing passenger habits and service gaps on busy lines.\n\n\nImportance of this Analysis\nBy connecting ridership, delays, and safety, we uncovered critical issues like infrastructure strain, and seasonal impacts. These insights support targeted improvements, better maintenance, resource allocation, and weather-specific strategies to enhance reliability, safety, and rider experience. This analysis demonstrates how combined metrics drive meaningful, system-wide improvements.\n\n\nAbout Our Data\nFor our analysis, we use multiple data sets from the NYC government’s official website. They focus on different metrics. First, we use transit performance data, which evaluates train wait times against scheduled intervals and delay rates during peak and off-peak hours. Secondly, for safety data, we focus on different types of incidents, including safety incidents, track-related incidents, and train-related incidents. Lastly, for ridership data, we examine metrics such as wait times and ridership. These datasets provide us a comprehensive overview of the subway performance.\n\n\nData Limitation\nThe datasets are strong for operational and performance analysis and are up-to-date but may be biased because the datasets focus only on 2020 to 2024, excluding deeper pre-pandemic trends, longer-term recovery patterns, and the full year of 2024. Additionally, the datasets do not include some external factors, such as policy, economics, etc.and they lack exploration of differences in infrastructure or track quality between subway lines.\n\n\nAnalysis addressing motivating questions\nThe analysis explores NYC subway trends from 2020 to 2024, focusing on incidents, performance, and ridership to answer key questions about system safety, efficiency, and capacity during and after the COVID-19 pandemic.\n\n\nIncidents & Safety Concerns\nTracking reported incidents over the study period reveals the most common safety issues on subway lines. Temporal patterns show how incident frequency and types shifted during COVID-19 and post-pandemic recovery. These insights help contextualize changes in rider behavior and vulnerabilities in system operations during periods of fluctuating ridership.\n\n\nSubway Performance:\nPerformance metrics such as terminal on-time rates and delay patterns during peak and off-peak hours are analyzed to assess operational efficiency. Comparing COVID and post-COVID periods shows how service reliability evolved as ridership recovered. Identifying high-performing lines and those with frequent delays highlights opportunities to enhance service reliability under varying demand condition\n\n\nKey Findings and Insights\n\n\nComparative Analysis: COVID vs. Post-COVID Incident Distribution\n\n\n\nMTA Incident During COVID\n\n\n\n\n\nMTA Incident Post COVID\n\n\nTo ensure a balanced analysis, both the COVID (2020-2021) and post-COVID (2022-2023) periods are scaled to two years each. Pie charts will illustrate the distribution of incident categories for each period. During the COVID period, signal-related incidents were most prevalent, comprising 33.2% of all incidents, followed by persons on trackbed/police/medical incidents at 30.2%. Other notable categories included track-related incidents (10.8%) and subway car incidents (8.9%). These distributions highlight the operational challenges of maintaining reliability and addressing immediate safety concerns during the pandemic.\nPost-COVID, the distribution shifts significantly. Persons on trackbed/police/medical incidents rose to 33.6%, becoming the largest category, while signal-related incidents decreased to 24.6%. Track-related incidents surged to 18.2%, reflecting emerging infrastructure maintenance challenges. These shifts indicate evolving priorities for MTA as the system transitions to a recovery phase, emphasizing the need for focused interventions to address rising safety and infrastructure issues.\nPlease go to this link for more detailed analysis.\n\n\nAnalysis of MTA Subway Line Performance: Peak vs. Off-Peak Delays\n\n\n\nMTA Train Delays Animation\n\n\nThe analysis of subway delay trends, comparing the COVID and Post-COVID periods, highlights significant changes in the performance of various subway lines. The animated bar chart offers a visual comparison of total delays for each subway line across these two timeframes. Delays generally increased for most subway lines, especially as New York City began to reopen. Certain lines, such as the N, F, 6, and A, experienced substantial increases in delays, with some nearly doubling. For example, delays on the N train surged from 60,073 to 100,434 after the pandemic, marking a 67.19% increase. Similarly, the 6 train delays rose from 41,200 to 98,141. This suggests that while delays were reduced during the shutdown, the return to full service and higher ridership in the post-pandemic period has led to an uptick in disruptions.\n\n\n\nMTA Train Delays Line Graph\n\n\nFurthermore,the animated plot was created to show how total delays changed monthly for both peak and off-peak periods. The plot revealed several trends, including a significant drop in delays during the early months of 2020, followed by a surge in delays as the city reopened and ridership increased. First quarter of 2020 saw a significant drop in delays due to the NYC shutdown in response to the pandemic. Mid-2021 marked the highest surge in delays, coinciding with the city’s phased reopening and increased ridership.\nPlease go to this link for more detailed analysis.\n\n\nHow have safety incidents (Persons on Trackbed/Police/Medical) reported to the MTA changed between COVID and Post-COVID?\nThe analysis focuses on the persons on trackbed/police/medical category from incidents and compares the average annual number of safety incidents across numbered subway lines and lettered subway lines. \n\n\n\nNumbered Subway Lines\n\n\nFor numbered subway lines, the bar chart shows that while the average annual number of safety incidents is decreased for the 6 and 4 train, it is increased on all other lines, so the overall trends from covid to post-covid time periods increase by 13.61%. The 6 train has the highest average incidents during both periods but decreased to 9.3 in post-COVID. The 5 train and 7 train have increase the most of almost any numbered subway line.\n\n\n\nLettered Subway Lines\n\n\nFor lettered subway lines, the numbers of annual average safety incidents of almost all subway increase, except the R and D train, with increase by 34%. The A, E, F, and N trains exhibit a significant increase from an average of 4 incidents during COVID to 7.3 incidents post-COVID, which increase the most across all lines.\nOverall, almost all subway lines face serious safety problems. With everyone returning to normal life from the post-covid recovery, it will pose a threat to rider safety if improvements are not made to the safety incident problem.\nPlease go to this link for more detailed analysis.\n\n\nWhat is the trend in terminal on-time performance from 2020 to 2024? How does performance differ between the COVID period and post-COVID recovery ?\n\n\nTrend of On-terminal by Borough\nThis shows comparative analysis of average terminal on-time performance by borough from 2020 to 2024. It highlights variations in performance across the Bronx, Brooklyn, Manhattan, and Queens. We aim to uncover how delays and safety incidents may have influenced overall efficiency. This will help pinpoint areas of operational strain and provide insights for improving performance and reliability across the subway system..\n\n\n\nLettered Subway Lines\n\n\nThe graph highlights borough-specific trends in on-time performance from 2020 to 2024, with a sharp decline in 2022 due to post-pandemic challenges. Queens consistently performed better, indicating greater resilience, while Brooklyn and the Bronx showed slower recovery and lower efficiency.The findings emphasize the need for targeted efforts to address delays, safety concerns, and operational inefficiencies.\n\n\nSeasonal Impact \nLet’s identify seasonal and monthly trends. The goal is to highlight how external factors, such as weather and operational challenges, impact system reliability.\n\n\n\nWeather Impact\n\n\nThe seasonal analysis shows distinct trends. Winter has variability, with a dip in February and recovery in December due to weather. Spring performs best, steadily improving to peak in May. Summer stays stable with a small drop in July and recovery in August. \nPlease go to this [On-Terminal Performance(2020-2024)](topic-https://nikitagtm.github.io/STA9750-2024-FALL/individual_report.html)for more detailed analysis.\n\n\nWhat is the relationship between ridership and wait times, and does higher ridership correlate with longer wait times?\nRidership Trend Analysis by Line\nGraph 1: \nRidership and wait time data provide insights into system dynamics across lines: High-Ridership Lines: Lines 1, 6, and 4 experience the highest ridership due to their service in densely populated and commercially active areas. Low-Ridership Lines: Shuttle services (e.g., S Rock, S Fkn, S 42nd) show the lowest ridership due to their limited scope. Passenger Distribution: Ridership on high-use lines (e.g., 1, 6) contrasts sharply with mid-tier lines (e.g., Q, R, C), reflecting variations in population density, service frequency, and transit connectivity\nAverage Wait Time Analysis by Line\nGraph 2: \nLine Variability: Shuttle lines consistently have the lowest wait times due to their shorter routes, while lines like B, D, M, and R face higher wait times, often due to congestion or complex operations. Trends Over Time: Most lines exhibit stable wait times across pre-COVID, COVID, and post-COVID periods, with notable variability on lines like C and R due to operational changes.\nCorrelation Analysis\nGraph3: \nHigher ridership shows a weak positive correlation with wait times, but service frequency and efficiency significantly influence outcomes. Effective strategies on shuttle and high-ridership lines highlight potential for managing demand. These insights support improving subway safety, performance, and reliability by addressing congestion and enhancing operations on busy routes.TimbilaNikiema\n\n\nRelation to Prior Work\nThe MTA’s annual performance metrics reports play a crucial role in evaluating the effectiveness of its public transportation services. By using benchmarking techniques, the MTA compares its performance against peer agencies, which helps identify best practices that can be implemented to improve operational efficiency and cost-effectiveness. These performance metrics are made publicly available to ensure transparency and are submitted to oversight agencies such as the FTA, while also being included in the National Transit Database.\nOur group’s work builds directly on this foundation by analyzing incident categories and trends, which aligns with the MTA’s commitment to using data for continuous service improvement. Our focus on incident trends, particularly post-COVID, offers more detailed insights that complement the broader performance metrics reported in the MTA’s annual reports. By focusing on the shifts in incidents during and after the pandemic, we shed light on the evolving operational challenges the MTA faces. Our analysis not only contributes to understanding the impact of these trends but also provides valuable context to help guide future decision-making within the organization.\n\n\nPotential Next Steps:\n\n\nPre-Pandemic Trends and Long-Term Patterns\nAnalyzing subway performance prior to the pandemic is vital for establishing a benchmark for comparison. Historical data can reveal long-term performance trends, pinpointing recurring issues with specific lines or infrastructure challenges that existed before COVID-19. Expanding the time frame back to the early 21st century will provide a clearer picture. This analysis helps differentiate between issues caused by the pandemic and those that have persisted over time.\n\n\nVariations Across Subway Lines and Boroughs\nFurther research should explore performance differences across subway lines and boroughs. Factors like infrastructure quality, line age, and service frequency affect service consistency. Older subway lines or those with outdated infrastructure tend to experience more maintenance issues and delays. Identifying these disparities allows for targeted solutions, such as infrastructure upgrades, more frequent service on high-demand routes, or new trains for aging lines."
  },
  {
    "objectID": "summary_report.html",
    "href": "summary_report.html",
    "title": "Group 2: Subway Surfers",
    "section": "",
    "text": "Authors\nNikita Gautam\nHaolin Mo\nYanting Zhao\nChloe Yu\nTimbila Nikiema\n\n\n\n\n\n\nOverarching Question\n\n\n\nHow have delays, on-time performance, and safety incidents influenced the efficiency of subway lines across NYC boroughs from 2020 to 2024?\n\n\n\nMotivations\nThe motivation for this analysis is to uncover how delays, on-time performance, and safety incidents impacted the efficiency of NYC subway lines and boroughs during the COVID and the post-COVID recovery period. By exploring these factors, we aim to identify patterns, address operational inefficiencies, and provide insights to improve reliability and safety for future subway operations.\n\n\nQuantitative Findings and Qualitative Insights\nHigh ridership leads to longer delays and higher safety risks, especially during peak hours. Track-related disruptions highlight the need for better maintenance. Winter months reduce reliability, while other seasons perform better. Post-COVID trends show changing passenger habits and service gaps on busy lines.\n\n\nImportance of this Analysis\nBy connecting ridership, delays, and safety, we uncovered critical issues like infrastructure strain, and seasonal impacts. These insights support targeted improvements, better maintenance, resource allocation, and weather-specific strategies to enhance reliability, safety, and rider experience. This analysis demonstrates how combined metrics drive meaningful, system-wide improvements.\n\n\nAbout Our Data\nFor our analysis, we use multiple data sets from the NYC government’s official website. They focus on different metrics. First, we use transit performance data, which evaluates train wait times against scheduled intervals and delay rates during peak and off-peak hours. Secondly, for safety data, we focus on different types of incidents, including safety incidents, track-related incidents, and train-related incidents. Lastly, for ridership data, we examine metrics such as wait times and ridership. These datasets provide us a comprehensive overview of the subway performance.\n\n\nData Limitation\nThe datasets are strong for operational and performance analysis and are up-to-date but may be biased because the datasets focus only on 2020 to 2024, excluding deeper pre-pandemic trends, longer-term recovery patterns, and the full year of 2024. Additionally, the datasets do not include some external factors, such as policy, economics, etc.and they lack exploration of differences in infrastructure or track quality between subway lines.\n\n\nAnalysis addressing motivating questions\nThe analysis explores NYC subway trends from 2020 to 2024, focusing on incidents, performance, and ridership to answer key questions about system safety, efficiency, and capacity during and after the COVID-19 pandemic.\n\n\nIncidents & Safety Concerns\nTracking reported incidents over the study period reveals the most common safety issues on subway lines. Temporal patterns show how incident frequency and types shifted during COVID-19 and post-pandemic recovery. These insights help contextualize changes in rider behavior and vulnerabilities in system operations during periods of fluctuating ridership.\n\n\nSubway Performance:\nPerformance metrics such as terminal on-time rates and delay patterns during peak and off-peak hours are analyzed to assess operational efficiency. Comparing COVID and post-COVID periods shows how service reliability evolved as ridership recovered. Identifying high-performing lines and those with frequent delays highlights opportunities to enhance service reliability under varying demand condition\n\n\nKey Findings and Insights\n\n\nComparative Analysis: COVID vs. Post-COVID Incident Distribution\n\n\n\nMTA Incident During COVID\n\n\n\n\n\nMTA Incident Post COVID\n\n\nTo ensure a balanced analysis, both the COVID (2020-2021) and post-COVID (2022-2023) periods are scaled to two years each. Pie charts will illustrate the distribution of incident categories for each period. During the COVID period, signal-related incidents were most prevalent, comprising 33.2% of all incidents, followed by persons on trackbed/police/medical incidents at 30.2%. Other notable categories included track-related incidents (10.8%) and subway car incidents (8.9%). These distributions highlight the operational challenges of maintaining reliability and addressing immediate safety concerns during the pandemic.\nPost-COVID, the distribution shifts significantly. Persons on trackbed/police/medical incidents rose to 33.6%, becoming the largest category, while signal-related incidents decreased to 24.6%. Track-related incidents surged to 18.2%, reflecting emerging infrastructure maintenance challenges. These shifts indicate evolving priorities for MTA as the system transitions to a recovery phase, emphasizing the need for focused interventions to address rising safety and infrastructure issues.\nPlease go to this link for more detailed analysis.\n\n\nAnalysis of MTA Subway Line Performance: Peak vs. Off-Peak Delays\n\n\n\nMTA Train Delays Animation\n\n\nThe analysis of subway delay trends, comparing the COVID and Post-COVID periods, highlights significant changes in the performance of various subway lines. The animated bar chart offers a visual comparison of total delays for each subway line across these two timeframes. Delays generally increased for most subway lines, especially as New York City began to reopen. Certain lines, such as the N, F, 6, and A, experienced substantial increases in delays, with some nearly doubling. For example, delays on the N train surged from 60,073 to 100,434 after the pandemic, marking a 67.19% increase. Similarly, the 6 train delays rose from 41,200 to 98,141. This suggests that while delays were reduced during the shutdown, the return to full service and higher ridership in the post-pandemic period has led to an uptick in disruptions.\n\n\n\nMTA Train Delays Line Graph\n\n\nFurthermore,the animated plot was created to show how total delays changed monthly for both peak and off-peak periods. The plot revealed several trends, including a significant drop in delays during the early months of 2020, followed by a surge in delays as the city reopened and ridership increased. First quarter of 2020 saw a significant drop in delays due to the NYC shutdown in response to the pandemic. Mid-2021 marked the highest surge in delays, coinciding with the city’s phased reopening and increased ridership.\nPlease go to this link for more detailed analysis.\n\n\nHow have safety incidents (Persons on Trackbed/Police/Medical) reported to the MTA changed between COVID and Post-COVID?\nThe analysis focuses on the persons on trackbed/police/medical category from incidents and compares the average annual number of safety incidents across numbered subway lines and lettered subway lines. \n\n\n\nNumbered Subway Lines\n\n\nFor numbered subway lines, the bar chart shows that while the average annual number of safety incidents is decreased for the 6 and 4 train, it is increased on all other lines, so the overall trends from covid to post-covid time periods increase by 13.61%. The 6 train has the highest average incidents during both periods but decreased to 9.3 in post-COVID. The 5 train and 7 train have increase the most of almost any numbered subway line.\n\n\n\nLettered Subway Lines\n\n\nFor lettered subway lines, the numbers of annual average safety incidents of almost all subway increase, except the R and D train, with increase by 34%. The A, E, F, and N trains exhibit a significant increase from an average of 4 incidents during COVID to 7.3 incidents post-COVID, which increase the most across all lines.\nOverall, almost all subway lines face serious safety problems. With everyone returning to normal life from the post-covid recovery, it will pose a threat to rider safety if improvements are not made to the safety incident problem.\nPlease go to this link for more detailed analysis.\n\n\nWhat is the trend in terminal on-time performance from 2020 to 2024? How does performance differ between the COVID period and post-COVID recovery ?\n\n\nTrend of On-terminal by Borough\nThis shows comparative analysis of average terminal on-time performance by borough from 2020 to 2024. It highlights variations in performance across the Bronx, Brooklyn, Manhattan, and Queens. We aim to uncover how delays and safety incidents may have influenced overall efficiency. This will help pinpoint areas of operational strain and provide insights for improving performance and reliability across the subway system..\n\n\n\nLettered Subway Lines\n\n\nThe graph highlights borough-specific trends in on-time performance from 2020 to 2024, with a sharp decline in 2022 due to post-pandemic challenges. Queens consistently performed better, indicating greater resilience, while Brooklyn and the Bronx showed slower recovery and lower efficiency.The findings emphasize the need for targeted efforts to address delays, safety concerns, and operational inefficiencies.\n\n\nSeasonal Impact \nLet’s identify seasonal and monthly trends. The goal is to highlight how external factors, such as weather and operational challenges, impact system reliability.\n\n\n\nWeather Impact\n\n\nThe seasonal analysis shows distinct trends. Winter has variability, with a dip in February and recovery in December due to weather. Spring performs best, steadily improving to peak in May. Summer stays stable with a small drop in July and recovery in August. \nPlease go to this [On-Terminal Performance(2020-2024)](topic-https://nikitagtm.github.io/STA9750-2024-FALL/individual_report.html)for more detailed analysis.\n\n\nWhat is the relationship between ridership and wait times, and does higher ridership correlate with longer wait times?\nRidership Trend Analysis by Line\nGraph 1: \nRidership and wait time data provide insights into system dynamics across lines: High-Ridership Lines: Lines 1, 6, and 4 experience the highest ridership due to their service in densely populated and commercially active areas. Low-Ridership Lines: Shuttle services (e.g., S Rock, S Fkn, S 42nd) show the lowest ridership due to their limited scope. Passenger Distribution: Ridership on high-use lines (e.g., 1, 6) contrasts sharply with mid-tier lines (e.g., Q, R, C), reflecting variations in population density, service frequency, and transit connectivity\nAverage Wait Time Analysis by Line\nGraph 2: \nLine Variability: Shuttle lines consistently have the lowest wait times due to their shorter routes, while lines like B, D, M, and R face higher wait times, often due to congestion or complex operations. Trends Over Time: Most lines exhibit stable wait times across pre-COVID, COVID, and post-COVID periods, with notable variability on lines like C and R due to operational changes.\nCorrelation Analysis\nGraph3: \nHigher ridership shows a weak positive correlation with wait times, but service frequency and efficiency significantly influence outcomes. Effective strategies on shuttle and high-ridership lines highlight potential for managing demand. These insights support improving subway safety, performance, and reliability by addressing congestion and enhancing operations on busy routes.TimbilaNikiema\n\n\nRelation to Prior Work\nThe MTA’s annual performance metrics reports play a crucial role in evaluating the effectiveness of its public transportation services. By using benchmarking techniques, the MTA compares its performance against peer agencies, which helps identify best practices that can be implemented to improve operational efficiency and cost-effectiveness. These performance metrics are made publicly available to ensure transparency and are submitted to oversight agencies such as the FTA, while also being included in the National Transit Database.\nOur group’s work builds directly on this foundation by analyzing incident categories and trends, which aligns with the MTA’s commitment to using data for continuous service improvement. Our focus on incident trends, particularly post-COVID, offers more detailed insights that complement the broader performance metrics reported in the MTA’s annual reports. By focusing on the shifts in incidents during and after the pandemic, we shed light on the evolving operational challenges the MTA faces. Our analysis not only contributes to understanding the impact of these trends but also provides valuable context to help guide future decision-making within the organization.\n\n\nPotential Next Steps:\n\n\nPre-Pandemic Trends and Long-Term Patterns\nAnalyzing subway performance prior to the pandemic is vital for establishing a benchmark for comparison. Historical data can reveal long-term performance trends, pinpointing recurring issues with specific lines or infrastructure challenges that existed before COVID-19. Expanding the time frame back to the early 21st century will provide a clearer picture. This analysis helps differentiate between issues caused by the pandemic and those that have persisted over time.\n\n\nVariations Across Subway Lines and Boroughs\nFurther research should explore performance differences across subway lines and boroughs. Factors like infrastructure quality, line age, and service frequency affect service consistency. Older subway lines or those with outdated infrastructure tend to experience more maintenance issues and delays. Identifying these disparities allows for targeted solutions, such as infrastructure upgrades, more frequent service on high-demand routes, or new trains for aging lines."
  }
]